{"text": "One issue limiting the adaption of large-scale multi-region segmentation is the sometimes prohibitive memory requirements. This is especially troubling considering advances in massively parallel computing and commercial graphics processing units because of their already limited memory compared to the current random access memory used in more traditional computation. To address this issue in the field of continuous max-flow segmentation, we have developed a pseudo-flow framework using the theory of Bregman proximal projections and entropic distances which implicitly represents flow variables between labels and designated source and sink nodes. This reduces the memory requirements for max-flow segmentation by approximately 20 for Potts models and approximately 30 for hierarchical max-flow (HMF) and directed acyclic graph max-flow (DAGMF) models. This represents a great improvement in the state-of-the-art in max-flow segmentation, allowing for much larger problems to be addressed and accelerated using commercially available graphics processing hardware.", "label": 1, "source": "scigen_human", "idx": 1, "lang": "en"}
{"text": "Long short-term memory (LSTM) and recurrent neural network (RNN) has achieved great successes on time-series prediction. In this paper, a methodology of using LSTM-based deep-RNN for two-phase flow regime prediction is proposed, motivated by previous research on constructing deep RNN. The method is featured with fast response and accuracy. The built RNN networks are trained and tested with time-series void fraction data collected using impedance void meter. The result shows that the prediction accuracy depends on the depth of network and the number of layer cells. However, deeper and larger network consumes more time in predicting.", "label": 1, "source": "scigen_human", "idx": 2, "lang": "en"}
{"text": "Visual Place Recognition (VPR) is the ability to correctly recall a previously visited place under changing viewpoints and appearances. A large number of handcrafted and deep-learning-based VPR techniques exist, where the former suffer from appearance changes and the latter have significant computational needs. In this paper, we present a new handcrafted VPR technique that achieves state-of-the-art place matching performance under challenging conditions. Our technique combines the best of 2 existing trainingless VPR techniques, SeqSLAM and CoHOG, which are each robust to conditional and viewpoint changes, respectively. This blend, namely ConvSequential-SLAM, utilises sequential information and block-normalisation to handle appearance changes, while using regional-convolutional matching to achieve viewpoint-invariance. We analyse content-overlap in-between query frames to find a minimum sequence length, while also re-using the image entropy information for environment-based sequence length tuning. State-of-the-art performance is reported in contrast to 8 contemporary VPR techniques on 4 public datasets. Qualitative insights and an ablation study on sequence length are also provided.", "label": 1, "source": "scigen_human", "idx": 3, "lang": "en"}
{"text": "We do a Probabilistic Analysis of the Network generated by robots involved inStochastic Boundary Coverage", "label": 1, "source": "scigen_human", "idx": 4, "lang": "en"}
{"text": "The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multi-layer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task.", "label": 1, "source": "scigen_human", "idx": 5, "lang": "en"}
{"text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors and social media content generated by local people. A bold obstacle to capturing, representing and integrating such heterogeneous and diverse information is lack of a proper ontology which properly conceptualizes this domain, aggregates and unifies datasets. Thus, in this paper, we introduce empathi ontology which conceptualizes the core concepts concerning with the domain of emergency managing and planning of hazard crises. Although empathi has a coarse-grained view, it considers the necessary concepts and relations being essential in this domain. This ontology is available at", "label": 1, "source": "scigen_human", "idx": 6, "lang": "en"}
{"text": "Audio-based cover song detection has received much attention in the MIR community in the recent years. To date, the most popular formulation of the problem has been to compare the audio signals of two tracks and to make a binary decision based on this information only. However, leveraging additional signals might be key if one wants to solve the problem at an industrial scale. In this paper, we introduce an ensemble-based method that approaches the problem from a many-to-many perspective. Instead of considering pairs of tracks in isolation, we consider larger sets of potential versions for a given composition, and create and exploit the graph of relationships between these tracks. We show that this can result in a significant improvement in performance, in particular when the number of existing versions of a given composition is large.", "label": 1, "source": "scigen_human", "idx": 7, "lang": "en"}
{"text": "Underwater imagery has enabled numerous civilian applications in various domains, ranging from academia to industry, and from industrial surveillance and maintenance to environmental protection and behavior of marine creatures studies. The accumulation of litter and plastic debris at the seafloor and the bottom of rivers are extremely harmful for the aquatic life. We propose a solution for monitoring this problem using a team of Autonomous Underwater Vehicles (AUVs) to exchange the recorded video in order to reconstruct the map of regions of interest. However, underwater video transmission is a challenge in the harsh environment in which radio-frequency waves are absorbed for distances above a few tens of meters, optical waves require narrow laser beams and suffer from scattering and ocean wave motion, and acoustic waves - while long range - provide a very low bandwidth and unreliable channel for communication. In our solution, the scalable coded video of each vehicle is shared in-network with a selected group of receiving vehicles through the underwater acoustic channel. Presented evaluations, including both simulations and experiments, confirm the efficiency and flexibility of the proposed solution using acoustic software-defined modems.", "label": 1, "source": "scigen_human", "idx": 8, "lang": "en"}
{"text": "The paper presents a new, robust control algorithm for position trajectory tracking in a 3D space, dedicated to underactuated airships. In order to take into account real characteristics of such vehicles, and to reflect practically motivated constraints, the algorithm assumes a highly uncertain system dynamics model. The tracking problem is solved in a uniform way, without dividing it into subtasks considered in 2D spaces, thanks to the introduction of an auxiliary tracking error. The proposed controller is based on the sliding mode approach. Its stability is investigated using Lyapunov theorem. Numerical simulations are conducted in order to verify properties of a closed-loop system for a generic model of the airship. Performance of the control system is examined via experiments in various scenarios using a prototype airship. The obtained results indicate that the control objectives are satisfied in practice with a reasonable accuracy. Moreover, it is shown that the controller is robust to some bounded additive measurement perturbations and delays in the control loop.", "label": 1, "source": "scigen_human", "idx": 9, "lang": "en"}
{"text": "Object detection has been vigorously investigated for years but fast accurate detection for real-world scenes remains a very challenging problem. Overcoming drawbacks of single-stage detectors, we take aim at precisely detecting objects for static and temporal scenes in real time. Firstly, as a dual refinement mechanism, a novel anchor-offset detection is designed, which includes an anchor refinement, a feature location refinement, and a deformable detection head. This new detection mode is able to simultaneously perform two-step regression and capture accurate object features. Based on the anchor-offset detection, a dual refinement network (DRNet) is developed for high-performance static detection, where a multi-deformable head is further designed to leverage contextual information for describing objects. As for temporal detection in videos, temporal refinement networks (TRNet) and temporal dual refinement networks (TDRNet) are developed by propagating the refinement information across time. We also propose a soft refinement strategy to temporally match object motion with the previous refinement. Our proposed methods are evaluated on PASCAL VOC, COCO, and ImageNet VID datasets. Extensive comparisons on static and temporal detection verify the superiority of DRNet, TRNet, and TDRNet. Consequently, our developed approaches run in a fairly fast speed, and in the meantime achieve a significantly enhanced detection accuracy, i.e., 84.4 mAP on VOC 2007, 83.6 mAP on VOC 2012, 69.4 mAP on VID 2017, and 42.4 AP on COCO. Ultimately, producing encouraging results, our methods are applied to online underwater object detection and grasping with an autonomous system. Codes are publicly available at", "label": 1, "source": "scigen_human", "idx": 10, "lang": "en"}
{"text": "This paper presents a tool for addressing a key component in many algorithms for planning robot trajectories under uncertainty: evaluation of the safety of a robot whose actions are governed by a closed-loop feedback policy near a nominal planned trajectory. We describe an adaptive importance sampling Monte Carlo framework that enables the evaluation of a given control policy for satisfaction of a probabilistic collision avoidance constraint which also provides an associated certificate of accuracy (in the form of a confidence interval). In particular this adaptive technique is well-suited to addressing the complexities of rigid-body collision checking applied to non-linear robot dynamics. As a Monte Carlo method it is amenable to parallelization for computational tractability, and is generally applicable to a wide gamut of simulatable systems, including alternative noise models. Numerical experiments demonstrating the effectiveness of the adaptive importance sampling procedure are presented and discussed.", "label": 1, "source": "scigen_human", "idx": 12, "lang": "en"}
{"text": "Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors.", "label": 1, "source": "scigen_human", "idx": 13, "lang": "en"}
{"text": "We develop a general framework for proving rigorous guarantees on the performance of the EM algorithm and a variant known as gradient EM. Our analysis is divided into two parts: a treatment of these algorithms at the population level (in the limit of infinite data), followed by results that apply to updates based on a finite set of samples. First, we characterize the domain of attraction of any global maximizer of the population likelihood. This characterization is based on a novel view of the EM updates as a perturbed form of likelihood ascent, or in parallel, of the gradient EM updates as a perturbed form of standard gradient ascent. Leveraging this characterization, we then provide non-asymptotic guarantees on the EM and gradient EM algorithms when applied to a finite set of samples. We develop consequences of our general theory for three canonical examples of incomplete-data problems: mixture of Gaussians, mixture of regressions, and linear regression with covariates missing completely at random. In each case, our theory guarantees that with a suitable initialization, a relatively small number of EM (or gradient EM) steps will yield (with high probability) an estimate that is within statistical error of the MLE. We provide simulations to confirm this theoretically predicted behavior.", "label": 1, "source": "scigen_human", "idx": 14, "lang": "en"}
{"text": "Video-based person re-identification matches video clips of people across non-overlapping cameras. Most existing methods tackle this problem by encoding each video frame in its entirety and computing an aggregate representation across all frames. In practice, people are often partially occluded, which can corrupt the extracted features. Instead, we propose a new spatiotemporal attention model that automatically discovers a diverse set of distinctive body parts. This allows useful information to be extracted from all frames without succumbing to occlusions and misalignments. The network learns multiple spatial attention models and employs a diversity regularization term to ensure multiple models do not discover the same body part. Features extracted from local image regions are organized by spatial attention model and are combined using temporal attention. As a result, the network learns latent representations of the face, torso and other body parts using the best available image patches from the entire video sequence. Extensive evaluations on three datasets show that our framework outperforms the state-of-the-art approaches by large margins on multiple metrics.", "label": 1, "source": "scigen_human", "idx": 15, "lang": "en"}
{"text": "We propose a compressive sensing algorithm that exploits geometric properties of images to recover images of high quality from few measurements. The image reconstruction is done by iterating the two following steps: 1) estimation of normal vectors of the image level curves and 2) reconstruction of an image fitting the normal vectors, the compressed sensing measurements and the sparsity constraint. The proposed technique can naturally extend to non local operators and graphs to exploit the repetitive nature of textured images in order to recover fine detail structures. In both cases, the problem is reduced to a series of convex minimization problems that can be efficiently solved with a combination of variable splitting and augmented Lagrangian methods, leading to fast and easy-to-code algorithms. Extended experiments show a clear improvement over related state-of-the-art algorithms in the quality of the reconstructed images and the robustness of the proposed method to noise, different kind of images and reduced measurements.", "label": 1, "source": "scigen_human", "idx": 16, "lang": "en"}
{"text": "Quantum memories are a fundamental of any global-scale quantum Internet, high-performance quantum networking and near-term quantum computers. A main problem of quantum memories is the low retrieval efficiency of the quantum systems from the quantum registers of the quantum memory. Here, we define a novel quantum memory called high-retrieval-efficiency (HRE) quantum memory for near-term quantum devices. An HRE quantum memory unit integrates local unitary operations on its hardware level for the optimization of the readout procedure and utilizes the advanced techniques of quantum machine learning. We define the integrated unitary operations of an HRE quantum memory, prove the learning procedure, and evaluate the achievable output signal-to-noise ratio values. We prove that the local unitaries of an HRE quantum memory achieve the optimization of the readout procedure in an unsupervised manner without the use of any labeled data or training sequences. We show that the readout procedure of an HRE quantum memory is realized in a completely blind manner without any information about the input quantum system or about the unknown quantum operation of the quantum register. We evaluate the retrieval efficiency of an HRE quantum memory and the output SNR (signal-to-noise ratio). The results are particularly convenient for gate-model quantum computers and the near-term quantum devices of the quantum Internet.", "label": 1, "source": "scigen_human", "idx": 17, "lang": "en"}
{"text": "Achieving transparency in black-box deep learning algorithms is still an open challenge. High dimensional features and decisions given by deep neural networks (NN) require new algorithms and methods to expose its mechanisms. Current state-of-the-art NN interpretation methods (e.g. Saliency maps, DeepLIFT, LIME, etc.) focus more on the direct relationship between NN outputs and inputs rather than the NN structure and operations itself. In current deep NN operations, there is uncertainty over the exact role played by neurons with fixed activation functions. In this paper, we achieve partially explainable learning model by symbolically explaining the role of activation functions (AF) under a scalable topology. This is carried out by modelling the AFs as adaptive Gaussian Processes (GP), which sit within a novel scalable NN topology, based on the Kolmogorov-Arnold Superposition Theorem (KST). In this scalable NN architecture, the AFs are generated by GP interpolation between control points and can thus be tuned during the back-propagation procedure via gradient descent. The control points act as the core enabler to both local and global adjustability of AF, where the GP interpolation constrains the intrinsic autocorrelation to avoid over-fitting. We show that there exists a trade-off between the NN's expressive power and interpretation complexity, under linear KST topology scaling. To demonstrate this, we perform a case study on a binary classification dataset of banknote authentication. Our model converge at better precision rate than state-of-the-art SVM algorithms which indicates that we do not make performance sacrifices in our approach. Meanwhile, by quantitatively and qualitatively investigating the mapping relationship between inputs and output, our explainable model can provide interpretation over each of the one-dimensional attributes. These early results suggest that our model has the potential to act as the final interpretation layer for deep neural networks.", "label": 1, "source": "scigen_human", "idx": 18, "lang": "en"}
{"text": "We study the sensitivity to noise of permanent (X) 2 for random real and complex x n n Gaussian matrices X, and show that asymptotically the correlation between the noisy and noiseless outcomes tends to zero when the noise level is o (1) n. This suggests that, under certain reasonable noise models, the probability distributions produced by noisy BosonSampling are very sensitive to noise. We also show that when the amount of noise is constant the noisy value of permanent (X) 2 can be approximated efficiently on a classical computer. These results seem to weaken the possibility of demonstrating quantum-speedup via BosonSampling without quantum fault-tolerance.", "label": 1, "source": "scigen_human", "idx": 19, "lang": "en"}
{"text": "Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first highlight both clinical needs and technical challenges in medical imaging and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.", "label": 1, "source": "scigen_human", "idx": 20, "lang": "en"}
{"text": "Most theoretical frameworks that focus on data errors and inconsistencies follow logic-based reasoning. Yet, practical data cleaning tools need to incorporate statistical reasoning to be effective in real-world data cleaning tasks. Motivated by empirical successes, we propose a formal framework for unclean databases, where two types of statistical knowledge are incorporated: The first represents a belief of how intended (clean) data is generated, and the second represents a belief of how noise is introduced in the actual observed database. To capture this noisy channel model, we introduce the concept of a Probabilistic Unclean Database (PUD), a triple that consists of a probabilistic database that we call the intention, a probabilistic data transformator that we call the realization and captures how noise is introduced, and an observed unclean database that we call the observation. We define three computational problems in the PUD framework: cleaning (infer the most probable intended database, given a PUD), probabilistic query answering (compute the probability of an answer tuple over the unclean observed database), and learning (estimate the most likely intention and realization models of a PUD, given examples as training data). We illustrate the PUD framework on concrete representations of the intention and realization, show that they generalize traditional concepts of repairs such as cardinality and value repairs, draw connections to consistent query answering, and prove tractability results. We further show that parameters can be learned in some practical instantiations, and in fact, prove that under certain conditions we can learn a PUD directly from a single dirty database without any need for clean examples.", "label": 1, "source": "scigen_human", "idx": 21, "lang": "en"}
{"text": "GPU accelerators have had a notable impact on high-performance computing across many disciplines. They provide high performance with low costpower, and therefore have become a primary compute resource on many of the largest supercomputers. Here, we implement multi-GPU acceleration into our Solar MHD code (MAS) using OpenACC in a fully portable, single-source manner. Our preliminary implementation is focused on MAS running in a reduced physics \"zero-beta\" mode. While valuable on its own, our main goal is to pave the way for a full physics, thermodynamic MHD implementation. We describe the OpenACC implementation methodology and challenges. \"Time-to-solution\" performance results of a production-level flux rope eruption simulation on multi-CPU and multi-GPU systems are shown. We find that the GPU-accelerated MAS code has the ability to run \"zero-beta\" simulations on a single multi-GPU server at speeds previously requiring multiple CPU server-nodes of a supercomputer.", "label": 1, "source": "scigen_human", "idx": 22, "lang": "en"}
{"text": "When we try to solve a system of linear equations, we can consider a simple iterative algorithm in which an equation including only one variable is chosen at each step, and the variable is fixed to the value satisfying the equation. The dynamics of this algorithm is captured by the peeling algorithm. Analyses of the peeling algorithm on random hypergraphs are required for many problems, e.g., the decoding threshold of low-density parity check codes, the inverting threshold of Goldreich's pseudorandom generator, the load threshold of cuckoo hashing, etc. In this work, we deal with random hypergraphs including superlinear number of hyperedges, and derive the tight threshold for the succeeding of the peeling algorithm. For the analysis, Wormald's method of differential equations, which is commonly used for analyses of the peeling algorithm on random hypergraph with linear number of hyperedges, cannot be used due to the superlinear number of hyperedges. A new method called the evolution of the moment generating function is proposed in this work.", "label": 1, "source": "scigen_human", "idx": 23, "lang": "en"}
{"text": "We propose a flexible framework for clustering hypergraph-structured data based on recently proposed random walks utilizing edge-dependent vertex weights. When incorporating edge-dependent vertex weights (EDVW), a weight is associated with each vertex-hyperedge pair, yielding a weighted incidence matrix of the hypergraph. Such weightings have been utilized in term-document representations of text data sets. We explain how random walks with EDVW serve to construct different hypergraph Laplacian matrices, and then develop a suite of clustering methods that use these incidence matrices and Laplacians for hypergraph clustering. Using several data sets from real-life applications, we compare the performance of these clustering algorithms experimentally against a variety of existing hypergraph clustering methods. We show that the proposed methods produce higher-quality clusters and conclude by highlighting avenues for future work.", "label": 1, "source": "scigen_human", "idx": 24, "lang": "en"}
{"text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not require major hardware or software investments. We design, implement, and experiment a force-directed algorithm in Giraph, a popular open source framework for distributed computing, based on a vertex-centric design paradigm. The algorithm is tested both on real and artificial graphs with up to million edges, by using a rather inexpensive PaaS (Platform as a Service) infrastructure of Amazon. The experiments show the scalability and effectiveness of our technique when compared to a centralized implementation of the same force-directed model. We show that graphs with about one million edges can be drawn in less than 8 minutes, by spending about 1 per drawing in the cloud computing infrastructure.", "label": 1, "source": "scigen_human", "idx": 26, "lang": "en"}
{"text": "The rapid growth of multimedia consumption has triggered technical, economic, and business innovations that improve the quality and accessibility of content. It has also opened new markets, promising large revenues for industry players. However, new technologies also pose new questions regarding the legal aspects of content delivery, which are often resolved through litigation between copyright owners and content distributors. The precedents set by these cases will act as a game changer in the content delivery industry and will shape the existing offerings in the market in terms of how new technologies can be deployed and what kind of pricing strategies can be associated with them. In this paper, we offer a tutorial on key copyright and communications laws and decisions related to storage and transmission of video content over the Internet. We summarize legal limitations on the deployment of new technologies and pricing mechanisms, and explain the implications of recent lawsuits. Understanding these concerns is essential for engineers engaged in designing the technical and economic aspects of video delivery systems.", "label": 1, "source": "scigen_human", "idx": 27, "lang": "en"}
{"text": "We study the power and limits of optimal dynamic pricing in combinatorial markets; i.e., dynamic pricing that leads to optimal social welfare. Previous work by Cohen-Addad et al. [EC'16] demonstrated the existence of optimal dynamic prices for unit-demand buyers, and showed a market with coverage valuations that admits no such prices. However, finding the frontier of markets (i.e., valuation functions) that admit optimal dynamic prices remains an open problem. In this work we establish positive and negative results that narrow the existing gap. On the positive side, we provide tools for handling markets beyond unit-demand valuations. In particular, we characterize all optimal allocations in multi-demand markets. This characterization allows us to partition the items into equivalence classes according to the role they play in achieving optimality. Using these tools, we provide a poly-time optimal dynamic pricing algorithm for up to 3 multi-demand buyers. On the negative side, we establish a maximal domain theorem, showing that for every non-gross substitutes valuation, there exist unit-demand valuations such that adding them yields a market that does not admit an optimal dynamic pricing. This result is reminiscent of the seminal maximal domain theorem by Gul and Stacchetti [JET'99] for Walrasian equilibrium. Yang [JET'17] discovered an error in their original proof, and established a different, incomparable version of their maximal domain theorem. En route to our maximal domain theorem for optimal dynamic pricing, we provide the first complete proof of the original theorem by Gul and Stacchetti.", "label": 1, "source": "scigen_human", "idx": 28, "lang": "en"}
{"text": "We study robust PCA for the fully observed setting, which is about separating a low rank matrix L and a sparse matrix S from their sum D L S. In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which significantly improves the computational efficiency of the existing alternating projections proposed in when updating the low rank factor. The acceleration is achieved by first projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-the-art algorithms for robust PCA.", "label": 1, "source": "scigen_human", "idx": 29, "lang": "en"}
{"text": "Neural program embedding has shown potential in aiding the analysis of large-scale, complicated software. Newly proposed deep neural architectures pride themselves on learning program semantics rather than superficial syntactic features. However, by considering the source code only, the vast majority of neural networks do not capture a deep, precise representation of program semantics. In this paper, we present DyPro, a novel deep neural network that learns from program execution traces. Compared to the prior dynamic models, not only is DyPro capable of generalizing across multiple executions for learning a program's dynamic semantics in its entirety, but DyPro is also more efficient when dealing with programs yielding long execution traces. For evaluation, we task DyPro with semantics classification (i.e. categorizing programs based on their semantics) and compared it against two prominent static models: Gated Graph Neural Network and TreeLSTM. We find that DyPro achieves the highest prediction accuracy among all models. To further reveal the capacity of all aforementioned deep neural architectures, we examine if the models can learn to detect deeper semantic properties of a program. In particular given a task of recognizing loop invariants, we find that DyPro outperforms all static models by a wide margin.", "label": 1, "source": "scigen_human", "idx": 31, "lang": "en"}
{"text": "As inertial and visual sensors are becoming ubiquitous, visual-inertial navigation systems (VINS) have prevailed in a wide range of applications from mobile augmented reality to aerial navigation to autonomous driving, in part because of the complementary sensing capabilities and the decreasing costs and size of the sensors. In this paper, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work - which is unfortunately missing in the literature while being greatly demanded by researchers and engineers - in the hope to accelerate the VINS research and beyond in our society as a whole.", "label": 1, "source": "scigen_human", "idx": 32, "lang": "en"}
{"text": "Matrix Product States (MPS), also known as Tensor Train (TT) decomposition in mathematics, has been proposed originally for describing an (especially one-dimensional) quantum system, and recently has found applications in various applications such as compressing high-dimensional data, supervised kernel linear classifier, and unsupervised generative modeling. However, when applied to systems which are not defined on one-dimensional lattices, a serious drawback of the MPS is the exponential decay of the correlations, which limits its power in capturing long-range dependences among variables in the system. To alleviate this problem, we propose to introduce long-range interactions, which act as shortcuts, to MPS, resulting in a new model Shortcut Matrix Product States (SMPS). When chosen properly, the shortcuts can decrease significantly the correlation length of the MPS, while preserving the computational efficiency. We develop efficient training methods of SMPS for various tasks, establish some of their mathematical properties, and show how to find a good location to add shortcuts. Finally, using extensive numerical experiments we evaluate its performance in a variety of applications, including function fitting, partition function calculation of 2 - d Ising model, and unsupervised generative modeling of handwritten digits, to illustrate its advantages over vanilla matrix product states.", "label": 1, "source": "scigen_human", "idx": 33, "lang": "en"}
{"text": "Most of the current action recognition algorithms are based on deep networks which stack multiple convolutional, pooling and fully connected layers. While convolutional and fully connected operations have been widely studied in the literature, the design of pooling operations that handle action recognition, with different sources of temporal granularity in action categories, has comparatively received less attention, and existing solutions rely mainly on max or averaging operations. The latter are clearly powerless to fully exhibit the actual temporal granularity of action categories and thereby constitute a bottleneck in classification performances. In this paper, we introduce a novel hierarchical pooling design that captures different levels of temporal granularity in action recognition. Our design principle is coarse-to-fine and achieved using a tree-structured network; as we traverse this network top-down, pooling operations are getting less invariant but timely more resolute and well localized. Learning the combination of operations in this network - which best fits a given ground-truth - is obtained by solving a constrained minimization problem whose solution corresponds to the distribution of weights that capture the contribution of each level (and thereby temporal granularity) in the global hierarchical pooling process. Besides being principled and well grounded, the proposed hierarchical pooling is also video-length and resolution agnostic. Extensive experiments conducted on the challenging UCF-101, HMDB-51 and JHMDB-21 databases corroborate all these statements.", "label": 1, "source": "scigen_human", "idx": 34, "lang": "en"}
{"text": "The problem of mesh matching is addressed in this work. For a given n -sided planar region bounded by one loop of n polylines we are selecting optimal quadrilateral mesh from existing catalogue of meshes. The formulation of matching between planar shape and quadrilateral mesh from the catalogue is based on the problem of finding longest common subsequence (LCS). Theoretical foundation of mesh matching method is provided. Suggested method represents a viable technique for selecting best mesh for planar region and stepping stone for further parametrization of the region.", "label": 1, "source": "scigen_human", "idx": 35, "lang": "en"}
{"text": "Partially answering a question of Paul Seymour, we obtain a sufficient eigenvalue condition for the existence of k edge-disjoint spanning trees in a regular graph, when k {2, 3 }. More precisely, we show that if the second largest eigenvalue of a d -regular graph G is less than - d - 2 k 1 d 1, then G contains at least k edge-disjoint spanning trees, when k {2, 3 }. We construct examples of graphs that show our bounds are essentially best possible. We conjecture that the above statement is true for any k d 2.", "label": 1, "source": "scigen_human", "idx": 36, "lang": "en"}
{"text": "For mobile robots navigating on sidewalks, it is essential to be able to safely cross street intersections. Most existing approaches rely on the recognition of the traffic light signal to make an informed crossing decision. Although these approaches have been crucial enablers for urban navigation, the capabilities of robots employing such approaches are still limited to navigating only on streets that contain signalized intersections. In this paper, we address this challenge and propose a multimodal convolutional neural network framework to predict the safety of a street intersection for crossing. Our architecture consists of two subnetworks; an interaction-aware trajectory estimation stream IA-TCNN, that predicts the future states of all observed traffic participants in the scene, and a traffic light recognition stream AtteNet. Our IA-TCNN utilizes dilated causal convolutions to model the behavior of all the observable dynamic agents in the scene without explicitly assigning priorities to the interactions among them. While AtteNet utilizes Squeeze-Excitation blocks to learn a content-aware mechanism for selecting the relevant features from the data, thereby improving the noise robustness. Learned representations from the traffic light recognition stream are fused with the estimated trajectories from the motion prediction stream to learn the crossing decision. Incorporating the uncertainty information from both modules enables our architecture to learn a likelihood function that is robust to noise and mispredictions from either subnetworks. Simultaneously, by learning to estimate motion trajectories of the surrounding traffic participants and incorporating knowledge of the traffic light signal, our network learns a robust crossing procedure that is invariant to the type of street intersection. Furthermore, we extend our previously introduced Freiburg Street Crossing dataset with sequences captured at multiple intersections of varying types, demonstrating complex interactions among the traffic participants as well as various lighting and weather conditions. We perform comprehensive experimental evaluations on public datasets as well as our Freiburg Street Crossing dataset, which demonstrate that our network achieves state-of-the-art performance for each of the subtasks, as well as for the crossing safety prediction. Moreover, we deploy the proposed architectural framework on a robotic platform and conduct real-world experiments which demonstrate the suitability of the approach for real-time deployment and robustness to various environments.", "label": 1, "source": "scigen_human", "idx": 37, "lang": "en"}
{"text": "We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: we combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.", "label": 1, "source": "scigen_human", "idx": 38, "lang": "en"}
{"text": "We present a novel algorithm for instrumental variable (IV) regression, DualIV, which simplifies traditional two-stage methods via a dual formulation. Inspired by problems in stochastic programming, we show that the two-stage procedure for nonlinear IV regression can be reformulated as a convex-concave saddle-point problem. Our formulation circumvents the first-stage regression which is a potential bottleneck in real-world applications. Based on this new approach, we develop a simple kernel-based algorithm with a closed-form solution. Empirical results show that we are competitive to existing, more complicated algorithms for instrumental variable regression.", "label": 1, "source": "scigen_human", "idx": 39, "lang": "en"}
{"text": "We consider data transmission over a network where each edge is an erasure channel and where the inner nodes transmit a random linear combination of their incoming information. We distinguish two channel models in this setting, the row and the column erasure channel model. For both models we derive the symbol erasure correction capabilities of spread codes and compare them to other known codes suitable for those models. Furthermore, we explain how to decode these codes in the two channel models and compare their decoding complexities. The results show that, depending on the application and the to-be-optimized aspect, any combination of codes and channel models can be the best choice.", "label": 1, "source": "scigen_human", "idx": 40, "lang": "en"}
{"text": "Move blocking (MB) is a widely used strategy to reduce the degrees of freedom of the Optimal Control Problem (OCP) arising in receding horizon control. The size of the OCP is reduced by forcing the input variables to be constant over multiple discretization steps. In this paper, we focus on developing computationally efficient MB schemes for multiple shooting based nonlinear model predictive control (NMPC). The degrees of freedom of the OCP is reduced by introducing MB in the shooting step, resulting in a smaller but sparse OCP. Therefore, the discretization accuracy and level of sparsity is maintained. A condensing algorithm that exploits the sparsity structure of the OCP is proposed, that allows to reduce the computation complexity of condensing from quadratic to linear in the number of discretization nodes. As a result, active-set methods with warm-start strategy can be efficiently employed, thus allowing the use of a longer prediction horizon. A detailed comparison between the proposed scheme and the nonuniform grid NMPC is given. Effectiveness of the algorithm in reducing computational burden while maintaining optimization accuracy and constraints fulfillment is shown by means of simulations with two different problems.", "label": 1, "source": "scigen_human", "idx": 41, "lang": "en"}
{"text": "Legged robots traversing in confined environments could find their only path is blocked by obstacles. In circumstances where the obstacles are movable, a multilegged robot can manipulate the obstacles using its legs to allow it to continue on its path. We present a method for a hexapod robot to autonomously generate manipulation trajectories for detected obstacles. Using a RGB-D sensor as input, the obstacle is extracted from the environment and filtered to provide key contact points for the manipulation algorithm to calculate a trajectory to move the obstacle out of the path. Experiments on a 30 degree of freedom hexapod robot show the effectiveness of the algorithm in manipulating a range of obstacles in a 3D environment using its front legs.", "label": 1, "source": "scigen_human", "idx": 42, "lang": "en"}
{"text": "We give an approximate formula of the distribution of the largest eigenvalue of real Wishart matrices by the expected Euler characteristic method for the general dimension. The formula is expressed in terms of a definite integral with parameters. We derive a differential equation satisfied by the integral for the x 2 2 matrix case and perform a numerical analysis of it.", "label": 1, "source": "scigen_human", "idx": 43, "lang": "en"}
{"text": "We introduce novel dynamic oracles for training two of the most accurate known shift-reduce algorithms for constituent parsing: the top-down and in-order transition-based parsers. In both cases, the dynamic oracles manage to notably increase their accuracy, in comparison to that obtained by performing classic static training. In addition, by improving the performance of the state-of-the-art in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained by a fully-supervised single-model greedy shift-reduce constituent parser on the WSJ benchmark.", "label": 1, "source": "scigen_human", "idx": 44, "lang": "en"}
{"text": "Feature extraction from financial data is one of the most important problems in market prediction domain for which many approaches have been suggested. Among other modern tools, convolutional neural networks (CNN) have recently been applied for automatic feature selection and market prediction. However, in experiments reported so far, less attention has been paid to the correlation among different markets as a possible source of information for extracting features. In this paper, we suggest a CNN-based framework with specially designed CNNs, that can be applied on a collection of data from a variety of sources, including different markets, in order to extract features for predicting the future of those markets. The suggested framework has been applied for predicting the next day's direction of movement for the indices of SP 500, NASDAQ, DJI, NYSE, and RUSSELL markets based on various sets of initial features. The evaluations show a significant improvement in prediction's performance compared to the state of the art baseline algorithms.", "label": 1, "source": "scigen_human", "idx": 45, "lang": "en"}
{"text": "Skills like computational thinking, problem solving, handling complexity, team-work and project management are essential for future careers and needs to be taught to students at the elementary level itself. Computer programming knowledge and skills, experiencing technology and conducting science and engineering experiments are also important for students at elementary level. However, teaching such skills effectively through active learning can be challenging for educators. In this paper, we present our approach and experiences in teaching such skills to several elementary level children using Lego Mindstorms EV3 robotics education kit. We describe our learning environment consisting of lessons, worksheets, hands-on activities and assessment. We taught students how to design, construct and program robots using components such as motors, sensors, wheels, axles, beams, connectors and gears. Students also gained knowledge on basic programming constructs such as control flow, loops, branches and conditions using a visual programming environment. We carefully observed how students performed various tasks and solved problems. We present experimental results which demonstrates that our teaching methodology consisting of both the course content and pedagogy was effective in imparting the desired skills and knowledge to elementary level children. The students also participated in a competitive World Robot Olympiad India event and qualified during the regional round which is an evidence of the effectiveness of the approach.", "label": 1, "source": "scigen_human", "idx": 46, "lang": "en"}
{"text": "This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab , Python , R etc. are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples for 1D (dummy) spatial data.", "label": 1, "source": "scigen_human", "idx": 47, "lang": "en"}
{"text": "Automotive companies increasingly adopt scaled agile methods to allow them to deal with their organisational and product complexity. Suitable methods are needed to ensure safety when developing automotive systems. On a small scale, R-Scrum and SafeScrum are two concrete suggestions for how to develop safety-critical systems using agile methods. However, for large-scale environments, existing frameworks like SAFe or LeSS do not support the development of safety-critical systems out of the box. We, therefore, aim to understand which challenges exist when developing safety-critical systems within large-scale agile industrial settings, in particular in the automotive domain. Based on an analysis of R-Scrum and SafeScrum , we conducted a focus group with three experts from industry to collect challenges in their daily work. We found challenges in the areas of living traceability, continuous compliance, and organisational flexibility. Among others, organisations struggle with defining a suitable traceability strategy, performing incremental safety analysis, and with integrating safety practices into their scaled way of working. Our results indicate a need to provide practical approaches to integrate safety work into large-scale agile development and point towards possible solutions, e.g., modular safety cases.", "label": 1, "source": "scigen_human", "idx": 48, "lang": "en"}
{"text": "The discriminator from generative adversarial nets (GAN) has been used by some researchers as a feature extractor in transfer learning and appeared worked well. However, there are also some studies that believe this is the wrong research direction because intuitively the task of the discriminator focuses on separating the real samples from the generated ones, making features extracted in this way useless for most of the downstream tasks. To avoid this dilemma, we first conducted a thorough theoretical analysis of the relationship between the discriminator task and the characteristics of the features extracted. We found that the connection between the task of the discriminator and the feature is not as strong as was thought, for that the main factor restricting the feature learned by the discriminator is not the task of the discriminator itself, but the need to prevent the entire GAN model from mode collapse during the training. From this perspective and combined with further analyses, we found that to avoid mode collapse in the training process of GAN, the features extracted by the discriminator are not guided to be different for the real samples, but divergence without noise is indeed allowed and occupies a large proportion of the feature space. This makes the features learned more robust and helps answer the question as to why the discriminator can succeed as a feature extractor in related research. Consequently, to expose the essence of the discriminator extractor as different from other extractors, we analyze the counterpart of the discriminator extractor, the classifier extractor that assigns the target samples to different categories. We found the performance of the discriminator extractor may be inferior to the classifier based extractor when the source classification task is similar to the target task, which is the common case, but the ability to avoid noise prevents the discriminator from being replaced by the classifier. Last but not least, as our research also revealed a ratio playing an important role in GAN's training to prevent mode collapse, it contributes to the basic GAN study.", "label": 1, "source": "scigen_human", "idx": 49, "lang": "en"}
{"text": "Modern pattern recognition methods are based on convolutional networks since they are able to learn complex patterns that benefit the classification. However, convolutional networks are computationally expensive and require a considerable amount of memory, which limits their deployment on low-power and resource-constrained systems. To handle these problems, recent approaches have proposed pruning strategies that find and remove unimportant neurons (i.e., filters) in these networks. Despite achieving remarkable results, existing pruning approaches are ineffective since the accuracy of the original network is degraded. In this work, we propose a novel approach to efficiently remove filters from convolutional networks. Our approach estimates the filter importance based on its relationship with the class label on a low-dimensional space. This relationship is computed using Partial Least Squares (PLS) and Variable Importance in Projection (VIP). Our method is able to reduce up to 67 of the floating point operations (FLOPs) without penalizing the network accuracy. With a negligible drop in accuracy, we can reduce up to 90 of FLOPs. Additionally, sometimes the method is even able to improve the accuracy compared to original, unpruned, network. We show that employing PLSVIP as the criterion for detecting the filters to be removed is better than recent feature selection techniques, which have been employed by state-of-the-art pruning methods. Finally, we show that the proposed method achieves the highest FLOPs reduction and the smallest drop in accuracy when compared to state-of-the-art pruning approaches. Codes are available at:", "label": 1, "source": "scigen_human", "idx": 50, "lang": "en"}
{"text": "We consider an extension of the massive unsourced random access originally proposed in to the case where the receiver has a very large number of antennas (a massive MIMO base station) and no channel state information is given to the receiver (fully non-coherent detection). Our coding approach borrows the concatenated coding idea from, combined with a novel non-Bayesian \"activity detection\" algorithm for massive MIMO random access channels, that outperforms currently proposed Bayesian vector AMP (VAMP) schemes currently proposed for activity detection, and does not suffer from the numerical instabilities and requirement for accurate a priori statistics as VAMP. We show that the required transmit E b N 0 for reliable communication can be made arbitrarily small as the number of receiver antennas M grows sufficiently large.", "label": 1, "source": "scigen_human", "idx": 51, "lang": "en"}
{"text": "We consider the problem of fitting variational posterior approximations using stochastic optimization methods. The performance of these approximations depends on (1) how well the variational family matches the true posterior distribution, (2) the choice of divergence, and (3) the optimization of the variational objective. We show that even in the best-case scenario when the exact posterior belongs to the assumed variational family, common stochastic optimization methods lead to poor variational approximations if the problem dimension is moderately large. We also demonstrate that these methods are not robust across diverse model types. Motivated by these findings, we develop a more robust and accurate stochastic optimization framework by viewing the underlying optimization algorithm as producing a Markov chain. Our approach is theoretically motivated and includes a diagnostic for convergence and a novel stopping rule, both of which are robust to noisy evaluations of the objective function. We show empirically that the proposed framework works well on a diverse set of models: it can automatically detect stochastic optimization failure or inaccurate variational approximation.", "label": 1, "source": "scigen_human", "idx": 52, "lang": "en"}
{"text": "Bitcoin introduced delegation of control over a monetary system from a select few to all who participate in that system. This delegation is known as the decentralization of controlling power and is a powerful security mechanism for the ecosystem. After the introduction of Bitcoin, the field of cryptocurrency has seen widespread attention from industry and academia, so much so that the original novel contribution of Bitcoin, i.e., decentralization, may be overlooked, due to decentralizations' assumed fundamental existence for the functioning of such crypto-assets. However, recent studies have observed a trend of increased centralization in cryptocurrencies such as Bitcoin and Ethereum. As this increased centralization has an impact the security of the blockchain, it is crucial that it is measured, towards adequate control. This research derives an initial taxonomy of centralization present in decentralized blockchains through rigorous synthesis using a systematic literature review. This is followed by iterative refinement through expert interviews. We systematically analyzed 89 research papers published between 2009 and 2019. Our study contributes to the existing body of knowledge by highlighting the multiple definitions and measurements of centralization in the literature. We identify different aspects of centralization and propose an encompassing taxonomy of centralization concerns. This taxonomy is based on empirically observable and measurable characteristics. It consists of 13 aspects of centralization, classified over six architectural layers: Governance, Network, Consensus, Incentive, Operational, and Application. We also discuss how the implications of centralization can vary depending on the aspects studied. We believe that this review and taxonomy provides a comprehensive overview of centralization in decentralized blockchains involving various conceptualizations and measures.", "label": 1, "source": "scigen_human", "idx": 53, "lang": "en"}
{"text": "The main limitation of visible light communication (VLC) is the narrow modulation bandwidth, which reduces the achievable data rates. In this paper, we apply the non-orthogonal multiple access (NOMA) scheme to enhance the achievable throughput in high-rate VLC downlink networks. We first propose a novel gain ratio power allocation (GRPA) strategy that takes into account the users' channel conditions to ensure efficient and fair power allocation. Our results indicate that GRPA significantly enhances system performance compared to the static power allocation. We also study the effect of tuning the transmission angles of the light emitting diodes (LEDs) and the field of views (FOVs) of the receivers, and demonstrate that these parameters can offer new degrees of freedom to boost NOMA performance. Simulation results reveal that NOMA is a promising multiple access scheme for the downlink of VLC networks.", "label": 1, "source": "scigen_human", "idx": 54, "lang": "en"}
{"text": "This paper develops a mechanical tool as well as its manipulation policies for 2-finger parallel robotic grippers. It primarily focuses on a mechanism that converts the gripping motion of 2-finger parallel grippers into a continuous rotation to realize tasks like fastening screws. The essential structure of the tool comprises a Scissor-Like Element (SLE) mechanism and a double-ratchet mechanism. They together convert repeated linear motion into continuous rotating motion. At the joints of the SLE mechanism, elastic elements are attached to provide resisting force for holding the tool as well as for producing torque output when a gripper releases the tool. The tool is entirely mechanical, allowing robots to use the tool without any peripherals and power supply. The paper presents the details of the tool design, optimizes its dimensions and effective stroke lengths, and studies the contacts and forces to achieve stable grasping and screwing. Besides the design, the paper develops manipulation policies for the tool. The policies include visual recognition, picking-up and manipulation, and exchanging tooltips. The developed tool produces clockwise rotation at the front end and counter-clockwise rotation at the back end. Various tooltips can be installed at both two ends. Robots may employ the developed manipulation policies to exchange the tooltips and rotating directions following the needs of specific fastening or loosening tasks. Robots can also reorient the tool using pick-and-place or handover, and move the tool to work poses using the policies. The designed tool, together with the developed manipulation policies, are analyzed and verified in several real-world applications. The tool is small, cordless, convenient, and has good robustness and adaptability.", "label": 1, "source": "scigen_human", "idx": 55, "lang": "en"}
{"text": "As of September 2020, the COVID-19 pandemic continues to devastate the health and well-being of the global population. With more than 33 million confirmed cases and over a million deaths, global health organizations are still a long way from fully containing the pandemic. This pandemic has raised serious questions about the emergency preparedness of health agencies, not only in terms of treatment of an unseen disease, but also in identifying its early symptoms. In the particular case of COVID-19, several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such as COVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of infected peoples' data could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate this problem within a one-class classification framework, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present COVIDomaly, a convolutional autoencoder framework to detect unseen COVID-19 cases from the chest radiographs. We tested two settings on a publicly available dataset (COVIDx) by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. After performing 3-fold cross validation, we obtain a pooled ROC-AUC of 0.7652 and 0.6902 in the two settings respectively. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays.", "label": 1, "source": "scigen_human", "idx": 58, "lang": "en"}
{"text": "While first-order optimization methods such as stochastic gradient descent (SGD) are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, in contrast to SGD with momentum, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.", "label": 1, "source": "scigen_human", "idx": 59, "lang": "en"}
{"text": "This paper proposes a parallel optimization algorithm for cooperative automation of large-scale connected vehicles. The task of cooperative automation is formulated as a centralized optimization problem taking the whole decision space of all vehicles into account. Considering the uncertainty of the environment, the problem is solved in a receding horizon fashion. Then, we employ the alternating direction method of multipliers (ADMM) to solve the centralized optimization in a parallel way, which scales more favorably to large-scale instances. Also, Taylor series is used to linearize nonconvex constraints caused by coupling collision avoidance constraints among interactive vehicles. Simulations with two typical traffic scenes for multiple vehicles demonstrate the effectiveness and efficiency of our method.", "label": 1, "source": "scigen_human", "idx": 60, "lang": "en"}
{"text": "This essay argues that a new form of democracy - an \"Emergent Democracy\" - will develop as a result of the use of Internet communication tools and platforms such as blogs. The essay explores a variety of tools available and explores the history of democracy, modern experiments with democracy and how these tools might support democracy. The essay also explores concerns as these new tools emerge. These issues include concerns such as privacy and the societally negative use of these tools by corporations, totalitarian regimes and terrorists.", "label": 1, "source": "scigen_human", "idx": 62, "lang": "en"}
{"text": "Segregating an audio mixture containing multiple simultaneous bird sounds is a challenging task. However, birdsong often contains rapid pitch modulations, and these modulations carry information which may be of use in automatic recognition. In this paper we demonstrate that an improved spectrogram representation, based on the distribution derivative method, leads to improved performance of a segregation algorithm which uses a Markov renewal process model to track vocalisation patterns consisting of singing and silences.", "label": 1, "source": "scigen_human", "idx": 63, "lang": "en"}
{"text": "Music recommender systems (MRS) have experienced a boom in recent years, thanks to the emergence and success of online streaming services, which nowadays make available almost all music in the world at the user's fingertip. While today's MRS considerably help users to find interesting music in these huge catalogs, MRS research is still facing substantial challenges. In particular when it comes to build, incorporate, and evaluate recommendation strategies that integrate information beyond simple user-item interactions or content-based descriptors, but dig deep into the very essence of listener needs, preferences, and intentions, MRS research becomes a big endeavor and related publications quite sparse. The purpose of this trends and survey article is twofold. We first identify and shed light on what we believe are the most pressing challenges MRS research is facing, from both academic and industry perspectives. We review the state of the art towards solving these challenges and discuss its limitations. Second, we detail possible future directions and visions we contemplate for the further evolution of the field. The article should therefore serve two purposes: giving the interested reader an overview of current challenges in MRS research and providing guidance for young researchers by identifying interesting, yet under-researched, directions in the field.", "label": 1, "source": "scigen_human", "idx": 64, "lang": "en"}
{"text": "Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to \"drinking from the firehose,\" as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains. In this work we present CacheOut, a new microarchitectural attack that is capable of bypassing Intel's buffer overwrite countermeasures. We observe that as data is being evicted from the CPU's L1 cache, it is often transferred back to the leaky CPU buffers where it can be recovered by the attacker. CacheOut improves over previous MDS attacks by allowing the attacker to choose which data to leak from the CPU's L1 cache, as well as which part of a cache line to leak. We demonstrate that CacheOut can leak information across multiple security boundaries, including those between processes, virtual machines, user and kernel space, and from SGX enclaves.", "label": 1, "source": "scigen_human", "idx": 65, "lang": "en"}
{"text": "This work introduces Conditional Image Retrieval (CIR) systems: IR methods that can efficiently specialize to specific subsets of images on the fly. These systems broaden the class of queries IR systems support, and eliminate the need for expensive re-fitting to specific subsets of data. Specifically, we adapt tree-based K-Nearest Neighbor (KNN) data-structures to the conditional setting by introducing additional inverted-index data-structures. This speeds conditional queries and does not slow queries without conditioning. We present two new datasets for evaluating the performance of CIR systems and evaluate a variety of design choices. As a motivating application, we present an algorithm that can explore shared semantic content between works of art of vastly different media and cultural origin. Finally, we demonstrate that CIR data-structures can identify Generative Adversarial Network (GAN) \"blind spots\": areas where GANs fail to properly model the true data distribution.", "label": 1, "source": "scigen_human", "idx": 66, "lang": "en"}
{"text": "We introduce a general and simple structural design called \"Multiplicative Integration\" (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.", "label": 1, "source": "scigen_human", "idx": 67, "lang": "en"}
{"text": "With the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the COVID-19 pandemic, there is a growing interest in finding solutions to help address the problem. A solution to this problem would be to use neurotechnology to provide them augmented cognition, senses and action for optimal diagnosis and treatment. Consequently, doing so can negatively impact them and others. We argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices, and harm to them and patients. In this paper, we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons. We will then review selected ethical concerns discussed within literature, discuss the neuroengineering behind using neurotechnology for augmentation purposes, then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice.", "label": 1, "source": "scigen_human", "idx": 68, "lang": "en"}
{"text": "In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is 2.8 x larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset.", "label": 1, "source": "scigen_human", "idx": 69, "lang": "en"}
{"text": "We introduce the new task of Acoustic Question Answering (AQA) to promote research in acoustic reasoning. The AQA task consists of analyzing an acoustic scene composed by a combination of elementary sounds and answering questions that relate the position and properties of these sounds. The kind of relational questions asked, require that the models perform non-trivial reasoning in order to answer correctly. Although similar problems have been extensively studied in the domain of visual reasoning, we are not aware of any previous studies addressing the problem in the acoustic domain. We propose a method for generating the acoustic scenes from elementary sounds and a number of relevant questions for each scene using templates. We also present preliminary results obtained with two models (FiLM and MAC) that have been shown to work for visual reasoning.", "label": 1, "source": "scigen_human", "idx": 70, "lang": "en"}
{"text": "A posteriori error estimates are constructed for the three-field variational formulation of the Biot problem involving the displacements, the total pressure and the fluid pressure. The discretization under focus is the H 1 (O) -conforming Taylor-Hood finite element combination, consisting of polynomial degrees k 1 for the displacements and the fluid pressure and k for the total pressure. An a posteriori error estimator is derived on the basis of H (div) -conforming reconstructions of the stress and flux approximations. The symmetry of the reconstructed stress is allowed to be satisfied only weakly. The reconstructions can be performed locally on a set of vertex patches and lead to a guaranteed upper bound for the error with a constant that depends only on local constants associated with the patches and thus on the shape regularity of the triangulation. Particular emphasis is given to nearly incompressible materials and the error estimates hold uniformly in the incompressible limit. Numerical results on the L-shaped domain confirm the theory and the suitable use of the error estimator in adaptive strategies.", "label": 1, "source": "scigen_human", "idx": 71, "lang": "en"}
{"text": "We propose to classify the power of algorithms by the complexity of the problems that they can be used to solve. Instead of restricting to the problem a particular algorithm was designed to solve explicitly, however, we include problems that, with polynomial overhead, can be solved ' implicitly'during the algorithm's execution. For example, we allow to solve a decision problem by suitably transforming the input, executing the algorithm, and observing whether a specific bit in its internal configuration ever switches during the execution. We show that the Simplex Method, the Network Simplex Method (both with Dantzig's original pivot rule), and the Successive Shortest Path Algorithm are NP-mighty, that is, each of these algorithms can be used to solve any problem in NP. This result casts a more favorable light on these algorithms' exponential worst-case running times. Furthermore, as a consequence of our approach, we obtain several novel hardness results. For example, for a given input to the Simplex Algorithm, deciding whether a given variable ever enters the basis during the algorithm's execution and determining the number of iterations needed are both NP-hard problems. Finally, we close a long-standing open problem in the area of network flows over time by showing that earliest arrival flows are NP-hard to obtain.", "label": 1, "source": "scigen_human", "idx": 72, "lang": "en"}
{"text": "Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website:", "label": 1, "source": "scigen_human", "idx": 73, "lang": "en"}
{"text": "Deep neuroevolution, that is evolutionary policy search methods based on deep neural networks, have recently emerged as a competitor to deep reinforcement learning algorithms due to their better parallelization capabilities. However, these methods still suffer from a far worse sample efficiency. In this paper we investigate whether a mechanism known as \"importance mixing\" can significantly improve their sample efficiency. We provide a didactic presentation of importance mixing and we explain how it can be extended to reuse more samples. Then, from an empirical comparison based on a simple benchmark, we show that, though it actually provides better sample efficiency, it is still far from the sample efficiency of deep reinforcement learning, though it is more stable.", "label": 1, "source": "scigen_human", "idx": 74, "lang": "en"}
{"text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to maintain social connections. In this online digital world, people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations. Thus, social media has established itself to bear signals relating to human behavior. One can easily design user characteristic network by scraping through someone's social media profiles. In this paper, we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social, psychological and linguistic behavior as evident from the content generated by them. Our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters. We use Twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78 with very high precision and recall. Note that such an automatic classification can have far-reaching impact - (i) on health research related to addiction prevention and control, and (ii) in eliminating abusive and vulgar contents from Twitter, borne by the tweets of drunk texters.", "label": 1, "source": "scigen_human", "idx": 75, "lang": "en"}
{"text": "Wireless Sensor Networks (WSNs) with their dynamic applications gained a tremendous attention of researchers. Constant monitoring of critical situations attracted researchers to utilize WSNs at vast platforms. The main focus in WSNs is to enhance network life-time as much as one could, for efficient and optimal utilization of resources. Different approaches based upon clustering are proposed for optimum functionality. Network life-time is always related with energy of sensor nodes deployed at remote areas for constant and fault tolerant monitoring. In this work, we propose Quadrature-LEACH (Q-LEACH) for homogenous networks which enhances stability period, network life-time and throughput quiet significantly.", "label": 1, "source": "scigen_human", "idx": 76, "lang": "en"}
{"text": "This work investigates the consensus problem for multi-agent nonlinear systems through the distributed real-time nonlinear receding horizon control methodology. With this work, we develop a scheme to reach the consensus for nonlinear multi agent systems under fixed directedundirected graph (s) without the need of any linearization techniques. For this purpose, the problem of consensus is converted into an optimization problem and is directly solved by the backwards sweep Riccati method to generate the control protocol which results in a non-iterative algorithm. Stability analysis is conducted to provide convergence guarantees of proposed scheme. In addition, an extension to the leader-following consensus of nonlinear multi-agent systems is presented. Several examples are provided to validate and demonstrate the effectiveness of the presented scheme and the corresponding theoretical results.", "label": 1, "source": "scigen_human", "idx": 77, "lang": "en"}
{"text": "Variational Auto-Encoders have often been used for unsupervised pretraining, feature extraction and out-of-distribution and anomaly detection in the medical field. However, VAEs often lack the ability to produce sharp images and learn high-level features. We propose to alleviate these issues by adding a new branch to conditional hierarchical VAEs. This enforces a division between higher-level and lower-level features. Despite the additional computational overhead compared to a normal VAE it results in sharper and better reconstructions and can capture the data distribution similarly well (indicated by a similar or slightly better OoD detection performance).", "label": 1, "source": "scigen_human", "idx": 78, "lang": "en"}
{"text": "New cryptographic techniques such as homomorphic encryption (HE) allow computations to be outsourced to and evaluated blindfolded in a resourceful cloud. These computations often require private data owned by multiple participants, engaging in joint evaluation of some functions. For example, Genome-Wide Association Study (GWAS) is becoming feasible because of recent proliferation of genome sequencing technology. Due to the sensitivity of genomic data, these data should be encrypted using different keys. However, supporting computation on ciphertexts encrypted under multiple keys is a non-trivial task. In this paper, we present a comprehensive survey on different state-of-the-art cryptographic techniques and schemes that are commonly used. We review techniques and schemes including Attribute-Based Encryption (ABE), Proxy Re-Encryption (PRE), Threshold Homomorphic Encryption (ThHE), and Multi-Key Homomorphic Encryption (MKHE). We analyze them based on different system and security models, and examine their complexities. We share lessons learned and draw observations for designing better schemes with reduced overheads.", "label": 1, "source": "scigen_human", "idx": 79, "lang": "en"}
{"text": "Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.", "label": 1, "source": "scigen_human", "idx": 80, "lang": "en"}
{"text": "Linear logic and the linear l -calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on AEthel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear l -calculus with an accuracy of as high as 70.", "label": 1, "source": "scigen_human", "idx": 81, "lang": "en"}
{"text": "To support a freight carrier in a combinatorial transport auction, we proposes an exact and two heuristic strategies for bidding on subsets of requests. The exact bidding strategy is based on the concept of elementary request combinations. We show that it is sufficient and necessary for a carrier to bid on each elementary request combination in order to guarantee the same result as bidding on each element of the powerset of the set of tendered requests. Both heuristic bidding strategies identify promising request combinations. For this, pairwise synergies based on saving values as well as the capacitated p-median problem are used. The bidding strategies are evaluated by a computational study that simulates an auction. It is based on 174 benchmark instances and therefore easily extendable by other researchers. On average, the two heuristic strategies achieve 91 percent and 81 percent of the available sales potential while generating 36 and only 4 percent of the bundle bids of the exact strategy. Therefore, the proposed bidding strategies help a carrier to increase her chance to win and at the same time reduce the computational burden to participate in a combinatorial transport auction.", "label": 1, "source": "scigen_human", "idx": 82, "lang": "en"}
{"text": "An innovative 3-D radar imaging technique is developed for fast and efficient identification and characterization of radar backscattering components of complex objects, when the collected scattered field is made of polarization-diverse measurements. In this context, all the polarimetric information seems irretrievably mixed. A direct model, derived from a simple but original extension of the widespread \"multiple scattering model\" leads to a high dimensional linear inverse problem. It is solved by a fast dedicated imaging algorithm that performs to determine at a time three huge 3-D scatterer maps which correspond to HH, VV and HV polarizations at emission and reception. It is applied successfully to various mock-ups and data sets collected from an accurate and dedicated 3D spherical experimental layout that provides concentric polarization-diverse RCS measurements.", "label": 1, "source": "scigen_human", "idx": 83, "lang": "en"}
{"text": "We consider the age-old problem of allocating items among different agents in a way that is efficient and fair. Two papers, by Dolev et al. and Ghodsi et al., have recently studied this problem in the context of computer systems. Both papers had similar models for agent preferences, but advocated different notions of fairness. We formalize both fairness notions in economic terms, extending them to apply to a larger family of utilities. Noting that in settings with such utilities efficiency is easily achieved in multiple ways, we study notions of fairness as criteria for choosing between different efficient allocations. Our technical results are algorithms for finding fair allocations corresponding to two fairness notions: Regarding the notion suggested by Ghodsi et al., we present a polynomial-time algorithm that computes an allocation for a general class of fairness notions, in which their notion is included. For the other, suggested by Dolev et al., we show that a competitive market equilibrium achieves the desired notion of fairness, thereby obtaining a polynomial-time algorithm that computes such a fair allocation and solving the main open problem raised by Dolev et al.", "label": 1, "source": "scigen_human", "idx": 84, "lang": "en"}
{"text": "Retrieving videos of a particular person with face image as query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing face videos with some robust set modeling techniques (e.g. covariance matrices as exploited in this study, which reside on Riemannian manifold), has recently shown appealing advantages. This hence results in a thorny heterogeneous spaces matching problem. Moreover, hashing with handcrafted features as done in many existing works is clearly inadequate to achieve desirable performance for this task. To address such problems, we present an end-to-end Deep Heterogeneous Hashing (DHH) method that integrates three stages including image feature learning, video modeling, and heterogeneous hashing in a single framework, to learn unified binary codes for both face images and videos. To tackle the key challenge of hashing on manifold, a well-studied Riemannian kernel mapping is employed to project data (i.e. covariance matrices) into Euclidean space and thus enables to embed the two heterogeneous representations into a common Hamming space, where both intra-space discriminability and inter-space compatibility are considered. To perform network optimization, the gradient of the kernel mapping is innovatively derived via structured matrix backpropagation in a theoretically principled way. Experiments on three challenging datasets show that our method achieves quite competitive performance compared with existing hashing methods.", "label": 1, "source": "scigen_human", "idx": 85, "lang": "en"}
{"text": "This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation - implicit concurrency 1 footnote 1 1 footnote 1 for short - a broad and versatile computational learning efficiency thought to underlie general-purpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover (UGAs). We demonstrate that implicit concurrency is indeed a form of efficient learning by showing that it can be used to obtain close-to-optimal bounds on the time and queries required to approximately correctly solve a constrained version (k 7, e 1 5) of a recognizable computational learning problem: learning parities with noisy membership queries. We argue that a UGA that treats the noisy membership query oracle as a fitness function can be straightforwardly used to approximately correctly learn the essential attributes in O (log 1.585 n) queries and O (n log 1.585 n) time, where n is the total number of attributes. Our proof relies on an accessible symmetry argument and the use of statistical hypothesis testing to reject a global null hypothesis at the 10 - 100 level of significance. It is, to the best of our knowledge, the first relatively rigorous identification of efficient computational learning in an evolutionary algorithm on a non-trivial learning problem.", "label": 1, "source": "scigen_human", "idx": 86, "lang": "en"}
{"text": "The digital identity problem is a complex one in large part because it involves personal data, the algorithms which compute reputations on the data and the management of the identifiers that are linked to personal data. The reality of today is that personal data of an individual is distributed throughout the Internet, in both private and public institutions, and increasingly also on the user's devices. In order to empower individuals to have a say in who has access to their personal data and to enable individuals to make use of their data for their own purposes, a coherent and scalable access authorization architecture is required. Such an architecture must allow different data holders, data providers and user-content generators to respond to an individual's wishes with regards to consent in a federated fashion. This federation must allow an individual to easily manage access policies and provide consent as required by current and forthcoming data privacy regulations. This paper describes the User Managed Access (UMA) architecture and protocols that provide the foundation for scalable access authorization.", "label": 1, "source": "scigen_human", "idx": 87, "lang": "en"}
{"text": "The majority of works in distributed storage networks assume a simple network model with a collection of identical storage nodes with the same communication cost between the nodes. In this paper, we consider a realistic multi-rack distributed data storage network and present a code design framework for this model. Considering the cheaper data transmission within the racks, our code construction method is able to locally repair the nodes failure within the same rack by using only the survived nodes in the same rack. However, in the case of severe failure patterns when the information content of the survived nodes is not sufficient to repair the failures, other racks will participate in the repair process. By employing the criteria of our multi-rack storage code, we establish a linear programming bound on the size of the code in order to maximize the code rate.", "label": 1, "source": "scigen_human", "idx": 88, "lang": "en"}
{"text": "In this article, we investigate the transient behavior of a sequence of packetsbits traversing a multi-hop wireless network. Our work is motivated by novel applications from the domain of process automation, Machine-Type Communication (MTC) and cyber-physical systems, where short messages are communicated and statistical guarantees need to be provided on a per-message level. In order to optimize such a network, apart from understanding the stationary system dynamics, an understanding of the short-term dynamics (i.e., transient behavior) is also required. To this end, we derive novel Wireless Transient Bounds (WTB) for end-to-end delay and backlog in a multi-hop wireless network using stochastic network calculus approach. WTB depends on the initial backlog at each node as well as the instantaneous channel states. We numerically compare WTB with State-Of-The-Art Transient bounds (SOTAT), that can be obtained by adapting existing stationary bounds, as well as simulation of the network. While SOTAT and stationary bounds are not able to capture the short-term system dynamics well, WTB provides relatively tight upper bound and has a decay rate that closely matches the simulation. This is achieved by WTB only with a slight increase in the computational complexity, by a factor of O (T N), where T is the duration of the arriving sequence and N is the number of hops in the network. We believe that the presented analysis and the bounds can be used as base for future work on transient network optimization, e.g., in massive MTC, critical MTC, edge computing and autonomous vehicle.", "label": 1, "source": "scigen_human", "idx": 89, "lang": "en"}
{"text": "Recently, a tabletop molecular communication platform has been developed for transmitting short text messages across a room. The end-to-end system impulse response for this platform does not follow previously published theoretical works because of imperfect receiver, transmitter, and turbulent flows. Moreover, it is observed that this platform resembles a nonlinear system, which makes the rich body of theoretical work that has been developed by communication engineers not applicable to this platform. In this work, we first introduce corrections to the previous theoretical models of the end-to-end system impulse response based on the observed data from experimentation. Using the corrected impulse response models, we then formulate the nonlinearity of the system as noise and show that through simplifying assumptions it can be represented as Gaussian noise. Through formulating the system's nonlinearity as the output a linear system corrupted by noise, the rich toolbox of mathematical models of communication systems, most of which are based on linearity assumption, can be applied to this platform.", "label": 1, "source": "scigen_human", "idx": 90, "lang": "en"}
{"text": "Neural Architecture Search (NAS) has shown great potentials in finding a better neural network design than human design. Sample-based NAS is the most fundamental method aiming at exploring the search space and evaluating the most promising architecture. However, few works have focused on improving the sampling efficiency for a multi-objective NAS. Inspired by the nature of the graph structure of a neural network, we propose BOGCN-NAS, a NAS algorithm using Bayesian Optimization with Graph Convolutional Network (GCN) predictor. Specifically, we apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. For NAS-oriented tasks, we also design a weighted loss focusing on architectures with high performance. Our method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speedaccuracy trade-off. Extensive experiments are conducted to verify the effectiveness of our method over many competing methods, e.g. 128.4 x more efficient than Random Search and 7.8 x more efficient than previous SOTA LaNAS for finding the best architecture on the largest NAS dataset NASBench-101.", "label": 1, "source": "scigen_human", "idx": 91, "lang": "en"}
{"text": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models footnote footnote Interactive demos, video, code, and data are available at GitHub and gandissect.csail.mit.edu..", "label": 1, "source": "scigen_human", "idx": 92, "lang": "en"}
{"text": "Deep learning models, such as the fully convolutional network (FCN), have been widely used in 3D biomedical segmentation and achieved state-of-the-art performance. Multiple modalities are often used for disease diagnosis and quantification. Two approaches are widely used in the literature to fuse multiple modalities in the segmentation networks: early-fusion (which stacks multiple modalities as different input channels) and late-fusion (which fuses the segmentation results from different modalities at the very end). These fusion methods easily suffer from the cross-modal interference caused by the input modalities which have wide variations. To address the problem, we propose a novel deep learning architecture, namely OctopusNet, to better leverage and fuse the information contained in multi-modalities. The proposed framework employs a separate encoder for each modality for feature extraction and exploits a hyper-fusion decoder to fuse the extracted features while avoiding feature explosion. We evaluate the proposed OctopusNet on two publicly available datasets, i.e. ISLES-2018 and MRBrainS-2013. The experimental results show that our framework outperforms the commonly-used feature fusion approaches and yields the state-of-the-art segmentation accuracy.", "label": 1, "source": "scigen_human", "idx": 93, "lang": "en"}
{"text": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library 1 footnote 1 1 footnote 1 The library is called TensorFlow Fold and lives at . of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "label": 1, "source": "scigen_human", "idx": 94, "lang": "en"}
{"text": "Cascaded regression method is a fast and accurate method on finding 2D pose of objects in RGB images. It is able to find the accurate pose of objects in an image by a great number of corrections on the good initial guess of the pose of objects. This paper explains the algorithm and shows the result of two experiments carried by the researchers. The presented new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing. Finally, we generate confidence-scored 3D proposals of several body parts by re-projecting the classification result and finding local modes.", "label": 1, "source": "scigen_human", "idx": 95, "lang": "en"}
{"text": "In NLP, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because the attention in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into convolution). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text t x . In this work, we propose an attentive convolution network, AttConv . It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text t x that are distant or (ii) from extra (i.e., external) contexts t y . Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of AttConv in sentence representation learning with the incorporation of context. In particular, attentive convolution outperforms attentive pooling and is a strong competitor to popular attentive RNNs. 1 footnote 1 1 footnote 1", "label": 1, "source": "scigen_human", "idx": 96, "lang": "en"}
{"text": "In this paper, we study the problem of approximating the minimum cut in a distributed message-passing model, the CONGEST model. The minimum cut problem has been well-studied in the context of centralized algorithms. However, there were no known non-trivial algorithms in the distributed model until the recent work of Ghaffari and Kuhn. They gave algorithms for finding cuts of size O ( - 1 ) and ( 2 ) in O (D) O (n 1 2 ) rounds and O ( D n) rounds respectively, where is the size of the minimum cut. This matches the lower bound they provided up to a polylogarithmic factor. Yet, no scheme that achieves ( 1 ) -approximation ratio is known. We give a distributed algorithm that finds a cut of size ( 1 ) in O ( D n) time, which is optimal up to polylogarithmic factors.", "label": 1, "source": "scigen_human", "idx": 97, "lang": "en"}
{"text": "Convolutional neural networks (CNN) have had unprecedented success in medical imaging and, in particular, in medical image segmentation. However, despite the fact that segmentation results are closer than ever to the inter-expert variability, CNNs are not immune to producing anatomically inaccurate segmentations, even when built upon a shape prior. In this paper, we present a framework for producing cardiac image segmentation maps that are guaranteed to respect pre-defined anatomical criteria, while remaining within the inter-expert variability. The idea behind our method is to use a well-trained CNN, have it process cardiac images, identify the anatomically implausible results and warp these results toward the closest anatomically valid cardiac shape. This warping procedure is carried out with a constrained variational autoencoder (cVAE) trained to learn a representation of valid cardiac shapes through a smooth, yet constrained, latent space. With this cVAE, we can project any implausible shape into the cardiac latent space and steer it toward the closest correct shape. We tested our framework on short-axis MRI as well as apical two and four-chamber view ultrasound images, two modalities for which cardiac shapes are drastically different. With our method, CNNs can now produce results that are both within the inter-expert variability and always anatomically plausible without having to rely on a shape prior.", "label": 1, "source": "scigen_human", "idx": 98, "lang": "en"}
{"text": "We consider the estimation of a n -dimensional vector x from the knowledge of noisy and possibility non-linear element-wise measurements of x x T , a very generic problem that contains, e.g. stochastic 2 -block model, submatrix localization or the spike perturbation of random matrices. We use an interpolation method proposed by Guerra and later refined by Korada and Macris . We prove that the Bethe mutual information (related to the Bethe free energy and conjectured to be exact by Lesieur et al. on the basis of the non-rigorous cavity method) always yields an upper bound to the exact mutual information. We also provide a lower bound using a similar technique. For concreteness, we illustrate our findings on the sparse PCA problem, and observe that (a) our bounds match for a large region of parameters and (b) that it exists a phase transition in a region where the spectum remains uninformative. While we present only the case of rank-one symmetric matrix estimation, our proof technique is readily extendable to low-rank symmetric matrix or low-rank symmetric tensor estimation.", "label": 1, "source": "scigen_human", "idx": 99, "lang": "en"}
{"text": "We study Doob's martingale convergence theorem for computable continuous time martingales on Brownian motion, in the context of algorithmic randomness. A characterization of the class of sample points for which the theorem holds is given. Such points are given the name of Doob random points. It is shown that a point is Doob random if its tail is computably random in a certain sense. Moreover, Doob randomness is strictly weaker than computable randomness and is incomparable with Schnorr randomness.", "label": 1, "source": "scigen_human", "idx": 100, "lang": "en"}
{"text": "Inspired by recent advances in neural machine translation, that jointly align and translate using encoder-decoder networks equipped with attention, we propose an attention-based LSTM model for human activity recognition. Our model jointly learns to classify actions and highlight frames associated with the action, by attending to salient visual information through a jointly learned soft-attention networks. We explore attention informed by various forms of visual semantic features, including those encoding actions, objects and scenes. We qualitatively show that soft-attention can learn to effectively attend to important objects and scene information correlated with specific human actions. Further, we show that, quantitatively, our attention-based LSTM outperforms the vanilla LSTM and CNN models used by state-of-the-art methods. On a large-scale youtube video dataset, ActivityNet , our model outperforms competing methods in action classification.", "label": 1, "source": "scigen_human", "idx": 102, "lang": "en"}
{"text": "Consider the scattering of a time-harmonic elastic plane wave by a bi-periodic rigid surface. The displacement of elastic wave motion is modeled by the three-dimensional Navier equation in an open domain above the surface. Based on the Dirichlet-to-Neumann (DtN) operator, which is given as an infinite series, an exact transparent boundary condition is introduced and the scattering problem is formulated equivalently into a boundary value problem in a bounded domain. An a posteriori error estimate based adaptive finite element DtN method is proposed to solve the discrete variational problem where the DtN operator is truncated into a finite number of terms. The a posteriori error estimate takes account of the finite element approximation error and the truncation error of the DtN operator which is shown to decay exponentially with respect to the truncation parameter. Numerical experiments are presented to illustrate the effectiveness of the proposed method.", "label": 1, "source": "scigen_human", "idx": 103, "lang": "en"}
{"text": "We determine the cost of performing Shor's algorithm for integer factorization on a ternary quantum computer, using two natural models of universal fault-tolerant computing: (i) a model based on magic state distillation that assumes the availability of the ternary Clifford gates, projective measurements, classical control as its natural instrumentation set; (ii) a model based on a metaplectic topological quantum computer (MTQC). A natural choice to implement Shor's algorithm on a ternary quantum computer is to translate the entire arithmetic into a ternary form. However, it is also possible to emulate the standard binary version of the algorithm by encoding each qubit in a three-level system. We compare the two approaches and analyze the complexity of implementing Shor's period finding function in the two models. We also highlight the fact that the cost of achieving universality through magic states in MTQC architecture is asymptotically lower than in generic ternary case.", "label": 1, "source": "scigen_human", "idx": 104, "lang": "en"}
{"text": "Integrating mobile edge computing (MEC) and wireless power transfer (WPT) has been regarded as a promising technique to improve computation capabilities for self-sustainable Internet of Things (IoT) devices. This paper investigates a wireless powered multiuser MEC system, where a multi-antenna access point (AP) (integrated with an MEC server) broadcasts wireless power to charge multiple users for mobile computing. We consider a time-division multiple access (TDMA) protocol for multiuser computation offloading. Under this setup, we aim to maximize the weighted sum of the computation rates (in terms of the number of computation bits) across all the users, by jointly optimizing the energy transmit beamformer at the AP, the task partition for the users (for local computing and offloading, respectively), and the time allocation among the users. We derive the optimal solution in a semi-closed form via convex optimization techniques. Numerical results show the merit of the proposed design over alternative benchmark schemes.", "label": 1, "source": "scigen_human", "idx": 105, "lang": "en"}
{"text": "We present Task Bench, a parameterized benchmark designed to explore the performance of parallel and distributed programming systems under a variety of application scenarios. Task Bench lowers the barrier to benchmarking multiple programming systems by making the implementation for a given system orthogonal to the benchmarks themselves: every benchmark constructed with Task Bench runs on every Task Bench implementation. Furthermore, Task Bench's parameterization enables a wide variety of benchmark scenarios that distill the key characteristics of larger applications. We conduct a comprehensive study with implementations of Task Bench in 15 programming systems on up to 256 Haswell nodes of the Cori supercomputer. We introduce a novel metric, minimum effective task granularity to study the baseline runtime overhead of each system. We show that when running at scale, 100 s is the smallest granularity that even the most efficient systems can reliably support with current technologies. We also study each system's scalability, ability to hide communication and mitigate load imbalance.", "label": 1, "source": "scigen_human", "idx": 106, "lang": "en"}
{"text": "Even though many machine algorithms have been proposed for entity resolution, it remains very challenging to find a solution with quality guarantees. In this paper, we propose a novel HUman and Machine cOoperation (HUMO) framework for entity resolution (ER), which divides an ER workload between the machine and the human. HUMO enables a mechanism for quality control that can flexibly enforce both precision and recall levels. We introduce the optimization problem of HUMO, minimizing human cost given a quality requirement, and then present three optimization approaches: a conservative baseline one purely based on the monotonicity assumption of precision, a more aggressive one based on sampling and a hybrid one that can take advantage of the strengths of both previous approaches. Finally, we demonstrate by extensive experiments on real and synthetic datasets that HUMO can achieve high-quality results with reasonable return on investment (ROI) in terms of human cost, and it performs considerably better than the state-of-the-art alternatives in quality control.", "label": 1, "source": "scigen_human", "idx": 107, "lang": "en"}
{"text": "Cosmic dust particles effectively attenuate starlight. Their absorption of starlight produces emission spectra from the near- to far-infrared, which depends on the sizes and properties of the dust grains, and spectrum of the heating radiation field. The near- to mid-infrared is dominated by the emissions by very small grains. Modeling the absorption of starlight by these particles is, however, computationally expensive and a significant bottleneck for self-consistent radiation transport codes treating the heating of dust by stars. In this paper, we summarize the formalism for computing the stochastic emissivity of cosmic dust, which was developed in earlier works, and present a new library HEATCODE implementing this formalism for the calculation for arbitrary grain properties and heating radiation fields. Our library is highly optimized for general-purpose processors with multiple cores and vector instructions, with hierarchical memory cache structure. The HEATCODE library also efficiently runs on co-processor cards implementing the Intel Many Integrated Core (Intel MIC) architecture. We discuss in detail the optimization steps that we took in order to optimize for the Intel MIC architecture, which also significantly benefited the performance of the code on general-purpose processors, and provide code samples and performance benchmarks for each step. The HEATCODE library performance on a single Intel Xeon Phi coprocessor (Intel MIC architecture) is 2 times a general-purpose two-socket multicore processor system with approximately the same nominal power consumption. The library supports heterogeneous calculations employing host processors simultaneously with multiple coprocessors, and can be easily incorporated into existing radiation transport codes.", "label": 1, "source": "scigen_human", "idx": 108, "lang": "en"}
{"text": "In 2012, Barbulescu, Detrey, Estibals and Zimmermann proposed a new framework to exhaustively search for optimal formulae for evaluating bilinear maps over finite fields, such as Strassen or Karatsuba formulae. The main contribution of this work is a new criterion to aggressively prune useless branches in the exhaustive search, thus leading to the computation of new optimal formulae. We apply in particular our approach to the short product modulo X 5 and the circulant product modulo X 5 1). Moreover, we are able to prove that there is essentially only one optimal decomposition of the product of 3 2 by 2 3 matrices up to the action of some group of automorphisms.", "label": 1, "source": "scigen_human", "idx": 110, "lang": "en"}
{"text": "In this paper, we focus on how to dynamically allocate a divisible resource fairly among n players who arrive and depart over time. The players may have general heterogeneous valuations over the resource. It is known that exact envy-free and proportional allocations may not exist in the dynamic setting . Thus, we will study to what extent we can guarantee the fairness in the dynamic setting. We first design two algorithms which are O (log n) -proportional and O (n) -envy-free for the setting with general valuations, and by constructing the adversary instances such that all dynamic algorithms must be at least (1) -proportional and (n log n) -envy-free, we show that the bounds are tight up to a logarithmic factor. Moreover, we introduce the setting where the players' valuations are uniform on the resource but with different demands, which generalize the setting of . We prove an O (log n) upper bound and a tight lower bound for this case.", "label": 1, "source": "scigen_human", "idx": 111, "lang": "en"}
{"text": "The engineering of machine learning systems is still a nascent field; relying on a seemingly daunting collection of quickly evolving tools and best practices. It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques. We outline four example machine learning problems that can be solved using open source machine learning libraries, and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications.", "label": 1, "source": "scigen_human", "idx": 112, "lang": "en"}
{"text": "The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.", "label": 1, "source": "scigen_human", "idx": 113, "lang": "en"}
{"text": "Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at .", "label": 1, "source": "scigen_human", "idx": 114, "lang": "en"}
{"text": "Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference.", "label": 1, "source": "scigen_human", "idx": 115, "lang": "en"}
{"text": "Long Short-Term Memory networks (LSTMs) can be trained to realize inverse control of physics-based sound synthesizers. Physics-based sound synthesizers simulate the laws of physics to produce output sound according to input gesture signals. When a user's gestures are measured in real time, she or he can use them to control physics-based sound synthesizers, thereby creating simulated virtual instruments. An intriguing question is how to program a computer to learn to play such physics-based models. This work demonstrates that LSTMs can be trained to accomplish this inverse control task with four physics-based sound synthesizers. Keywords: LSTM, physics-based models, sound synthesis", "label": 1, "source": "scigen_human", "idx": 116, "lang": "en"}
{"text": "Unlike many complex networks studied in the literature, social networks rarely exhibit unanimous behavior, or consensus . This requires a development of mathematical models that are sufficiently simple to be examined and capture, at the same time, the complex behavior of real social groups, where opinions and actions related to them may form clusters of different size. One such model, proposed by Friedkin and Johnsen, extends the idea of conventional consensus algorithm (also referred to as the iterative opinion pooling) to take into account the actors' prejudices, caused by some exogenous factors and leading to disagreement in the final opinions. In this paper, we offer a novel multidimensional extension, describing the evolution of the agents' opinions on several topics. Unlike the existing models, these topics are interdependent, and hence the opinions being formed on these topics are also mutually dependent. We rigorous examine stability properties of the proposed model, in particular, convergence of the agents' opinions. Although our model assumes synchronous communication among the agents, we show that the same final opinions may be reached \"on average\" via asynchronous gossip-based protocols.", "label": 1, "source": "scigen_human", "idx": 117, "lang": "en"}
{"text": "Dominators provide a general mechanism for identifying reconverging paths in graphs. This is useful for a number of applications in Computer-Aided Design (CAD) including signal probability computation in biased random simulation, switching activity estimation in power and noise analysis, and cut points identification in equivalence checking. However, traditional single-vertex dominators are too rare in circuit graphs. In order to handle reconverging paths more efficiently, we consider the case of double-vertex dominators which occur more frequently. First, we derive a number of specific properties of double-vertex dominators. Then, we describe a data structure for representing all double-vertex dominators of a given vertex in linear space. Finally, we present an algorithm for finding all double-vertex dominators of a given vertex in linear time. Our results provide an efficient systematic way of partitioning large graphs along the reconverging points of the signal flow.", "label": 1, "source": "scigen_human", "idx": 118, "lang": "en"}
{"text": "We propose a novel randomized incremental gradient algorithm, namely, VAriance-Reduced Accelerated Gradient (Varag), for finite-sum optimization. Equipped with a unified step-size policy that adjusts itself to the value of the conditional number, Varag exhibits the unified optimal rates of convergence for solving smooth convex finite-sum problems directly regardless of their strong convexity. Moreover, Varag is the first of its kind that benefits from the strong convexity of the data-fidelity term, and solves a wide class of problems only satisfying an error bound condition rather than strong convexity, both resulting in the optimal linear rate of convergence. Varag can also be extended to solve stochastic finite-sum problems.", "label": 1, "source": "scigen_human", "idx": 119, "lang": "en"}
{"text": "The fact that individuals will most likely behave differently in different situations begets the introduction of conditional strategies. Inspired by this, we study the evolution of cooperation in the spatial public goods game, where besides unconditional cooperators and defectors, also different types of conditional cooperators compete for space. Conditional cooperators will contribute to the public good only if other players within the group are likely to cooperate as well, but will withhold their contribution otherwise. Depending on the number of other cooperators that are required to elicit cooperation of a conditional cooperator, the latter can be classified in as many types as there are players within each group. We find that the most cautious cooperators, such that require all other players within a group to be conditional cooperators, are the undisputed victors of the evolutionary process, even at very low synergy factors. We show that the remarkable promotion of cooperation is due primarily to the spontaneous emergence of quarantining of defectors, which become surrounded by conditional cooperators and are forced into isolated convex \"bubbles\" from where they are unable to exploit the public good. This phenomenon can be observed only in structured populations, thus adding to the relevance of pattern formation for the successful evolution of cooperation.", "label": 1, "source": "scigen_human", "idx": 120, "lang": "en"}
{"text": "In a guessing game, players guess the value of a random real number selected using some probability density function. The winner may be determined in various ways; for example, a winner can be a player whose guess is closest in magnitude to the target or a winner can be a player coming closest without guessing higher than the target. We study optimal strategies for players in these games and determine some of them for two, three, and four players.", "label": 1, "source": "scigen_human", "idx": 121, "lang": "en"}
{"text": "We propose a novel network pruning approach by information preserving of pre-trained network weights (filters). Network pruning with the information preserving is formulated as a matrix sketch problem, which is efficiently solved by the off-the-shelf Frequent Direction method. Our approach, referred to as FilterSketch, encodes the second-order information of pre-trained weights, which enables the representation capacity of pruned networks being recovered with a simple fine-tuning procedure. FilterSketch requires neither training from scratch nor data-driven iterative optimization, leading to a several-orders-of-magnitude reduction of time cost in the optimization of pruning. Experiments on CIFAR-10 show that FilterSketch reduces 63.3 of FLOPs and prunes 59.9 of network parameters with negligible accuracy cost for ResNet-110. On ILSVRC-2012, it reduces 45.5 of FLOPs and removes 43.0 of parameters with only 0.69 accuracy drop for ResNet-50. Codes and experimental results can be available at .", "label": 1, "source": "scigen_human", "idx": 123, "lang": "en"}
{"text": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture.", "label": 1, "source": "scigen_human", "idx": 125, "lang": "en"}
{"text": "We combine momentum from machine learning with evolutionary dynamics, where momentum can be viewed as a simple mechanism of intergenerational memory. Using information divergences as Lyapunov functions, we show that momentum accelerates the convergence of evolutionary dynamics including the replicator equation and Euclidean gradient descent on populations. When evolutionarily stable states are present, these methods prove convergence for small learning rates or small momentum, and yield an analytic determination of the relative decrease in time to converge that agrees well with computations. The main results apply even when the evolutionary dynamic is not a gradient flow. We also show that momentum can alter the convergence properties of these dynamics, for example by breaking the cycling associated to the rock-paper-scissors landscape, leading to either convergence to the ordinarily non-absorbing equilibrium, or divergence, depending on the value and mechanism of momentum.", "label": 1, "source": "scigen_human", "idx": 126, "lang": "en"}
{"text": "The arXiv has collected 1.5 million pre-print articles over 28 years, hosting literature from scientific fields including Physics, Mathematics, and Computer Science. Each pre-print features text, figures, authors, citations, categories, and other metadata. These rich, multi-modal features, combined with the natural graph structure - created by citation, affiliation, and co-authorship - makes the arXiv an exciting candidate for benchmarking next-generation models. Here we take the first necessary steps toward this goal, by providing a pipeline which standardizes and simplifies access to the arXiv's publicly available data. We use this pipeline to extract and analyze a 6.7 million edge citation graph, with an 11 billion word corpus of full-text research articles. We present some baseline classification results, and motivate application of more exciting generative graph models.", "label": 1, "source": "scigen_human", "idx": 127, "lang": "en"}
{"text": "Point clouds provide a flexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervised learning tasks such as object classification and semantic segmentation. While massive point cloud datasets can be captured using modern scanning technology, manually labelling such large 3D point clouds for supervised learning tasks is a cumbersome process. This necessitates methods that can learn from unlabelled data to significantly reduce the number of annotated samples needed in supervised learning. We propose a self-supervised learning task for deep learning on raw point cloud data in which a neural network is trained to reconstruct point clouds whose parts have been randomly rearranged. While solving this task, representations that capture semantic properties of the point cloud are learned. Our method is agnostic of network architecture and outperforms current unsupervised learning approaches in downstream object classification tasks. We show experimentally, that pre-training with our method before supervised training improves the performance of state-of-the-art models and significantly improves sample efficiency.", "label": 1, "source": "scigen_human", "idx": 129, "lang": "en"}
{"text": "In cost sharing games with delays, a set of agents jointly allocates a finite subset of resources. Each resource has a fixed cost that has to be shared by the players, and each agent has a non-shareable player-specific delay for each resource. A prominent example is uncapacitated facility location (UFL), where facilities need to be opened (at a shareable cost) and clients want to connect to opened facilities. Each client pays a cost share and his non-shareable physical connection cost. Given any profile of subsets allocated by the agents, a separable cost sharing protocol determines cost shares that satisfy budget balance on every resource and separability over the resources. Moreover, a separable protocol guarantees existence of pure Nash equilibria in the induced strategic game for the agents. In this paper, we study separable cost sharing protocols in several general combinatorial domains. We provide black-box reductions to reduce the design of a separable cost-sharing protocol to the design of an approximation algorithm for the underlying cost minimization problem. In this way, we obtain new separable cost-sharing protocols in games based on arbitrary player-specific matroids, single-source connection games without delays, and connection games on n -series-parallel graphs with delays. All these reductions are efficiently computable - given an initial allocation profile, we obtain a cheaper profile and separable cost shares turning the profile into a pure Nash equilibrium. Hence, in these domains any approximation algorithm can be used to obtain a separable cost sharing protocol with a price of stability bounded by the approximation factor.", "label": 1, "source": "scigen_human", "idx": 130, "lang": "en"}
{"text": "This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learning Environment using deep reinforcement learning. The proposed method, called human checkpoint replay , consists in using checkpoints sampled from human gameplay as starting points for the learning process. This is meant to compensate for the difficulties of current exploration strategies, such as -greedy, to find successful control policies in games with sparse rewards. Like other deep reinforcement learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma's Revenge and Private Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human experience replay.", "label": 1, "source": "scigen_human", "idx": 131, "lang": "en"}
{"text": "This paper presents a distance-based discriminative framework for learning with probability distributions. Instead of using kernel mean embeddings or generalized radial basis kernels, we introduce embeddings based on dissimilarity of distributions to some reference distributions denoted as templates. Our framework extends the theory of similarity of to the population distribution case and we show that, for some learning problems, some dissimilarity on distribution achieves low-error linear decision functions with high probability. Our key result is to prove that the theory also holds for empirical distributions. Algorithmically, the proposed approach consists in computing a mapping based on pairwise dissimilarity where learning a linear decision function is amenable. Our experimental results show that the Wasserstein distance embedding performs better than kernel mean embeddings and computing Wasserstein distance is far more tractable than estimating pairwise Kullback-Leibler divergence of empirical distributions.", "label": 1, "source": "scigen_human", "idx": 132, "lang": "en"}
{"text": "Aim: In contrast to studies of defects found during code review, we aim to clarify whether code reviews measures can explain the prevalence of post-release defects. Method: We replicate McIntosh et al.'s study that uses additive regression to model the relationship between defects and code reviews. To increase external validity, we apply the same methodology on a new software project. We discuss our findings with the first author of the original study, McIntosh. We then investigate how to reduce the impact of correlated predictors in the variable selection process and how to increase understanding of the inter-relationships among the predictors by employing Bayesian Network (BN) models. Context: As in the original study, we use the same measures authors obtained for Qt project in the original study. We mine data from version control and issue tracker of Google Chrome and operationalize measures that are close analogs to the large collection of code, process, and code review measures used in the replicated the study. Results: Both the data from the original study and the Chrome data showed high instability of the influence of code review measures on defects with the results being highly sensitive to variable selection procedure. Models without code review predictors had as good or better fit than those with review predictors. Replication, however, confirms with the bulk of prior work showing that prior defects, module size, and authorship have the strongest relationship to post-release defects. The application of BN models helped explain the observed instability by demonstrating that the review-related predictors do not affect post-release defects directly and showed indirect effects. For example, changes that have no review discussion tend to be associated with files that have had many prior defects which in turn increase the number of post-release defects. We hope that similar analyses of other software engineering techniques may also yield a more nuanced view of their impact. Our replication package including our data and scripts is publicly available .", "label": 1, "source": "scigen_human", "idx": 133, "lang": "en"}
{"text": "Population synthesis is concerned with the generation of synthetic yet realistic representations of populations. It is a fundamental problem in the modeling of transport where the synthetic populations of micro-agents represent a key input to most agent-based models. In this paper, a new methodological framework for how to 'grow' pools of micro-agents is presented. The model framework adopts a deep generative modeling approach from machine learning based on a Variational Autoencoder (VAE). Compared to the previous population synthesis approaches, including Iterative Proportional Fitting (IPF), Gibbs sampling and traditional generative models such as Bayesian Networks or Hidden Markov Models, the proposed method allows fitting the full joint distribution for high dimensions. The proposed methodology is compared with a conventional Gibbs sampler and a Bayesian Network by using a large-scale Danish trip diary. It is shown that, while these two methods outperform the VAE in the low-dimensional case, they both suffer from scalability issues when the number of modeled attributes increases. It is also shown that the Gibbs sampler essentially replicates the agents from the original sample when the required conditional distributions are estimated as frequency tables. In contrast, the VAE allows addressing the problem of sampling zeros by generating agents that are virtually different from those in the original data but have similar statistical properties. The presented approach can support agent-based modeling at all levels by enabling richer synthetic populations with smaller zones and more detailed individual characteristics.", "label": 1, "source": "scigen_human", "idx": 134, "lang": "en"}
{"text": "The likelihood model of high dimensional data X n can often be expressed as p ( X n Z n , ), where : ( k) k [ K ] is a collection of hidden features shared across objects, indexed by n , and Z n is a non-negative factor loading vector with K entries where Z n k indicates the strength of k used to express X n . In this paper, we introduce random function priors for Z n for modeling correlations among its K dimensions Z n 1 through Z n K , which we call population random measure embedding (PRME). Our model can be viewed as a generalized paintbox model (,) using random functions, and can be learned efficiently with neural networks via amortized variational inference. We derive our Bayesian nonparametric method by applying a representation theorem on separately exchangeable discrete random measures.", "label": 1, "source": "scigen_human", "idx": 135, "lang": "en"}
{"text": "We study the metric facility location problem with client insertions and deletions. This setting differs from the classic dynamic facility location problem, where the set of clients remains the same, but the metric space can change over time. We show a deterministic algorithm that maintains a constant factor approximation to the optimal solution in worst-case time O (2 O ( 2 per client insertion or deletion in metric spaces while answering queries about the cost in O (1) time, where denotes the doubling dimension of the metric. For metric spaces with bounded doubling dimension, the update time is polylogarithmic in the parameters of the problem.", "label": 1, "source": "scigen_human", "idx": 136, "lang": "en"}
{"text": "Although shill bidding is a common auction fraud, it is however very tough to detect. Due to the unavailability and lack of training data, in this study, we build a high-quality labeled shill bidding dataset based on recently collected auctions from eBay. Labeling shill biding instances with multidimensional features is a critical phase for the fraud classification task. For this purpose, we introduce a new approach to systematically label the fraud data with the help of the hierarchical clustering CURE that returns remarkable results as illustrated in the experiments.", "label": 1, "source": "scigen_human", "idx": 137, "lang": "en"}
{"text": "Compressive sensing (CS) is a promising technology for realizing energy-efficient wireless sensors for long-term health monitoring. However, conventional model-driven CS frameworks suffer from limited compression ratio and reconstruction quality when dealing with physiological signals due to inaccurate models and the overlook of individual variability. In this paper, we propose a data-driven CS framework that can learn signal characteristics and personalized features from any individual recording of physiologic signals to enhance CS performance with a minimized number of measurements. Such improvements are accomplished by a co-training approach that optimizes the sensing matrix and the dictionary towards improved restricted isometry property and signal sparsity, respectively. Experimental results upon ECG signals show that the proposed method, at a compression ratio of 10x, successfully reduces the isometry constant of the trained sensing matrices by 86 against random matrices and improves the overall reconstructed signal-to-noise ratio by 15dB over conventional model-driven approaches.", "label": 1, "source": "scigen_human", "idx": 138, "lang": "en"}
{"text": "Despite being popularly referred to as the ultimate solution for all problems of our current electric power system, smart grid is still a growing and unstable concept. It is usually considered as a set of advanced features powered by promising technological solutions. In this paper, we describe smart grid as a socio-technical transition and illustrate the evolutionary path on which a smart grid can be realized. Through this conceptual lens, we revealed the role of big data, and how it can fuel the organic growth of smart grid. We also provided a rough estimate of how much data will be potentially generated from different data sources, which helps clarify the big data challenge during the evolutionary process.", "label": 1, "source": "scigen_human", "idx": 139, "lang": "en"}
{"text": "An instance of the Connected Maximum Cut problem consists of an undirected graph G (V , E) and the goal is to find a subset of vertices S V that maximizes the number of edges in the cut (S) such that the induced graph G [ S ] is connected. We present the first non-trivial (1 log n) approximation algorithm for the Connected Maximum Cut problem in general graphs using novel techniques. We then extend our algorithm to edge weighted case and obtain a poly-logarithmic approximation algorithm. Interestingly, in contrast to the classical Max-Cut problem that can be solved in polynomial time on planar graphs, we show that the Connected Maximum Cut problem remains NP-hard on unweighted, planar graphs. On the positive side, we obtain a polynomial time approximation scheme for the Connected Maximum Cut problem on planar graphs and more generally on bounded genus graphs.", "label": 1, "source": "scigen_human", "idx": 141, "lang": "en"}
{"text": "Given a social network modeled as a weighted graph G , the influence maximization problem seeks k vertices to become initially influenced, to maximize the expected number of influenced nodes under a particular diffusion model. The influence maximization problem has been proven to be NP-hard, and most proposed solutions to the problem are approximate greedy algorithms, which can guarantee a tunable approximation ratio for their results with respect to the optimal solution. The state-of-the-art algorithms are based on Reverse Influence Sampling (RIS) technique, which can offer both computational efficiency and non-trivial 1 1 e ) -approximation ratio guarantee for any 0 . RIS-based algorithms, despite their lower computational cost compared to other methods, still require long running times to solve the problem in large-scale graphs with low values of . In this paper, we present a novel and efficient parallel implementation of a RIS-based algorithm, namely IMM, on GPGPU. The proposed solution can significantly reduce the running time on large-scale graphs with low values of . Furthermore, we show that our proposed parallel algorithm can solve other variations of the IM problem, only by applying minor modifications. Experimental results show that the proposed solution reduces the runtime by a factor up to 220 .", "label": 1, "source": "scigen_human", "idx": 142, "lang": "en"}
{"text": "Graph-specific computing with the support of dedicated accelerator has greatly boosted the graph processing in both efficiency and energy. Nevertheless, their data conflict management is still sequential in essential when some vertex needs a large number of conflicting updates at the same time, leading to prohibitive performance degradation. This is particularly true for processing natural graphs. In this paper, we have the insight that the atomic operations for the vertex updating of many graph algorithms (e.g., BFS, PageRank and WCC) are typically incremental and simplex. This hence allows us to parallelize the conflicting vertex updates in an accumulative manner. We architect a novel graph-specific accelerator that can simultaneously process atomic vertex updates for massive parallelism on the conflicting data access while ensuring the correctness. A parallel accumulator is designed to remove the serialization in atomic protection for conflicting vertex updates through merging their results in parallel. Our implementation on Xilinx Virtex UltraScale XCVU9P with a wide variety of typical graph algorithms shows that our accelerator achieves an average throughput by 2.36 GTEPS as well as up to 3.14x performance speedup in comparison with state-of-the-art ForeGraph (with single-chip version).", "label": 1, "source": "scigen_human", "idx": 143, "lang": "en"}
{"text": "Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with writing software. We propose TEASPN 1 footnote 1 1 footnote 1 See for the screencast and for more general info about TEASPN. , a protocol and an open-source framework for achieving integrated writing assistance environments. The protocol standardizes the way writing software communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in natural language processing (NLP) with low cost. As a result, users can enjoy the integrated experience in their favorite writing software. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text.", "label": 1, "source": "scigen_human", "idx": 144, "lang": "en"}
{"text": "We have shown previously that our parameter-reduced variants of Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) are comparable in performance to the standard LSTM RNN on the MNIST dataset. In this study, we show that this is also the case for two diverse benchmark datasets, namely, the review sentiment IMDB and the 20 Newsgroup datasets. Specifically, we focus on two of the simplest variants, namely LSTM6 (i.e., standard LSTM with three constant fixed gates) and LSTMC6 (i.e., LSTM6 with further reduced cell body input block). We demonstrate that these two aggressively reduced-parameter variants are competitive with the standard LSTM when hyper-parameters, e.g., learning parameter, number of hidden units and gate constants are set properly. These architectures enable speeding up training computations and hence, these networks would be more suitable for online training and inference onto portable devices with relatively limited computational resources.", "label": 1, "source": "scigen_human", "idx": 145, "lang": "en"}
{"text": "In this paper, we show that any scaled-up version of any discrete self-similar tree fractal does not strictly self-assemble, at any temperature, in Winfree's abstract Tile Assembly Model.", "label": 1, "source": "scigen_human", "idx": 146, "lang": "en"}
{"text": "In this paper, we uncover a new off-path TCP hijacking attack that can be used to terminate victim TCP connections or inject forged data into victim TCP connections by manipulating the new mixed IPID assignment method, which is widely used in Linux kernel version 4.18 and beyond to help defend against TCP hijacking attacks. The attack has three steps. First, an off-path attacker can downgrade the IPID assignment for TCP packets from the more secure per-socket-based policy to the less secure hash-based policy, building a shared IPID counter that forms a side channel on the victim. Second, the attacker detects the presence of TCP connections by observing the shared IPID counter on the victim. Third, the attacker infers the sequence number and the acknowledgment number of the detected connection by observing the side channel of the shared IPID counter. Consequently, the attacker can completely hijack the connection, i.e., resetting the connection or poisoning the data stream. We evaluate the impacts of this off-path TCP attack in the real world. Our case studies of SSH DoS, manipulating web traffic, and poisoning BGP routing tables show its threat on a wide range of applications. Our experimental results show that our off-path TCP attack can be constructed within 215 seconds and the success rate is over 88. Finally, we analyze the root cause of the exploit and develop a new IPID assignment method to defeat this attack. We prototype our defense in Linux 4.18 and confirm its effectiveness through extensive evaluation over real applications on the Internet.", "label": 1, "source": "scigen_human", "idx": 148, "lang": "en"}
{"text": "In this paper, we first propose a method that can efficiently compute the maximal robust controlled invariant set for discrete-time linear systems with pure delay in input. The key to this method is to construct an auxiliary linear system (without delay) with the same state-space dimension of the original system in consideration and to relate the maximal invariant set of the auxiliary system to that of the original system. When the system is subject to disturbances, guaranteeing safety is harder for systems with input delays. Ability to incorporate any additional information about the disturbance becomes more critical in these cases. Motivated by this observation, in the second part of the paper, we generalize the proposed method to take into account additional preview information on the disturbances, while maintaining computational efficiency. Compared with the naive approach of constructing a higher dimensional system by appending the state-space with the delayed inputs and previewed disturbances, the proposed approach is demonstrated to scale much better with the increasing delay time.", "label": 1, "source": "scigen_human", "idx": 149, "lang": "en"}
{"text": "Increasing technological sophistication and widespread use of smartphones and wearable devices provide opportunities for innovative and highly personalized health interventions. A Just-In-Time Adaptive Intervention (JITAI) uses real-time data collection and communication capabilities of modern mobile devices to deliver interventions in real-time that are adapted to the in-the-moment needs of the user. The lack of methodological guidance in constructing data-based JITAIs remains a hurdle in advancing JITAI research despite the increasing popularity of JITAIs among clinical scientists. In this article, we make a first attempt to bridge this methodological gap by formulating the task of tailoring interventions in real-time as a contextual bandit problem. Interpretability requirements in the domain of mobile health lead us to formulate the problem differently from existing formulations intended for web applications such as ad or news article placement. Under the assumption of linear reward function, we choose the reward function (the \"critic parameterization separately from a lower dimensional parameterization of stochastic policies (the \"actor. We provide an online actor-critic algorithm that guides the construction and refinement of a JITAI. Asymptotic properties of the actor-critic algorithm are developed and backed up by numerical experiments. Additional numerical experiments are conducted to test the robustness of the algorithm when idealized assumptions used in the analysis of contextual bandit algorithm are breached.", "label": 1, "source": "scigen_human", "idx": 150, "lang": "en"}
{"text": "We describe a way of assigning labels to the vertices of any undirected graph on up to n vertices, each composed of n 2 O (1) bits, such that given the labels of two vertices, and no other information regarding the graph, it is possible to decide whether or not the vertices are adjacent in the graph. This is optimal, up to an additive constant, and constitutes the first improvement in almost 50 years of an n 2 O (log n) bound of Moon. As a consequence, we obtain an induced-universal graph for n -vertex graphs containing only O (2 n 2) vertices, which is optimal up to a multiplicative constant, solving an open problem of Vizing from 1968. We obtain similar tight results for directed graphs, tournaments and bipartite graphs.", "label": 1, "source": "scigen_human", "idx": 151, "lang": "en"}
{"text": "Laminated glass structures are formed by stiff layers of glass connected with a compliant plastic interlayer. Due to their slenderness and heterogeneity, they exhibit a complex mechanical response that is difficult to capture by single-layer models even in the elastic range. The purpose of this paper is to introduce an efficient and reliable finite element approach to the simulation of the immediate response of laminated glass beams. It proceeds from a refined plate theory due to Mau (1973), as we treat each layer independently and enforce the compatibility by the Lagrange multipliers. At the layer level, we adopt the finite-strain shear deformable formulation of Reissner (1972) and the numerical framework by Ibrahimbegovic and Frey (1993). The resulting system is solved by the Newton method with consistent linearization. By comparing the model predictions against available experimental data, analytical methods and two-dimensional finite element simulations, we demonstrate that the proposed formulation is reliable and provides accuracy comparable to the detailed two-dimensional finite element analyzes. As such, it offers a convenient basis to incorporate more refined constitutive description of the interlayer.", "label": 1, "source": "scigen_human", "idx": 152, "lang": "en"}
{"text": "If a Micro Processor Unit (MPU) receives an external electric signal as noise, the system function will freeze or malfunction easily. A new resilience strategy is implemented in order to reset the MPU automatically and stop the MPU from freezing or malfunctioning. The technique is useful for embedded systems which work in non-human environments. However, evaluating resilience strategies is difficult because their effectiveness depends on numerous, complex, interacting factors. In this paper, we use probabilistic model checking to evaluate the embedded systems installed with the above mentioned new resilience strategy. Qualitative evaluations are implemented with 6 PCTL formulas, and quantitative evaluations use two kinds of evaluation. One is system failure reduction, and the other is ADT (Average Down Time), the industry standard. Our work demonstrates the benefits brought by the resilience strategy. Experimental results indicate that our evaluation is cost-effective and reliable.", "label": 1, "source": "scigen_human", "idx": 153, "lang": "en"}
{"text": "We present goal-oriented a posteriori error estimates for the automatic variationally stable finite element (AVS-FE) method for scalar-valued convection-diffusion problems. The AVS-FE method is a Petrov-Galerkin method in which the test space is broken, whereas the trial space consists of classical FE basis functions, e.g., C 0 or Raviart-Thomas functions. We employ the concept of optimal test functions of the discontinuous Petrov-Galerkin (DPG) method by Demkowicz and Gopalakrishnan , leading to unconditionally stable FE approximations. Remarkably, by using C 0 or Raviart-Thomas trial spaces, the optimal discontinuous test functions can be computed in a completely decoupled element-by-element fashion. To establish the error estimators we present two approaches: i) following the classical approach of Becker and Rannacher , i.e., the dual solution is sought in the (broken) test space, and i i) introducing an alternative approach in which we seek C 0 , or Raviart-Thomas, AVS-FE approximations of the dual solution by using the underlying strong form of the dual boundary value problem (BVP). Various numerical verifications for 2D convection-dominated diffusion BVPs show that the estimates of the approximation error by the new alternative method are highly accurate, while the classical approach leads to error estimates of poor quality. Lastly, we present an algorithm for h - adaptive processes based on control of the numerical approximation error via the new alternative approach. Numerical verifications show that the estimator maintains high accuracy as the error converges to zero.", "label": 1, "source": "scigen_human", "idx": 155, "lang": "en"}
{"text": "Session types have been proposed as a means of statically verifying implementations of communication protocols. Although prior work has been successful in verifying some classes of protocols, it does not cope well with parameterized, multi-actor scenarios with inherent asynchrony. For example, the sliding window protocol is inexpressible in previously proposed session type systems. This paper describes System-A, a new typing language which overcomes many of the expressiveness limitations of prior work. System-A explicitly supports asynchrony and parallelism, as well as multiple forms of parameterization. We define System-A and show how it can be used for the static verification of a large class of asynchronous communication protocols.", "label": 1, "source": "scigen_human", "idx": 157, "lang": "en"}
{"text": "We propose an improved discriminative model prediction method for robust long-term tracking based on a pre-trained short-term tracker. The baseline pre-trained short-term tracker is SuperDiMP which combines the bounding-box regressor of PrDiMP with the standard DiMP classifier. Our tracker RLT-DiMP improves SuperDiMP in the following three aspects: (1) Uncertainty reduction using random erasing: To make our model robust, we exploit an agreement from multiple images after erasing random small rectangular areas as a certainty. And then, we correct the tracking state of our model accordingly. (2) Random search with spatio-temporal constraints: we propose a robust random search method with a score penalty applied to prevent the problem of sudden detection at a distance. (3) Background augmentation for more discriminative feature learning: We augment various backgrounds that are not included in the search area to train a more robust model in the background clutter. In experiments on the VOT-LT2020 benchmark dataset, the proposed method achieves comparable performance to the state-of-the-art long-term trackers. The source code is available at: .", "label": 1, "source": "scigen_human", "idx": 158, "lang": "en"}
{"text": "We show that for every integer k 2 , the Res (k) propositional proof system does not have the weak feasible disjunction property. Next, we generalize a recent result of Atserias and Muller to Res (k). We show that if NP is not included in P (resp. QP, SUBEXP) then for every integer k 1 , Res (k) is not automatable in polynomial (resp. quasi-polynomial, subexponential) time.", "label": 1, "source": "scigen_human", "idx": 159, "lang": "en"}
{"text": "Coronavirus or COVID-19, which has been declared pandemic by the World Health Organization, has incurred huge losses to the lives of people throughout the world. Although, the scientists, researchers and doctors are working round the clock to develop a vaccine for COVID-19, it may take a year or two to make a safe and effective vaccine available for the world. In current circumstances, a solution must be developed to control or stop the spread of the virus. For this purpose, a novel technique based on call data record analysis (CDRA) and contact tracing is proposed that can effectively control the coronavirus outbreak. A positive coronavirus patient can be traced through CDRA and contact tracing. The technique can track the path traversed by the patient and collect the cell numbers of all those people who have met with the patient. Keeping in tact the privacy of this group of people, who are contacted through their cell numbers so that they can isolate themselves till the result of their coronavirus test arrives. If a test result of a person comes positive among the group, then heshe must be isolated and same CDRA and contact tracing procedures are adopted for that person. A COVID-19 patient is geo tagged and alerts are sent if any violation of isolation is done by the patient. Moreover, the general public is informed in advance to avoid the path followed by the patients. This cost effective mechanism is not only capable to control the coronavirus outbreak but also helps in isolating the patient in hisher house.", "label": 1, "source": "scigen_human", "idx": 160, "lang": "en"}
{"text": "We are experiencing an unprecedented healthcare crisis caused by the newly-discovered corona-virus disease (COVID-19). The outbreaks of COVID-19 reveal the frailties of existing healthcare systems. Therefore, the digital transformation of healthcare systems becomes an inevitable trend. During this process, the Internet of Medical Things (IoMT) plays a crucial role while intrinsic vulnerabilities of security and privacy deter the wide adoption of IoMT. In this article, we present a blockchain-enabled IoMT to address the security and privacy concerns of IoMT systems. We also discuss the solutions brought by blockchain-enabled IoMT to COVID-19 from five different perspectives. Moreover, we outline the open challenges and future directions of blockchain-enabled IoMT.", "label": 1, "source": "scigen_human", "idx": 161, "lang": "en"}
{"text": "In an article written five years ago, we described a method for predicting which scientific papers will be highly cited in the future, even if they are currently not highly cited. Applying the method to real citation data we made predictions about papers we believed would end up being well cited. Here we revisit those predictions, five years on, to see how well we did. Among the over 2000 papers in our original data set, we examine the fifty that, by the measures of our previous study, were predicted to do best and we find that they have indeed received substantially more citations in the intervening years than other papers, even after controlling for the number of prior citations. On average these top fifty papers have received 23 times as many citations in the last five years as the average paper in the data set as a whole, and 15 times as many as the average paper in a randomly drawn control group that started out with the same number of citations. Applying our prediction technique to current data, we also make new predictions of papers that we believe will be well cited in the next few years.", "label": 1, "source": "scigen_human", "idx": 162, "lang": "en"}
{"text": "The estimation of the motor torque and friction parameters are crucial for implementing an efficient low level joint torque control. In a set of coupled joints, the actuators torques are mapped to the output joint torques through a coupling matrix, such that the motor torque and friction parameters appear entangled from the point of view of the joints. As a result, their identification is problematic when using the same methodology as for single joints. This paper proposes an identification method with an improved accuracy with respect to classical closed loop methods on coupled joints. The method stands out through the following key points: it is a direct open loop identification; it addresses separately each motor in the coupling; it accounts for the static friction in the actuation elements. The identified parameters should significantly improve the contribution of the feed-forward terms in the low level control of coupled joints with static friction.", "label": 1, "source": "scigen_human", "idx": 163, "lang": "en"}
{"text": "We study the problem of estimating a p -dimensional s -sparse vector in a linear model with Gaussian design and additive noise. In the case where the labels are contaminated by at most o adversarial outliers, we prove that the 1 -penalized Huber's M -estimator based on n samples attains the optimal rate of convergence ( s n) 1 2 ( o n), up to a logarithmic factor. For more general design matrices, our results highlight the importance of two properties: the transfer principle and the incoherence property. These properties with suitable constants are shown to yield the optimal rates, up to log-factors, of robust estimation with adversarial contamination.", "label": 1, "source": "scigen_human", "idx": 164, "lang": "en"}
{"text": "We consider the problem of determining the existence of a sequence of matrices driving a discrete-time multi-agent consensus system to consensus. We transform this problem into the problem of the existence of a product of the (stochastic) transition matrices that has a positive column. This allows us to make use of results from automata theory to sets of stochastic matrices. Our main result is a polynomial-time algorithm to decide the existence of a sequence of matrices achieving consensus.", "label": 1, "source": "scigen_human", "idx": 165, "lang": "en"}
{"text": "An Intrusion Detection System (IDS) is a key cybersecurity tool for network administrators as it identifies malicious traffic and cyberattacks. With the recent successes of machine learning techniques such as deep learning, more and more IDS are now using machine learning algorithms to detect attacks faster. However, these systems lack robustness when facing previously unseen types of attacks. With the increasing number of new attacks, especially against Internet of Things devices, having a robust IDS able to spot unusual and new attacks becomes necessary. This work explores the possibility of leveraging generative adversarial models to improve the robustness of machine learning based IDS. More specifically, we propose a new method named SIGMA, that leverages adversarial examples to strengthen IDS against new types of attacks. Using Generative Adversarial Networks (GAN) and metaheuristics, SIGMA generates adversarial examples, iteratively, and uses it to retrain a machine learning-based IDS, until a convergence of the detection rate (i.e. until the detection system is not improving anymore). A round of improvement consists of a generative phase, in which we use GANs and metaheuristics to generate instances; an evaluation phase in which we calculate the detection rate of those newly generated attacks; and a training phase, in which we train the IDS with those attacks. We have evaluated the SIGMA method for four standard machine learning classification algorithms acting as IDS, with a combination of GAN and a hybrid local-search and genetic algorithm, to generate new datasets of attacks. Our results show that SIGMA can successfully generate adversarial attacks against different machine learning based IDS. Also, using SIGMA, we can improve the performance of an IDS to up to 100 after as little as two rounds of improvement.", "label": 1, "source": "scigen_human", "idx": 166, "lang": "en"}
{"text": "This paper addresses the problem of monocular 3D human shape and pose estimation from an RGB image. Despite great progress in this field in terms of pose prediction accuracy, state-of-the-art methods often predict inaccurate body shapes. We suggest that this is primarily due to the scarcity of in-the-wild training data with diverse and accurate body shape labels. Thus, we propose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system that utilises proxy representations, such as silhouettes and 2D joints, as inputs to a shape and pose regression neural network, which is trained with synthetic training data (generated on-the-fly during training using the SMPL statistical body model) to overcome data scarcity. We bridge the gap between synthetic training inputs and noisy real inputs, which are predicted by keypoint detection and segmentation CNNs at test-time, by using data augmentation and corruption during training. In order to evaluate our approach, we curate and provide a challenging evaluation dataset for monocular human shape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via multi-frame optimisation. We show that STRAPS outperforms other state-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets and metrics.", "label": 1, "source": "scigen_human", "idx": 167, "lang": "en"}
{"text": "This paper addresses the problem of designing an optimal output feedback controller with a specified controller structure for linear time-invariant (LTI) systems to maximize the passivity level for the closed-loop system, in both continuous-time (CT) and discrete-time (DT). Specifically, the set of controllers under consideration is linearly parameterized with constrained parameters. Both input feedforward passivity (IFP) and output feedback passivity (OFP) indices are used to capture the level of passivity. Given a set of stabilizing controllers, a necessary and sufficient condition is proposed for the existence of such fixed-structured output feedback controllers that can passivate the closed-loop system. Moreover, it is shown that the condition can be used to obtain the controller that maximizes the IFP or the OFP index by solving a convex optimization problem.", "label": 1, "source": "scigen_human", "idx": 168, "lang": "en"}
{"text": "In this paper, a multi-scale approach to spectrum sensing in cognitive cellular networks is proposed. In order to overcome the huge cost incurred in the acquisition of full network state information, a hierarchical scheme is proposed, based on which local state estimates are aggregated up the hierarchy to obtain aggregate state information at multiple scales, which are then sent back to each cell for local decision making. Thus, each cell obtains fine-grained estimates of the channel occupancies of nearby cells, but coarse-grained estimates of those of distant cells. The performance of the aggregation scheme is studied in terms of the trade-off between the throughput achievable by secondary users and the interference generated by the activity of these secondary users to primary users. In order to account for the irregular structure of interference patterns arising from path loss, shadowing, and blockages, which are especially relevant in millimeter wave networks, a greedy algorithm is proposed to find a multi-scale aggregation tree to optimize the performance. It is shown numerically that this tailored hierarchy outperforms a regular tree construction by 60.", "label": 1, "source": "scigen_human", "idx": 169, "lang": "en"}
{"text": "Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.", "label": 1, "source": "scigen_human", "idx": 171, "lang": "en"}
{"text": "We adapt the rectangular splitting technique of Paterson and Stockmeyer to the problem of evaluating terms in holonomic sequences that depend on a parameter. This approach allows computing the n -th term in a recurrent sequence of suitable type using O (n 1 2) \"expensive\" operations at the cost of an increased number of \"cheap\" operations. Rectangular splitting has little overhead and can perform better than either naive evaluation or asymptotically faster algorithms for ranges of n encountered in applications. As an example, fast numerical evaluation of the gamma function is investigated. Our work generalizes two previous algorithms of Smith.", "label": 1, "source": "scigen_human", "idx": 172, "lang": "en"}
{"text": "Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments - as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.", "label": 1, "source": "scigen_human", "idx": 173, "lang": "en"}
{"text": "The Full Dimension-MIMO (FD-MIMO) technology is capable of achieving huge improvements in network throughput with simultaneous connectivity of a large number of mobile wireless devices, unmanned aerial vehicles, and the Internet of Things (IoT). In FD-MIMO, with a large number of antennae at the base station and the ability to perform beamforming, the capacity of the physical downlink shared channel (PDSCH) has increased a lot. However, the current specifications of the 3 r d Generation Partnership Project (3GPP) does not allow the base station to perform beamforming techniques for the physical downlink control channel (PDCCH), and hence, PDCCH has neither the capacity nor the coverage of PDSCH. Therefore, PDCCH capacity will still limit the performance of a network as it dictates the number of users that can be scheduled at a given time instant. In Release 11, 3GPP introduced enhanced PDCCH (EPDCCH) to increase the PDCCH capacity at the cost of sacrificing the PDSCH resources. The problem of enhancing the PDCCH capacity within the available control channel resources has not been addressed yet in the literature. Hence, in this paper, we propose a novel beamformed PDCCH (BF-PDCCH) design which is aligned to the 3GPP specifications and requires simple software changes at the base station. We rely on the sounding reference signals transmitted in the uplink to decide the best beam for a user and ingeniously schedule the users in PDCCH. We perform system level simulations to evaluate the performance of the proposed design and show that the proposed BF-PDCCH achieves larger network throughput when compared with the current state of art algorithms, PDCCH and EPDCCH schemes. Keywords: Beamforming, blind decoding, control channel, rate matching, search space.", "label": 1, "source": "scigen_human", "idx": 174, "lang": "en"}
{"text": "Individual identification is essential to animal behavior and ecology research and is of significant importance for protecting endangered species. Red pandas, among the world's rarest animals, are currently identified mainly by visual inspection and microelectronic chips, which are costly and inefficient. Motivated by recent advancement in computer-vision-based animal identification, in this paper, we propose an automatic framework for identifying individual red pandas based on their face images. We implement the framework by exploring well-established deep learning models with necessary adaptation for effectively dealing with red panda images. Based on a database of red panda images constructed by ourselves, we evaluate the effectiveness of the proposed automatic individual red panda identification method. The evaluation results show the promising potential of automatically recognizing individual red pandas from their faces. We are going to release our database and model in the public domain to promote the research on automatic animal identification and particularly on the technique for protecting red pandas.", "label": 1, "source": "scigen_human", "idx": 175, "lang": "en"}
{"text": "Nowadays, ubiquitous network access has become a reality thanks to Unmanned Aerial Vehicles (UAVs) that have gained extreme popularity due to their flexible deployment and higher chance of Line-of-Sight (LoS) links to ground users. Telecommunication service providers deploy UAVs to provide flying network access in remote rural areas, disaster-affected areas or massive-attended events (sport venues, festivals, etc.) where full set-up to provide temporary wireless coverage would be very expensive. Of course, a UAV i s battery-powered which means limited energy budget for both mobility aspect and communication aspect. An efficient solution is to allow UAVs swhiching their radio modules to sleep mode in order to extend battery lifetime. This results in temporary unavailability of communication feature. Within such a situation, the ultimate deal for a UAV operator is to provide a cost effective service with acceptable availability. This would allow to meet some target Quality of Service (QoS) while having a good market share granting satisfactory benefits. In this article, we exhibit a new framework with many interesting insights on how to jointly define the availability and the access cost in UAV-empowered flying access networks to opportunistically cover a target geographical area. Yet, we construct a duopoly model to capture the adversarial behavior of service providers in terms of their pricing policies and their respective availability probabilities. Optimal periodic beaconing (small messages advertising existence of a UAV) is a vital issue that needs to be addressed, given the UAVs limited battery capacity and their recharging constraints. A full analysis of the game outcome, both in terms of equilibrium pricing and equilibrium availability, is derived. We show that the availability-pricing game exhibits some nice features as it is sub-modular with respect to the availability policy, whereas it is super-modular with respect to the service fee. Furthermore, we implement a learning scheme using best-response dynamics that allows operators to learn their joint pricing-availability strategies in a fast, accurate and distributed fashion. Extensive simulations show convergence of the proposed scheme to the joint pricing-availability equilibrium and offer promising insights on how the game parameters should be chosen to efficiently control the duopoly game. Index Terms:UAV, Coverage probability; Service probability; Cournot Duopoly; Non-cooperative Game; Pricing Game; Availability Game; Sub-Modular Game; Super-Modular Game; Nash Equilibrium; Best Response Dynamics.", "label": 1, "source": "scigen_human", "idx": 176, "lang": "en"}
{"text": "We propose a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. Inspired by the classic intrinsic image decomposition, our learning signal builds upon two insights: 1) combining the disentangled factors should reconstruct the original image, and 2) the permanent factors should stay constant across multiple temporal samples of the same scene. To facilitate training, we assemble a city-scale dataset of outdoor timelapse imagery from Google Street View, where the same locations are captured repeatedly through time. This data represents an unprecedented scale of spatio-temporal outdoor imagery. We show that our learned disentangled factors can be used to manipulate novel images in realistic ways, such as changing lighting effects and scene geometry. Please visit factorize-a-city.github.io for animated results.", "label": 1, "source": "scigen_human", "idx": 177, "lang": "en"}
{"text": "This paper proposes a vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles. Different from previous sky editing methods that either focus on static photos or require inertial measurement units integrated in smartphones on shooting videos, our method is purely vision-based, without any requirements on the capturing devices, and can be well applied to either online or offline processing scenarios. Our method runs in real-time and is free of user interactions. We decompose this artistic creation process into a couple of proxy tasks including sky matting, motion estimation, and image blending. Experiments are conducted on videos diversely captured in the wild by handheld smartphones and dash cameras, and show high fidelity and good generalization of our method in both visual quality and lightingmotion dynamics. Our code and animated results are available at .", "label": 1, "source": "scigen_human", "idx": 178, "lang": "en"}
{"text": "In this article a DNN-based system for detection of three common voice disorders (vocal nodules, polyps and cysts; laryngeal neoplasm; unilateral vocal paralysis) is presented. The input to the algorithm is (at least 3-second long) audio recording of sustained vowel sound a:. The algorithm was developed as part of the \"2018 FEMH Voice Data Challenge\" organized by Far Eastern Memorial Hospital and obtained score value (defined in the challenge specification) of 77.44. This was the second best result before final submission. Final challenge results are not yet known during writing of this document. The document also reports changes that were made for the final submission which improved the score value in cross-validation by 0.6 points.", "label": 1, "source": "scigen_human", "idx": 179, "lang": "en"}
{"text": "Can an adversary exploit model explanations to infer sensitive information about the models' training set? To investigate this question, we first focus on membership inference attacks: given a data point and a model explanation, the attacker's goal is to decide whether or not the point belongs to the training data. We study this problem for two popular transparency methods: gradient-based attribution methods and record-based influence measures. We develop membership inference attacks based on these model explanations, and extensively test them on a variety of datasets. For gradient-based methods, we show that the explanations can leak a significant amount of information about the individual data points in the training set, much beyond what is leaked through the predicted labels. We also show that record-based measures can be effectively, and even more significantly, exploited for membership inference attacks. More importantly, we design reconstruction attacks against this class of model explanations. We demonstrate that they can be exploited to recover significant parts of the training set. Finally, our results indicate that minorities and outliers are more vulnerable to these type of attacks than the rest of the population. Thus, there is a significant disparity for the privacy risks of model explanations across different groups.", "label": 1, "source": "scigen_human", "idx": 180, "lang": "en"}
{"text": "Variational Bayes (VB) is a recent approximate method for Bayesian inference. It has the merit of being a fast and scalable alternative to Markov Chain Monte Carlo (MCMC) but its approximation error is often unknown. In this paper, we derive the approximation error of VB in terms of mean, mode, variance, predictive density and KL divergence for the linear Gaussian multi-equation regression. Our results indicate that VB approximates the posterior mean perfectly. Factors affecting the magnitude of underestimation in the posterior variance and mode are revealed. Importantly, We demonstrate that VB estimates predictive densities accurately.", "label": 1, "source": "scigen_human", "idx": 181, "lang": "en"}
{"text": "Given the rise of a new approach to MT, Neural MT (NMT), and its promising performance on different text types, we assess the translation quality it can attain on what is perceived to be the greatest challenge for MT: literary text. Specifically, we target novels, arguably the most popular type of literary text. We build a literary-adapted NMT system for the English-to-Catalan translation direction and evaluate it against a system pertaining to the previous dominant paradigm in MT: statistical phrase-based MT (PBSMT). To this end, for the first time we train MT systems, both NMT and PBSMT, on large amounts of literary text (over 100 million words) and evaluate them on a set of twelve widely known novels spanning from the the 1920s to the present day. According to the BLEU automatic evaluation metric, NMT is significantly better than PBSMT ( p 0.01) on all the novels considered. Overall, NMT results in a 11 relative improvement (3 points absolute) over PBSMT. A complementary human evaluation on three of the books shows that between 17 and 34 of the translations, depending on the book, produced by NMT (versus 8 and 20 with PBSMT) are perceived by native speakers of the target language to be of equivalent quality to translations produced by a professional human translator.", "label": 1, "source": "scigen_human", "idx": 182, "lang": "en"}
{"text": "For testing goodness of fit it is very popular to use either the 2 -statistic or G 2 -statistics (information divergence). Asymptotically both are 2 -distributed so an obvious question is which of the two statistics that has a distribution that is closest to the 2 -distribution. Surprisingly, when there is only one degree of freedom it seems like the distribution of information divergence is much better approximated by a 2 -distribution than the 2 -statistic. For random variables we introduce a new transformation that transform several important distributions into new random variables that are almost Gaussian. For the binomial distributions and the Poisson distributions we formulate a general conjecture about how close their transform are to the Gaussian. The conjecture is proved for Poisson distributions.", "label": 1, "source": "scigen_human", "idx": 183, "lang": "en"}
{"text": "How are the meanings of linguistic expressions related to their use in concrete cognitive tasks? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain quantifiers. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network - a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) - on the \"most\" verification task from , manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as cognitive models of this and other psychosemantic tasks.", "label": 1, "source": "scigen_human", "idx": 184, "lang": "en"}
{"text": "We propose a computational framework for ranking images (group photos in particular) taken at the same event within a short time span. The ranking is expected to correspond with human perception of overall appeal of the images. We hypothesize and provide evidence through subjective analysis that the factors that appeal to humans are its emotional content, aesthetics and image quality. We propose a network which is an ensemble of three information channels, each predicting a score corresponding to one of the three visual appeal factors. For group emotion estimation, we propose a convolutional neural network (CNN) based architecture for predicting group emotion from images. This new architecture enforces the network to put emphasis on the important regions in the images, and achieves comparable results to the state-of-the-art. Next, we develop a network for the image ranking task that combines group emotion, aesthetics and image quality scores. Owing to the unavailability of suitable databases, we created a new database of manually annotated group photos taken during various social events. We present experimental results on this database and other benchmark databases whenever available. Overall, our experiments show that the proposed framework can reliably predict the overall appeal of images with results closely corresponding to human ranking.", "label": 1, "source": "scigen_human", "idx": 185, "lang": "en"}
{"text": "In Internet of Things (IoT) systems with security demands, there is often a need to distribute sensitive information (such as encryption keys, digital signatures, or login credentials, etc.) among the devices, so that it can be retrieved for confidential purposes at a later moment. However, this information cannot be entrusted to any one device, since the failure of that device or an attack on it will jeopardize the security of the entire network. Even if the information is divided among devices, there is still the danger that an attacker can compromise a group of devices and expose the sensitive information. In this work, we design and implement a secure and robust scheme to enable the distribution of sensitive information in IoT networks. The proposed approach has two important properties: (1) it uses Threshold Secret Sharing (TSS) to split the information into pieces distributed among all devices in the system - and so the information can only be retrieved collaboratively by groups of devices; and (2) it ensures the privacy and integrity of the information, even when attackers hijack a large number of devices and use them in concert - specifically, all the compromised devices can be identified, the confidentiality of information is kept, and authenticity of the secret can be guaranteed.", "label": 1, "source": "scigen_human", "idx": 186, "lang": "en"}
{"text": "During the Coincheck incident, which recorded the largest damages in cryptocurrency history in 2018, it was demonstrated that using Mosaic token can have a certain effect. Although it seems attractive to employ tokens as countermeasures for cryptocurrency leakage, Mosaic is a specific token for the New Economy Movement (NEM) cryptocurrency and is not employed for other blockchain systems or cryptocurrencies. Moreover, although some volunteers tracked leaked NEM using Mosaic in the CoinCheck incident, it would be better to verify that the volunteers can be trusted. Simultaneously, if someone (e.g., who stole cryptocurrencies) can identify the volunteers, then that person or organization may be targets of them. In this paper, we propose an anonymous trust-marking scheme on blockchain systems that is universally applicable to any cryptocurrency. In our scheme, entities called token admitters are allowed to generate tokens adding trustworthiness or untrustworthiness to addresses. Anyone can anonymously verify whether these tokens were issued by a token admitter. Simultaneously, only the designated auditor and no one else, including nondesignated auditors, can identify the token admitters. Our scheme is based on accountable ring signatures and commitment, and is implemented on an elliptic curve called Curve25519, and we confirm that both cryptographic tools are efficient. Moreover, we also confirm that our scheme is applicable to Bitcoin, Ethereum, and NEM.", "label": 1, "source": "scigen_human", "idx": 187, "lang": "en"}
{"text": "Unlike the traditional dock-based systems, dockless bike-sharing systems are more convenient for users in terms of flexibility. However, the flexibility of these dockless systems comes at the cost of management and operation complexity. Indeed, the imbalanced and dynamic use of bikes leads to mandatory rebalancing operations, which impose a critical need for effective bike traffic flow prediction. While efforts have been made in developing traffic flow prediction models, existing approaches lack interpretability, and thus have limited value in practical deployment. To this end, we propose an Interpretable Bike Flow Prediction (IBFP) framework, which can provide effective bike flow prediction with interpretable traffic patterns. Specifically, by dividing the urban area into regions according to flow density, we first model the spatio-temporal bike flows between regions with graph regularized sparse representation, where graph Laplacian is used as a smooth operator to preserve the commonalities of the periodic data structure. Then, we extract traffic patterns from bike flows using subspace clustering with sparse representation to construct interpretable base matrices. Moreover, the bike flows can be predicted with the interpretable base matrices and learned parameters. Finally, experimental results on real-world data show the advantages of the IBFP method for flow prediction in dockless bike sharing systems. In addition, the interpretability of our flow pattern exploitation is further illustrated through a case study where IBFP provides valuable insights into bike flow analysis.", "label": 1, "source": "scigen_human", "idx": 188, "lang": "en"}
{"text": "Let 1 p . In this paper, we consider solving a nonlinear functional equation f (x) y , where x , y belong to p and f has continuous bounded gradient in an inverse-closed subalgebra of B ( 2), the Banach algebra of all bounded linear operators on the Hilbert space 2 . We introduce strict monotonicity property for functions f on Banach spaces p so that the above nonlinear functional equation is solvable and the solution x depends continuously on the given data y in p . We show that the Van-Cittert iteration converges in p with exponential rate and hence it could be used to locate the true solution of the above nonlinear functional equation. We apply the above theory to handle two problems in signal processing: nonlinear sampling termed with instantaneous companding and subsequently average sampling; and local identification of innovation positions and qualification of amplitudes of signals with finite rate of innovation.", "label": 1, "source": "scigen_human", "idx": 189, "lang": "en"}
{"text": "The dynamic complexity of the reachability query is studied in the dynamic complexity framework of Patnaik and Immerman, restricted to quantifier-free update formulas. It is shown that, with this restriction, the reachability query cannot be dynamically maintained, neither with binary auxiliary relations nor with unary auxiliary functions, and that ternary auxiliary relations are more powerful with respect to graph queries than binary auxiliary relations. Further inexpressibility results are given for the reachability query in a different setting as well as for a syntactical restriction of quantifier-free update formulas. Moreover inexpressibility results for some other queries are presented.", "label": 1, "source": "scigen_human", "idx": 190, "lang": "en"}
{"text": "Simulating dynamic rupture propagation is challenging due to the uncertainties involved in the underlying physics of fault slip, stress conditions, and frictional properties of the fault. A trial and error approach is often used to determine the unknown parameters describing rupture, but running many simulations usually requires human review to determine how to adjust parameter values and is thus not very efficient. To reduce the computational cost and improve our ability to determine reasonable stress and friction parameters, we take advantage of the machine learning approach. We develop two models for earthquake rupture propagation using the artificial neural network (ANN) and the random forest (RF) algorithms to predict if a rupture can break a geometric heterogeneity on a fault. We train the models using a database of 1600 dynamic rupture simulations computed numerically. Fault geometry, stress conditions, and friction parameters vary in each simulation. We cross-validate and test the predictive power of the models using an additional 400 simulated ruptures, respectively. Both RF and ANN models predict rupture propagation with more than 81 accuracy and model parameters can be used to infer the underlying factors most important for rupture propagation. Both of the models are computationally efficient such that the 400 testings require a fraction of a second, leading to potential applications of dynamic rupture that have previously not been possible due to the computational demands of physics-based rupture simulations.", "label": 1, "source": "scigen_human", "idx": 191, "lang": "en"}
{"text": "Accurate mobile traffic forecast is important for efficient network planning and operations. However, existing traffic forecasting models have high complexity, making the forecasting process slow and costly. In this paper, we analyze some characteristics of mobile traffic such as periodicity, spatial similarity and short term relativity. Based on these characteristics, we propose a Block Regression (BR) model for mobile traffic forecasting. This model employs seasonal differentiation so as to take into account of the temporally repetitive nature of mobile traffic. One of the key features of our BR model lies in its low complexity since it constructs a single model for all base stations. We evaluate the accuracy of BR model based on real traffic data and compare it with the existing models. Results show that our BR model offers equal accuracy to the existing models but has much less complexity.", "label": 1, "source": "scigen_human", "idx": 192, "lang": "en"}
{"text": "Deep convolutional neural networks have demonstrated promising performance on image classification tasks, but the manual design process becomes more and more complex due to the fast depth growth and the increasingly complex topologies of convolutional neural networks. As a result, neural architecture search has emerged to automatically design convolutional neural networks that outperform handcrafted counterparts. However, the computational cost is immense, e.g. 22,400 GPU-days and 2,000 GPU-days for two outstanding neural architecture search works named NAS and NASNet, respectively, which motivates this work. A new effective and efficient surrogate-assisted particle swarm optimisation algorithm is proposed to automatically evolve convolutional neural networks. This is achieved by proposing a novel surrogate model, a new method of creating a surrogate dataset and a new encoding strategy to encode variable-length blocks of convolutional neural networks, all of which are integrated into a particle swarm optimisation algorithm to form the proposed method. The proposed method shows its effectiveness by achieving competitive error rates of 3.49 on the CIFAR-10 dataset, 18.49 on the CIFAR-100 dataset, and 1.82 on the SVHN dataset. The convolutional neural network blocks are efficiently learned by the proposed method from CIFAR-10 within 3 GPU-days due to the acceleration achieved by the surrogate model and the surrogate dataset to avoid the training of 80.1 of convolutional neural network blocks represented by the particles. Without any further search, the evolved blocks from CIFAR-10 can be successfully transferred to CIFAR-100 and SVHN, which exhibits the transferability of the block learned by the proposed method.", "label": 1, "source": "scigen_human", "idx": 193, "lang": "en"}
{"text": "For fear of retribution, the victim of a crime may be willing to report it only if other victims of the same perpetrator also step forward. Common examples include 1) identifying oneself as the victim of sexual harassment, especially by a person in a position of authority or 2) accusing an influential politician, an authoritarian government, or ones own employer of corruption. To handle such situations, legal literature has proposed the concept of an allegation escrow: a neutral third-party that collects allegations anonymously, matches them against each other, and de-anonymizes allegers only after de-anonymity thresholds (in terms of number of co-allegers), pre-specified by the allegers, are reached. An allegation escrow can be realized as a single trusted third party; however, this party must be trusted to keep the identity of the alleger and content of the allegation private. To address this problem, this paper introduces Secure Allegation Escrows (SAE, pronounced \"say. A SAE is a group of parties with independent interests and motives, acting jointly as an escrow for collecting allegations from individuals, matching the allegations, and de-anonymizing the allegations when designated thresholds are reached. By design, SAEs provide a very strong property: No less than a majority of parties constituting a SAE can de-anonymize or disclose the content of an allegation without a sufficient number of matching allegations (even in collusion with any number of other allegers). Once a sufficient number of matching allegations exist, the join escrow discloses the allegation with the allegers' identities. We describe how SAEs can be constructed using a novel authentication protocol and a novel allegation matching and bucketing algorithm, provide formal proofs of the security of our constructions, and evaluate a prototype implementation, demonstrating feasibility in practice.", "label": 1, "source": "scigen_human", "idx": 194, "lang": "en"}
{"text": "The technology in the area of automated vehicles is gaining speed and promises many advantages. However, with the recent introduction of conditionally automated driving, we have also seen accidents. Test protocols for both, conditionally automated (e.g., on highways) and automated vehicles do not exist yet and leave researchers and practitioners with different challenges. For instance, current test procedures do not suffice for fully automated vehicles, which are supposed to be completely in charge for the driving task and have no driver as a back up. This paper presents current challenges of testing the functionality and safety of automated vehicles derived from conducting focus groups and interviews with 26 participants from five countries having a background related to testing automotive safety-related topics. We provide an overview of the state-of-practice of testing active safety features as well as challenges that needs to be addressed in the future to ensure safety for automated vehicles. The major challenges identified through the interviews and focus groups, enriched by literature on this topic are related to 1) virtual testing and simulation, 2) safety, reliability, and quality, 3) sensors and sensor models, 4) required scenario complexity and amount of test cases, and 5) handover of responsibility between the driver and the vehicle.", "label": 1, "source": "scigen_human", "idx": 195, "lang": "en"}
{"text": "Attention is an increasingly popular mechanism used in a wide range of neural architectures. Because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures for natural language processing, with a focus on architectures designed to work with vector representation of the textual data. We discuss the dimensions along which proposals differ, the possible uses of attention, and chart the major research activities and open challenges in the area.", "label": 1, "source": "scigen_human", "idx": 196, "lang": "en"}
{"text": "Automatic cell segmentation in microscopy images works well with the support of deep neural networks trained with full supervision. Collecting and annotating images, though, is not a sustainable solution for every new microscopy database and cell type. Instead, we assume that we can access a plethora of annotated image data sets from different domains (sources) and a limited number of annotated image data sets from the domain of interest (target), where each domain denotes not only different image appearance but also a different type of cell segmentation problem. We pose this problem as meta-learning where the goal is to learn a generic and adaptable few-shot learning model from the available source domain data sets and cell segmentation tasks. The model can be afterwards fine-tuned on the few annotated images of the target domain that contains different image appearance and different cell type. In our meta-learning training, we propose the combination of three objective functions to segment the cells, move the segmentation results away from the classification boundary using cross-domain tasks, and learn an invariant representation between tasks of the source domains. Our experiments on five public databases show promising results from 1- to 10-shot meta-learning using standard segmentation neural network architectures.", "label": 1, "source": "scigen_human", "idx": 197, "lang": "en"}
{"text": "With the ratification of the IEEE 802.15.3d amendment to the 802.15.3, a first step has been made to standardize consumer wireless communications in the sub-THz frequency band. The IEEE 802.15.3d offers switched point-to-point connectivity with the data rates of 100Gbits and higher at distances ranging from tens of centimeters up to a few hundred meters. In this article, we provide a detailed introduction to the IEEE 802.15.3d and the key design principles beyond the developed standard. We particularly describe the target applications and usage scenarios, as well as the specifics of the IEEE 802.15.3d physical and medium access layers. Later, we present the results of the initial performance evaluation of IEEE 802.15.3d wireless communications. The obtained first-order performance predictions show non-incremental benefits compared to the characteristics of the fifth-generation wireless systems, thus paving the way towards the six-generation (6G) THz networks. We conclude the article by outlining the further standardization and regulatory activities on wireless networking in the THz frequency band.", "label": 1, "source": "scigen_human", "idx": 198, "lang": "en"}
{"text": "We study three-way joins on MapReduce. Joins are very useful in a multitude of applications from data integration and traversing social networks, to mining graphs and automata-based constructions. However, joins are expensive, even for moderate data sets; we need efficient algorithms to perform distributed computation of joins using clusters of many machines. MapReduce has become an increasingly popular distributed computing system and programming paradigm. We consider a state-of-the-art MapReduce multi-way join algorithm by Afrati and Ullman and show when it is appropriate for use on very large data sets. By providing a detailed experimental study, we demonstrate that this algorithm scales much better than what is suggested by the original paper. However, if the join result needs to be summarized or aggregated, as opposed to being only enumerated, then the aggregation step can be integrated into a cascade of two-way joins, making it more efficient than the other algorithm, and thus becomes the preferred solution.", "label": 1, "source": "scigen_human", "idx": 199, "lang": "en"}
{"text": "Advertising is a primary means for revenue generation for millions of websites and smartphone apps. Naturally, a fraction abuse ad networks to systematically defraud advertisers of their money. Modern defences have matured to overcome some forms of click fraud but measurement studies have reported that a third of clicks supplied by ad networks could be clickspam. Our work develops novel inference techniques which can isolate click fraud attacks using their fundamental properties. We propose two defences, mimicry and bait-click , which provide clickspam detection with substantially improved results over current approaches. Mimicry leverages the observation that organic clickfraud involves the reuse of legitimate click traffic, and thus isolates clickspam by detecting patterns of click reuse within ad network clickstreams. The bait-click defence leverages the vantage point of an ad network to inject a pattern of bait clicks into a user's device. Any organic clickspam generated involving the bait clicks will be subsequently recognisable by the ad network. Our experiments show that the mimicry defence detects around 81 of fake clicks in stealthy (low rate) attacks, with a false-positive rate of 110 per hundred thousand clicks. Similarly, the bait-click defence enables further improvements in detection, with rates of 95 and a reduction in false-positive rates of between 0 and 30 clicks per million - a substantial improvement over current approaches.", "label": 1, "source": "scigen_human", "idx": 200, "lang": "en"}
{"text": "Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Te mporal M essage P assing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments 1 footnote 1 1 footnote 1 Code and data are published at on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7 average relative improvement in Hits10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.", "label": 1, "source": "scigen_human", "idx": 201, "lang": "en"}
{"text": "Separating a singing voice from its music accompaniment remains an important challenge in the field of music information retrieval. We present a unique neural network approach inspired by a technique that has revolutionized the field of vision: pixel-wise image classification, which we combine with cross entropy loss and pretraining of the CNN as an autoencoder on singing voice spectrograms. The pixel-wise classification technique directly estimates the sound source label for each time-frequency (T-F) bin in our spectrogram image, thus eliminating common pre- and postprocessing tasks. The proposed network is trained by using the Ideal Binary Mask (IBM) as the target output label. The IBM identifies the dominant sound source in each T-F bin of the magnitude spectrogram of a mixture signal, by considering each T-F bin as a pixel with a multi-label (for each sound source). Cross entropy is used as the training objective, so as to minimize the average probability error between the target and predicted label for each pixel. By treating the singing voice separation problem as a pixel-wise classification task, we additionally eliminate one of the commonly used, yet not easy to comprehend, postprocessing steps: the Wiener filter postprocessing. The proposed CNN outperforms the first runner up in the Music Information Retrieval Evaluation eXchange (MIREX) 2016 and the winner of MIREX 2014 with a gain of 2.2702 5.9563 dB global normalized source to distortion ratio (GNSDR) when applied to the iKala dataset. An experiment with the DSD100 dataset on the full-tracks song evaluation task also shows that our model is able to compete with cutting-edge singing voice separation systems which use multi-channel modeling, data augmentation, and model blending.", "label": 1, "source": "scigen_human", "idx": 202, "lang": "en"}
{"text": "Dense 3D shape acquisition of swimming human or live fish is an important research topic for sports, biological science and so on. For this purpose, active stereo sensor is usually used in the air, however it cannot be applied to the underwater environment because of refraction, strong light attenuation and severe interference of bubbles. Passive stereo is a simple solution for capturing dynamic scenes at underwater environment, however the shape with textureless surfaces or irregular reflections cannot be recovered. Recently, the stereo camera pair with a pattern projector for adding artificial textures on the objects is proposed. However, to use the system for underwater environment, several problems should be compensated, i.e. , disturbance by fluctuation and bubbles. Simple solution is to use convolutional neural network for stereo to cancel the effects of bubbles andor water fluctuation. Since it is not easy to train CNN with small size of database with large variation, we develop a special bubble generation device to efficiently create real bubble database of multiple size and density. In addition, we propose a transfer learning technique for multi-scale CNN to effectively remove bubbles and projected-patterns on the object. Further, we develop a real system and actually captured live swimming human, which has not been done before. Experiments are conducted to show the effectiveness of our method compared with the state of the art techniques.", "label": 1, "source": "scigen_human", "idx": 203, "lang": "en"}
{"text": "One key use of k-means clustering is to identify cluster prototypes which can serve as representative points for a dataset. However, a drawback of using k-means cluster centers as representative points is that such points distort the distribution of the underlying data. This can be highly disadvantageous in problems where the representative points are subsequently used to gain insights on the data distribution, as these points do not mimic the distribution of the data. To this end, we propose a new clustering method called \"distributional clustering,\" which ensures cluster centers capture the distribution of the underlying data. We first prove the asymptotic convergence of the proposed cluster centers to the data generating distribution, then present an efficient algorithm for computing these cluster centers in practice. Finally, we demonstrate the effectiveness of distributional clustering on synthetic and real datasets.", "label": 1, "source": "scigen_human", "idx": 204, "lang": "en"}
{"text": "An important problem on graph-structured data is that of quantifying similarity between graphs. Graph kernels are an established technique for such tasks; in particular, those based on random walks and return probabilities have proven to be effective in wide-ranging applications, from bioinformatics to social networks to computer vision. However, random walk kernels generally suffer from slowness and tottering , an effect which causes walks to overemphasize local graph topology, undercutting the importance of global structure. To correct for these issues, we recast return probability graph kernels under the more general framework of density of states - a framework which uses the lens of spectral analysis to uncover graph motifs and properties hidden within the interior of the spectrum - and use our interpretation to construct scalable, composite density of states based graph kernels which balance local and global information, leading to higher classification accuracies on a host of benchmark datasets.", "label": 1, "source": "scigen_human", "idx": 205, "lang": "en"}
{"text": "Generating training examples for supervised tasks is a long sought after goal in AI. We study the problem of heart signal electrocardiogram (ECG) synthesis for improved heartbeat classification. ECG synthesis is challenging: the generation of training examples for such biological-physiological systems is not straightforward, due to their dynamic nature in which the various parts of the system interact in complex ways. However, an understanding of these dynamics has been developed for years in the form of mathematical process simulators. We study how to incorporate this knowledge into the generative process by leveraging a biological simulator for the task of ECG classification. Specifically, we use a system of ordinary differential equations (ODE) representing heart dynamics, and incorporate this ODE system into the optimization process of a generative adversarial network to create biologically plausible ECG training examples. We perform empirical evaluation and show that heart simulation knowledge during the generation process improves ECG classification.", "label": 1, "source": "scigen_human", "idx": 206, "lang": "en"}
{"text": "We study a constrained contextual linear bandit setting, where the goal of the agent is to produce a sequence of policies, whose expected cumulative reward over the course of T rounds is maximum, and each has an expected cost below a certain threshold . We propose an upper-confidence bound algorithm for this problem, called optimistic pessimistic linear bandit (OPLB), and prove an O ( d T - c 0) bound on its T -round regret, where the denominator is the difference between the constraint threshold and the cost of a known feasible action. We further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting. We prove a regret bound of O ( K T - c 0) for this algorithm in K -armed bandits, which is a K improvement over the regret bound we obtain by simply casting multi-armed bandits as an instance of contextual linear bandits and using the regret bound of OPLB. We also prove a lower-bound for the problem studied in the paper and provide simulations to validate our theoretical results.", "label": 1, "source": "scigen_human", "idx": 207, "lang": "en"}
{"text": "When analyzing the statistical and topological characteristics of complex networks, an effective and convenient way is to compute the centralities for recognizing influential and significant nodes or structures, yet most of them are restricted to local environment or some specific configurations. In this paper we propose a new centrality for nodes based on the von Neumann entropy, which allows us to investigate the importance of nodes in the view of spectrum eigenvalues distribution. By presenting the performances of this centrality with network examples in reality, it is shown that the von Neumann entropy node centrality is an excellent index for selecting crucial nodes as well as classical ones. Then to lower down the computational complexity, an approximation calculation to this centrality is given which only depends on its first and second neighbors. Furthermore, in the optimal spreader problem and reducing average clustering coefficients, this entropy centrality presents excellent efficiency and unveil topological structure features of networks accurately. The entropy centrality could reduce the scales of giant connected components fastly in Erdos-Renyi and scale-free networks, and break down the cluster structures efficiently in random geometric graphs. This new methodology reveals the node importance in the perspective of spectrum, which provides a new insight into networks research and performs great potentials to discover essential structural features in networks.", "label": 1, "source": "scigen_human", "idx": 208, "lang": "en"}
{"text": "We study approximation algorithms for variants of the median string problem, which asks for a string that minimizes the sum of edit distances from a given set of m strings of length n . Only the straightforward 2 -approximation is known for this NP-hard problem. This problem is motivated e.g. by computational biology, and belongs to the class of median problems (over different metric spaces), which are fundamental tasks in data analysis. Our main result is for the Ulam metric, where all strings are permutations over [ n ] and each edit operation moves a symbol (deletion plus insertion). We devise for this problem an algorithms that breaks the 2 -approximation barrier, i.e., computes a 2 ) -approximate median permutation for some constant 0 in time O ( n m 2 n 3). We further use these techniques to achieve a 2 ) approximation for the median string problem in the special case where the median is restricted to length n and the optimal objective is large ( m n). We also design an approximation algorithm for the following probabilistic model of the Ulam median: the input consists of m perturbations of an (unknown) permutation x , each generated by moving every symbol to a random position with probability (a parameter) 0 . Our algorithm computes with high probability a ( 1 o ( 1 -approximate median permutation in time O ( m n 2 n 3).", "label": 1, "source": "scigen_human", "idx": 209, "lang": "en"}
{"text": "Predicting which patients are more likely to be readmitted to a hospital within 30 days after discharge is a valuable piece of information in clinical decision-making. Building a successful readmission risk classifier based on the content of Electronic Health Records (EHRs) has proved, however, to be a challenging task. Previously explored features include mainly structured information, such as sociodemographic data, comorbidity codes and physiological variables. In this paper we assess incorporating additional clinically interpretable NLP-based features such as topic extraction and clinical sentiment analysis to predict early readmission risk in psychiatry patients.", "label": 1, "source": "scigen_human", "idx": 210, "lang": "en"}
{"text": "Mobility on Demand (MoD) services, like Uber and Lyft, are revolutionizing the way people move in cities around the world and are often considered a convenient alternative to public transit, since they offer higher Quality of Service (QoS - less waiting time, door-to-door service) at a cheap price. In the next decades, these advantages are expected to be further amplified by Automated MoD (AMoD), in which drivers will be replaced by automated vehicles, with a big gain in terms of cost-efficiency. MoD is usually intended as a door-to-door service. However, there has been recent interest toward consolidating, e.g., aggregating, the travel demand by limiting the number of admitted stop locations. This implies users have to walk fromto their intended origindestination. The contribution of this paper is a systematic study the impact of consolidation on the operator cost and on user QoS. We introduce a MoD system where pick-ups and drop-offs can only occur in a limited subset of admitted stop locations. The density of such locations is a system parameter: the less the density, the more the user demand is consolidated. We show that, by decreasing stop density, we can increase system capacity (number of passengers we are able to serve). On the contrary, increasing it, we can improve QoS. The system is tested in AMoDSim, an open-source simulator. The code to reproduce the results presented here is available on-line. This work is a first step toward flexible mobility services that are able to autonomously re-configure themselves, favoring capacity or QoS, depending on the amount of travel demand coming from users. In other words, the services we envisage in this work shift their operational mode to any intermediate point in the range from a taxi-like door-to-door service to a bus-like service, with few served stops and more passengers on-board.", "label": 1, "source": "scigen_human", "idx": 211, "lang": "en"}
{"text": "Motivation. Diffusion-based network models are widely used for protein function prediction using protein network data and have been shown to outperform neighborhood-based and module-based methods. Recent studies have shown that integrating the hierarchical structure of the Gene Ontology (GO) data dramatically improves prediction accuracy. However, previous methods usually either used the GO hierarchy to refine the prediction results of multiple classifiers, or flattened the hierarchy into a function-function similarity kernel. No study has taken the GO hierarchy into account together with the protein network as a two-layer network model. Results. We first construct a Bi-relational graph (Birg) model comprised of both protein-protein association and function-function hierarchical networks. We then propose two diffusion-based methods, BirgRank and AptRank, both of which use PageRank to diffuse information on this two-layer graph model. BirgRank is a direct application of traditional PageRank with fixed decay parameters. In contrast, AptRank utilizes an adaptive diffusion mechanism to improve the performance of BirgRank. We evaluate the ability of both methods to predict protein function on yeast, fly, and human protein datasets, and compare with four previous methods: GeneMANIA, TMC, ProteinRank and clusDCA. We design three different validation strategies: missing function prediction, de novo function prediction, and guided function prediction to comprehensively evaluate predictability of all six methods. We find that both BirgRank and AptRank outperform the previous methods, especially in missing function prediction when using only 10 of the data for training. Conclusion. AptRank naturally combines protein-protein associations and the GO function-function hierarchy into a two-layer network model without flattening the hierarchy into a similarity kernel. Introducing an adaptive mechanism to the traditional, fixed-parameter model of PageRank greatly improves the accuracy of protein function prediction. Code. . Contact.", "label": 1, "source": "scigen_human", "idx": 212, "lang": "en"}
{"text": "Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete monophonic musical pieces, conditioned on surrounding context, and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architectures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process.", "label": 1, "source": "scigen_human", "idx": 213, "lang": "en"}
{"text": "In this paper, we investigate the encoding circuit size of Hamming codes and Hadamard codes. To begin with, we prove the exact lower bound of circuit size required in the encoding of (punctured) Hadamard codes and (extended) Hamming codes. Then the encoding algorithms for (punctured) Hadamard codes are presented to achieve the derived lower bounds. For (extended) Hamming codes, we also propose encoding algorithms that achieve the lower bounds.", "label": 1, "source": "scigen_human", "idx": 214, "lang": "en"}
{"text": "Many photography websites such as Flickr, 500px, Unsplash, and Adobe Behance are used by amateur and professional photography enthusiasts. Unlike content-based image search, such users of photography websites are not just looking for photos with certain content, but more generally for photos with a certain photographic \"aesthetic.\" In this context, we explore personalized photo recommendation and propose two aesthetic feature extraction methods based on (i) color space and (ii) deep style transfer embeddings. Using a dataset from 500px, we evaluate how these features can be best leveraged by collaborative filtering methods and show that (ii) provides a significant boost in photo recommendation performance.", "label": 1, "source": "scigen_human", "idx": 216, "lang": "en"}
{"text": "We present SmartLoc , a localization system to estimate the location and the traveling distance by leveraging the lower-power inertial sensors embedded in smartphones as a supplementary to GPS. To minimize the negative impact of sensor noises, SmartLoc exploits the intermittent strong GPS signals and uses the linear regression to build a prediction model which is based on the trace estimated from inertial sensors and the one computed from the GPS. Furthermore, we utilize landmarks (e.g. , bridge, traffic lights) detected automatically and special driving patterns (e.g. , turning, uphill, and downhill) from inertial sensory data to improve the localization accuracy when the GPS signal is weak. Our evaluations of SmartLoc in the city demonstrates its technique viability and significant localization accuracy improvement compared with GPS and other approaches: the error is approximately 20 m for 90 of time while the known mean error of GPS is 42.22 m.", "label": 1, "source": "scigen_human", "idx": 217, "lang": "en"}
{"text": "NLOS4D2D", "label": 1, "source": "scigen_human", "idx": 250, "lang": "zh"}
{"text": "123123JEEJEE", "label": 1, "source": "scigen_human", "idx": 251, "lang": "zh"}
{"text": "CommonsenseQAConceptNet12247BERT5689", "label": 1, "source": "scigen_human", "idx": 252, "lang": "zh"}
{"text": "Intel Software Guard ExtensionsLeNet-5IID98.78IID97.6034.851.8MobileNetV2IID85.40IID81.6615.851.2", "label": 1, "source": "scigen_human", "idx": 253, "lang": "zh"}
{"text": "iKalaDSD100", "label": 1, "source": "scigen_human", "idx": 254, "lang": "zh"}
{"text": "KKRFWSHOGK-clusterKRFWSAPRAPR2D3D-APR3DAPR3D-APRIBUGLBF20300-W", "label": 1, "source": "scigen_human", "idx": 255, "lang": "zh"}
{"text": "AP5GAPAP5G", "label": 1, "source": "scigen_human", "idx": 256, "lang": "zh"}
{"text": "ChimeraEAChimeraIsingSidonIsing1000GAEDAEAGAEDAIsing", "label": 1, "source": "scigen_human", "idx": 257, "lang": "zh"}
{"text": "DNNDNNDNNDNNQUANOSANSDNNANSANSQUANOSQUANOSQUANOSMACCIFAR10CIFAR100QUANOS83.452QUANOS10QUANOS-5 16", "label": 1, "source": "scigen_human", "idx": 258, "lang": "zh"}
{"text": "HETHETHybrid LSTMHETHETRRM", "label": 1, "source": "scigen_human", "idx": 259, "lang": "zh"}
{"text": "5000Bharatanatyam Adavu sAdavuKinectRGB-DBharatanatyam AdavuLabanonotation", "label": 1, "source": "scigen_human", "idx": 260, "lang": "zh"}
{"text": "LipschitzO1T", "label": 1, "source": "scigen_human", "idx": 261, "lang": "zh"}
{"text": "Encoder-DecDEncDec-ADEncDec ADEncDec AD30500", "label": 1, "source": "scigen_human", "idx": 262, "lang": "zh"}
{"text": "LQRiccati", "label": 1, "source": "scigen_human", "idx": 263, "lang": "zh"}
{"text": "Bode", "label": 1, "source": "scigen_human", "idx": 264, "lang": "zh"}
{"text": "SimexPalSimexPalSimexPal", "label": 1, "source": "scigen_human", "idx": 265, "lang": "zh"}
{"text": "GRADEEGRGRADEkPearsonSpearmanGRADE", "label": 1, "source": "scigen_human", "idx": 267, "lang": "zh"}
{"text": "IPPASCALRTLSCAFSM", "label": 1, "source": "scigen_human", "idx": 268, "lang": "zh"}
{"text": "copula", "label": 1, "source": "scigen_human", "idx": 269, "lang": "zh"}
{"text": "MCCMCCCAPMCCCAPNPCAPSDRMUMTOCAPMUMTO-CCAPMUMTO SDRCAPCAP", "label": 1, "source": "scigen_human", "idx": 270, "lang": "zh"}
{"text": "Osbornen nRnL2OsborneLppOsborneLpO-2n9kKOsborneL2Lpp2017SODALpSchulmanSinclairSTOC 2015L", "label": 1, "source": "scigen_human", "idx": 271, "lang": "zh"}
{"text": "LaTeXACM SIG ProceedingsLaTeX 2BibTeXACM SIGLaTeX 2BibTeXLaTeXBibTeXdviPDF", "label": 1, "source": "scigen_human", "idx": 272, "lang": "zh"}
{"text": "t-t-SNEt-SNEt-SNEt-viSNEt-SNEt-SNEt-viSNEt-SNEt-SNE", "label": 1, "source": "scigen_human", "idx": 273, "lang": "zh"}
{"text": "GANs", "label": 1, "source": "scigen_human", "idx": 274, "lang": "zh"}
{"text": "IID23", "label": 1, "source": "scigen_human", "idx": 275, "lang": "zh"}
{"text": "PDE", "label": 1, "source": "scigen_human", "idx": 276, "lang": "zh"}
{"text": "AoIAoIAoI", "label": 1, "source": "scigen_human", "idx": 277, "lang": "zh"}
{"text": "GDPRGDPRGDPR", "label": 1, "source": "scigen_human", "idx": 278, "lang": "zh"}
{"text": "mHealthN39EMMAEMotion Aware mHealth AgentEMMAmHealth", "label": 1, "source": "scigen_human", "idx": 279, "lang": "zh"}
{"text": "SoundNetSoundNet1.2", "label": 1, "source": "scigen_human", "idx": 280, "lang": "zh"}
{"text": "SCLCRCLDPCSCLR-SCL20481024CRCR-SCL258SCLCRC163848192CRCR-SCL85200.04dB", "label": 1, "source": "scigen_human", "idx": 281, "lang": "zh"}
{"text": "MILsoftmaxF111.410.2MIL", "label": 1, "source": "scigen_human", "idx": 283, "lang": "zh"}
{"text": "LjpDtGsLjvLjpLjvLjpDtGNscOutexTC10OutexTC12KTH-TIPSBrodatzCUReT10099.9299.7599.1699.65", "label": 1, "source": "scigen_human", "idx": 284, "lang": "zh"}
{"text": "NSREXFOR", "label": 1, "source": "scigen_human", "idx": 285, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 287, "lang": "zh"}
{"text": "WaterfillingWaterfillingTorTorPSShadowWaterfillingWaterfillingShadowWaterfillingWaterfillingTorTor", "label": 1, "source": "scigen_human", "idx": 288, "lang": "zh"}
{"text": "RmR nx R nRWuVerdu2010mx", "label": 1, "source": "scigen_human", "idx": 289, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 290, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 291, "lang": "zh"}
{"text": "BoutillierDarwishePearlLeviHarperKoniecznyPino-Perez", "label": 1, "source": "scigen_human", "idx": 292, "lang": "zh"}
{"text": "NLOSNLOSMINTMINTNLOS", "label": 1, "source": "scigen_human", "idx": 293, "lang": "zh"}
{"text": "JELD03L31", "label": 1, "source": "scigen_human", "idx": 294, "lang": "zh"}
{"text": "CNN-CRFMSCCCNNmU-Net-B3CRFVGG-16EM420EMMSCC355 MB103 MBDiceJaccardRecallAccuracy85.2477.4282.2796.7687.1379.7487.1296.9122.5820.26MSCCEM", "label": 1, "source": "scigen_human", "idx": 295, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 296, "lang": "zh"}
{"text": "SGDPolyak ojasewicztop-k", "label": 1, "source": "scigen_human", "idx": 297, "lang": "zh"}
{"text": "3DARVRCFL3603DEquiConvsEquiConvs", "label": 1, "source": "scigen_human", "idx": 298, "lang": "zh"}
{"text": "DNN0-10-10-1CLCL0-1CL", "label": 1, "source": "scigen_human", "idx": 299, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 300, "lang": "zh"}
{"text": "81.5011.602", "label": 1, "source": "scigen_human", "idx": 301, "lang": "zh"}
{"text": "SGDFisherFisher", "label": 1, "source": "scigen_human", "idx": 302, "lang": "zh"}
{"text": "BISTKripkeBISTBiSKtBISKtHilbertBISKBISTBiSKt", "label": 1, "source": "scigen_human", "idx": 303, "lang": "zh"}
{"text": "RTSCNNCNNPuppet SearchRTSRTSRTS", "label": 1, "source": "scigen_human", "idx": 304, "lang": "zh"}
{"text": "abWordNet79.3", "label": 1, "source": "scigen_human", "idx": 305, "lang": "zh"}
{"text": "BERTGPT-2LightPAFFLightPAFFBERTGPT-2MASSBERTGPU-2MASSLightPAFF557", "label": 1, "source": "scigen_human", "idx": 306, "lang": "zh"}
{"text": "LIMEGrad-CAMGrad CAMLIME80Grad CAM", "label": 1, "source": "scigen_human", "idx": 307, "lang": "zh"}
{"text": "DNNDNNDNNDNNDNNDNNImageNet ILSVRC 2012Jetson TX2DNNDNN7.521.8", "label": 1, "source": "scigen_human", "idx": 308, "lang": "zh"}
{"text": "NLOSNLOS", "label": 1, "source": "scigen_human", "idx": 309, "lang": "zh"}
{"text": "DNN-DNNDNNDNNDNNDNN", "label": 1, "source": "scigen_human", "idx": 310, "lang": "zh"}
{"text": "VLCDLDL-DLVLCDLVLCVLC", "label": 1, "source": "scigen_human", "idx": 311, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 312, "lang": "zh"}
{"text": "MediaEval 2018LIRIS-ACCEDEGRUMediaEval 2018", "label": 1, "source": "scigen_human", "idx": 313, "lang": "zh"}
{"text": "TandonGCGCGDSGDLDGMSGDGC", "label": 1, "source": "scigen_human", "idx": 314, "lang": "zh"}
{"text": "UAVUGV", "label": 1, "source": "scigen_human", "idx": 315, "lang": "zh"}
{"text": "LandauerMaxwell-Ladymank T ln 2LandauerNorton LadymanLandauer", "label": 1, "source": "scigen_human", "idx": 316, "lang": "zh"}
{"text": "TlogTT23MABUCBDSEE", "label": 1, "source": "scigen_human", "idx": 317, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 318, "lang": "zh"}
{"text": "logn", "label": 1, "source": "scigen_human", "idx": 319, "lang": "zh"}
{"text": "MBSP", "label": 1, "source": "scigen_human", "idx": 320, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 321, "lang": "zh"}
{"text": "TyperTyperuserCAPTCHACAPTCHAwebweb20Typer", "label": 1, "source": "scigen_human", "idx": 322, "lang": "zh"}
{"text": "QQ", "label": 1, "source": "scigen_human", "idx": 323, "lang": "zh"}
{"text": "LipschitzLipschitzLipschitzBregmanBregmanLipschitz", "label": 1, "source": "scigen_human", "idx": 324, "lang": "zh"}
{"text": "DPLLSATDPLLHaskellDPLLMinlog", "label": 1, "source": "scigen_human", "idx": 325, "lang": "zh"}
{"text": "S cene G ated S social G graphSGSGSGSGVAESGSG", "label": 1, "source": "scigen_human", "idx": 326, "lang": "zh"}
{"text": "Lehmann-SheffeMVUMVUMVUMVU", "label": 1, "source": "scigen_human", "idx": 327, "lang": "zh"}
{"text": "i.i.d.PSD1-PSDPSDPSD", "label": 1, "source": "scigen_human", "idx": 328, "lang": "zh"}
{"text": "10JPEGJPEG", "label": 1, "source": "scigen_human", "idx": 329, "lang": "zh"}
{"text": "LTISDPGramianKYPSDPSDPgramman", "label": 1, "source": "scigen_human", "idx": 330, "lang": "zh"}
{"text": "1950NPNPnGnGond-d-", "label": 1, "source": "scigen_human", "idx": 331, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 333, "lang": "zh"}
{"text": "IPFi ii1IPF2.5mmIPFIPF", "label": 1, "source": "scigen_human", "idx": 334, "lang": "zh"}
{"text": "JavaScriptJavaScriptJavaScriptJavaScript18JavaScriptJavaScriptJavaScript", "label": 1, "source": "scigen_human", "idx": 335, "lang": "zh"}
{"text": "BERTCoNLL 2018iii", "label": 1, "source": "scigen_human", "idx": 336, "lang": "zh"}
{"text": "-", "label": 1, "source": "scigen_human", "idx": 337, "lang": "zh"}
{"text": "-", "label": 1, "source": "scigen_human", "idx": 338, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 339, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 340, "lang": "zh"}
{"text": "HOIHOIHOIHICOHOI1036.1 mAP 39.9 mAP", "label": 1, "source": "scigen_human", "idx": 341, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 342, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 344, "lang": "zh"}
{"text": "logitlogitsl2logitsSKDlogitsSOTA", "label": 1, "source": "scigen_human", "idx": 345, "lang": "zh"}
{"text": "GalerkinLDGStokesV-LDGiStokesiiStokes", "label": 1, "source": "scigen_human", "idx": 346, "lang": "zh"}
{"text": "dCGPANNdCGPANN", "label": 1, "source": "scigen_human", "idx": 347, "lang": "zh"}
{"text": "word2vecW2Vw xw yW2V", "label": 1, "source": "scigen_human", "idx": 348, "lang": "zh"}
{"text": "80K-100", "label": 1, "source": "scigen_human", "idx": 349, "lang": "zh"}
{"text": "emph{}AAAAPSPACEAA", "label": 1, "source": "scigen_human", "idx": 350, "lang": "zh"}
{"text": "5G300500MNOCapEX255G5GMVNOMNO5G40605GMNOTowerCosabc5G5G5G", "label": 1, "source": "scigen_human", "idx": 351, "lang": "zh"}
{"text": "OWG NPCAAA OWG", "label": 1, "source": "scigen_human", "idx": 352, "lang": "zh"}
{"text": "11abc", "label": 1, "source": "scigen_human", "idx": 353, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 354, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 355, "lang": "zh"}
{"text": "RNNRNNRNNDMFTRNNRNNRMTDMFTovRMTovWainrib TouboulRNNDMFTRMTRNNMartin Siggia-RoseRNNDMFT", "label": 1, "source": "scigen_human", "idx": 356, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 357, "lang": "zh"}
{"text": "MaskMask R-CNN1500", "label": 1, "source": "scigen_human", "idx": 358, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 359, "lang": "zh"}
{"text": "ACTFBACFBACPEIFBACFBAC", "label": 1, "source": "scigen_human", "idx": 360, "lang": "zh"}
{"text": "i.i.d.", "label": 1, "source": "scigen_human", "idx": 361, "lang": "zh"}
{"text": "Marcello1997SoloveichikSeanCAINXMLtranspythonCAINSoloveichikDNACRNAndrews PhillipsDSDDSDManish GuptaCRN2DSDCAINMicrosoftVisual DSD", "label": 1, "source": "scigen_human", "idx": 362, "lang": "zh"}
{"text": "13431", "label": 1, "source": "scigen_human", "idx": 363, "lang": "zh"}
{"text": "CPDPCPDP1034", "label": 1, "source": "scigen_human", "idx": 364, "lang": "zh"}
{"text": "LSTMLSTMLSTMCell-aware Stacked LSTMCAS-LSTMLSTM", "label": 1, "source": "scigen_human", "idx": 365, "lang": "zh"}
{"text": "LISLISRicianSAASPG-CPLISLIS", "label": 1, "source": "scigen_human", "idx": 366, "lang": "zh"}
{"text": "RGB-IR", "label": 1, "source": "scigen_human", "idx": 367, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 368, "lang": "zh"}
{"text": "word2vec", "label": 1, "source": "scigen_human", "idx": 369, "lang": "zh"}
{"text": "90153025/99", "label": 1, "source": "scigen_human", "idx": 370, "lang": "zh"}
{"text": "ABoxesDL-LiteDL-LiteDL Lite", "label": 1, "source": "scigen_human", "idx": 371, "lang": "zh"}
{"text": "LandiReeb1-Lipschitz", "label": 1, "source": "scigen_human", "idx": 372, "lang": "zh"}
{"text": "CDSSEHRCDSSEHRICD9CDSSCDSSICD9AUROCEHR15AUPRC40", "label": 1, "source": "scigen_human", "idx": 373, "lang": "zh"}
{"text": "SteinernGtR VGkGtRV RkRVGRSteinerNPSteinernOtkkSteiner2OknO1FPTSteinerkSteinerW[1hard", "label": 1, "source": "scigen_human", "idx": 374, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 375, "lang": "zh"}
{"text": "IOHardwareHybridHACCSZHACCSZ", "label": 1, "source": "scigen_human", "idx": 376, "lang": "zh"}
{"text": "AFAFADFsDungADF", "label": 1, "source": "scigen_human", "idx": 377, "lang": "zh"}
{"text": "SGHMCSGHMCi.i.d", "label": 1, "source": "scigen_human", "idx": 378, "lang": "zh"}
{"text": "FakeHealth-", "label": 1, "source": "scigen_human", "idx": 379, "lang": "zh"}
{"text": "90", "label": 1, "source": "scigen_human", "idx": 380, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 381, "lang": "zh"}
{"text": "CACACARiposteCA18kCACACA", "label": 1, "source": "scigen_human", "idx": 382, "lang": "zh"}
{"text": "microPhantommicroRTS2020microRTS AImicroPhantomPOAdaptive20182019microRTS AImicroRTSmicroPhantom", "label": 1, "source": "scigen_human", "idx": 383, "lang": "zh"}
{"text": "WaterFowlRDFRDFRDFSe", "label": 1, "source": "scigen_human", "idx": 384, "lang": "zh"}
{"text": "SANDANSANDANSANDANSAN", "label": 1, "source": "scigen_human", "idx": 385, "lang": "zh"}
{"text": "CDNCDN123CDNSDNQoEQoS2CDN", "label": 1, "source": "scigen_human", "idx": 386, "lang": "zh"}
{"text": "23MagnatagatuneMillion SongDCNNmel", "label": 1, "source": "scigen_human", "idx": 387, "lang": "zh"}
{"text": "SDDRAM", "label": 1, "source": "scigen_human", "idx": 388, "lang": "zh"}
{"text": "HARKing", "label": 1, "source": "scigen_human", "idx": 389, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 390, "lang": "zh"}
{"text": "GloVeword2vecConceptNetPPDB.59616", "label": 1, "source": "scigen_human", "idx": 391, "lang": "zh"}
{"text": "QDQNDQNActor-Critic", "label": 1, "source": "scigen_human", "idx": 392, "lang": "zh"}
{"text": "iiiMLMLCNNsML", "label": 1, "source": "scigen_human", "idx": 393, "lang": "zh"}
{"text": "MapleMagnus88Magnus", "label": 1, "source": "scigen_human", "idx": 394, "lang": "zh"}
{"text": "MaxSATMaxSATMaxSATMaxSATHornMaxSATHorn MaxSATHornHorn-MaxSAT", "label": 1, "source": "scigen_human", "idx": 395, "lang": "zh"}
{"text": "AIS aTh12-0.083ms0.100ms0.116ms0.145ms", "label": 1, "source": "scigen_human", "idx": 397, "lang": "zh"}
{"text": "Explorer", "label": 1, "source": "scigen_human", "idx": 398, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 399, "lang": "zh"}
{"text": "Burgers", "label": 1, "source": "scigen_human", "idx": 400, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 401, "lang": "zh"}
{"text": "StiefelojasewiczMorse Bott1Stiefel-1LROATS-LROATHOPMS-HOPM", "label": 1, "source": "scigen_human", "idx": 402, "lang": "zh"}
{"text": "AlphaGoMuzeroCritic PI2-Critic PI2", "label": 1, "source": "scigen_human", "idx": 403, "lang": "zh"}
{"text": "MSC-RNNMILRNNRNNRNNMSC-RNN0.972ML3", "label": 1, "source": "scigen_human", "idx": 405, "lang": "zh"}
{"text": "DNNLSTMSNRPL-CRNNPL-CRNNPL-DNNPL-LSTMCRNNPL-DNNPL-LSTMCRNNPL-CRNN939792", "label": 1, "source": "scigen_human", "idx": 406, "lang": "zh"}
{"text": "Watts-StrogatzWSErdos-RenyiERERWS", "label": 1, "source": "scigen_human", "idx": 407, "lang": "zh"}
{"text": "-BS-", "label": 1, "source": "scigen_human", "idx": 408, "lang": "zh"}
{"text": "iiiiiiArch", "label": 1, "source": "scigen_human", "idx": 409, "lang": "zh"}
{"text": "10F", "label": 1, "source": "scigen_human", "idx": 410, "lang": "zh"}
{"text": "20145-11%", "label": 1, "source": "scigen_human", "idx": 411, "lang": "zh"}
{"text": "RESTwebwebTypeScriptJavaScriptRESTSafeRESTScriptSRSRESTRESTSafeRESTScriptJavaScriptiiiRESTAPIHeadRESTHoareRESTAPIBoogieSafeRESTScriptEclipseSafeRESTScript", "label": 1, "source": "scigen_human", "idx": 412, "lang": "zh"}
{"text": "GPiDWhy3", "label": 1, "source": "scigen_human", "idx": 413, "lang": "zh"}
{"text": "CSPCSPpromise CSPpromise CSPpromise CSPpromise CSCSPSchaefer", "label": 1, "source": "scigen_human", "idx": 414, "lang": "zh"}
{"text": "CSICSI", "label": 1, "source": "scigen_human", "idx": 415, "lang": "zh"}
{"text": "SomethingSomething", "label": 1, "source": "scigen_human", "idx": 416, "lang": "zh"}
{"text": "ADMM", "label": 1, "source": "scigen_human", "idx": 417, "lang": "zh"}
{"text": "SSNESSNEHSNHAMSNHAMd", "label": 1, "source": "scigen_human", "idx": 418, "lang": "zh"}
{"text": "iii", "label": 1, "source": "scigen_human", "idx": 419, "lang": "zh"}
{"text": "WiFi", "label": 1, "source": "scigen_human", "idx": 420, "lang": "zh"}
{"text": "ohyphenscl-cpsc MAB-EXc MAB-EX", "label": 1, "source": "scigen_human", "idx": 421, "lang": "zh"}
{"text": "AutoMLMLAutoMLAuto PyTorchAutoDLAuto PyTorchDNNAutoDLDNNLCBenchAutoMLPyTorchPyTorc", "label": 1, "source": "scigen_human", "idx": 422, "lang": "zh"}
{"text": "DNRCCERNNi DNRCCEiiMIMIC-IIIIIILSTMLSTM-CRFCRFLSTM-CRFDrugBankMedLinei2b2VADNRCCE", "label": 1, "source": "scigen_human", "idx": 423, "lang": "zh"}
{"text": "MetricsVisMetricsVis12 3 4", "label": 1, "source": "scigen_human", "idx": 424, "lang": "zh"}
{"text": "-MRLMRLn 34n 31Simpson5Likert-", "label": 1, "source": "scigen_human", "idx": 425, "lang": "zh"}
{"text": "SchwarzSchwarz", "label": 1, "source": "scigen_human", "idx": 426, "lang": "zh"}
{"text": "2010150001000002003-2010", "label": 1, "source": "scigen_human", "idx": 427, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 428, "lang": "zh"}
{"text": "BCA", "label": 1, "source": "scigen_human", "idx": 429, "lang": "zh"}
{"text": "MetalliaxisMUSEMetalliaxisMUSE183710255080", "label": 1, "source": "scigen_human", "idx": 430, "lang": "zh"}
{"text": "17", "label": 1, "source": "scigen_human", "idx": 431, "lang": "zh"}
{"text": "MDPVIRHOVIRHO", "label": 1, "source": "scigen_human", "idx": 432, "lang": "zh"}
{"text": "DNNRLDNN", "label": 1, "source": "scigen_human", "idx": 433, "lang": "zh"}
{"text": "5", "label": 1, "source": "scigen_human", "idx": 434, "lang": "zh"}
{"text": "PPESPPESPPES", "label": 1, "source": "scigen_human", "idx": 435, "lang": "zh"}
{"text": "12NLPNLP11 112015EMNLP", "label": 1, "source": "scigen_human", "idx": 436, "lang": "zh"}
{"text": "agent", "label": 1, "source": "scigen_human", "idx": 437, "lang": "zh"}
{"text": "ERS", "label": 1, "source": "scigen_human", "idx": 438, "lang": "zh"}
{"text": "HetNetHetNetC-SBSAPC-SBSMUIPC-SBSAPC-SBSMBSMUMBSMUMBSMU60AP", "label": 1, "source": "scigen_human", "idx": 439, "lang": "zh"}
{"text": "VNP-NP-VNPMod p p p polyVNP-hardVPVNP-Burgisser2000GrochowVP-Durand2014VP-VBP", "label": 1, "source": "scigen_human", "idx": 440, "lang": "zh"}
{"text": "3D3Dskeleton BoxesPKU-MMD", "label": 1, "source": "scigen_human", "idx": 441, "lang": "zh"}
{"text": "GANMC-GANMC-GANMC-GANCaltech-200Oxford-102128 128MC-GAN11 11", "label": 1, "source": "scigen_human", "idx": 442, "lang": "zh"}
{"text": "VMDVVMDV", "label": 1, "source": "scigen_human", "idx": 443, "lang": "zh"}
{"text": "MovieLens-20MRecSys15NCG", "label": 1, "source": "scigen_human", "idx": 444, "lang": "zh"}
{"text": "NASCIFAR-102.46ImageNet23.7NAS-Bench-201", "label": 1, "source": "scigen_human", "idx": 445, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 446, "lang": "zh"}
{"text": "Twitter", "label": 1, "source": "scigen_human", "idx": 447, "lang": "zh"}
{"text": "Accel12ResNet-18ResNet-101-AccelAccelAccel", "label": 1, "source": "scigen_human", "idx": 448, "lang": "zh"}
{"text": "2", "label": 1, "source": "scigen_human", "idx": 449, "lang": "zh"}
{"text": "10000CUB-92.31Birdsnap85.41FGVC93.42Stanford Dogs80.81", "label": 1, "source": "scigen_human", "idx": 450, "lang": "zh"}
{"text": "BanachBanachBanachBanachBanachBanachCLSCLSP-LCPKKT", "label": 1, "source": "scigen_human", "idx": 451, "lang": "zh"}
{"text": "CAD3D3D2D", "label": 1, "source": "scigen_human", "idx": 452, "lang": "zh"}
{"text": "PBE", "label": 1, "source": "scigen_human", "idx": 453, "lang": "zh"}
{"text": "NLPfugashiPythonMeCab", "label": 1, "source": "scigen_human", "idx": 454, "lang": "zh"}
{"text": "GZSLZSLSDZSLGZSL8.521.9", "label": 1, "source": "scigen_human", "idx": 455, "lang": "zh"}
{"text": "", "label": 1, "source": "scigen_human", "idx": 457, "lang": "zh"}
