{"text": "One issue limiting the adaption of large-scale multi-region segmentation is the sometimes prohibitive memory requirements. This is especially troubling considering advances in massively parallel computing and commercial graphics processing units because of their already limited memory compared to the current random access memory used in more traditional computation. To address this issue in the field of continuous max-flow segmentation, we have developed a pseudo-flow framework using the theory of Bregman proximal projections and entropic distances which implicitly represents flow variables between labels and designated source and sink nodes. This reduces the memory requirements for max-flow segmentation by approximately 20 for Potts models and approximately 30 for hierarchical max-flow (HMF) and directed acyclic graph max-flow (DAGMF) models. This represents a great improvement in the state-of-the-art in max-flow segmentation, allowing for much larger problems to be addressed and accelerated using commercially available graphics processing hardware.", "label": 1, "source": "scigen_human", "idx": 1, "lang": "en"}
{"text": "Long short-term memory (LSTM) and recurrent neural network (RNN) has achieved great successes on time-series prediction. In this paper, a methodology of using LSTM-based deep-RNN for two-phase flow regime prediction is proposed, motivated by previous research on constructing deep RNN. The method is featured with fast response and accuracy. The built RNN networks are trained and tested with time-series void fraction data collected using impedance void meter. The result shows that the prediction accuracy depends on the depth of network and the number of layer cells. However, deeper and larger network consumes more time in predicting.", "label": 1, "source": "scigen_human", "idx": 2, "lang": "en"}
{"text": "Visual Place Recognition (VPR) is the ability to correctly recall a previously visited place under changing viewpoints and appearances. A large number of handcrafted and deep-learning-based VPR techniques exist, where the former suffer from appearance changes and the latter have significant computational needs. In this paper, we present a new handcrafted VPR technique that achieves state-of-the-art place matching performance under challenging conditions. Our technique combines the best of 2 existing trainingless VPR techniques, SeqSLAM and CoHOG, which are each robust to conditional and viewpoint changes, respectively. This blend, namely ConvSequential-SLAM, utilises sequential information and block-normalisation to handle appearance changes, while using regional-convolutional matching to achieve viewpoint-invariance. We analyse content-overlap in-between query frames to find a minimum sequence length, while also re-using the image entropy information for environment-based sequence length tuning. State-of-the-art performance is reported in contrast to 8 contemporary VPR techniques on 4 public datasets. Qualitative insights and an ablation study on sequence length are also provided.", "label": 1, "source": "scigen_human", "idx": 3, "lang": "en"}
{"text": "We do a Probabilistic Analysis of the Network generated by robots involved inStochastic Boundary Coverage", "label": 1, "source": "scigen_human", "idx": 4, "lang": "en"}
{"text": "The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multi-layer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task.", "label": 1, "source": "scigen_human", "idx": 5, "lang": "en"}
{"text": "In the domain of emergency management during hazard crises, having sufficient situational awareness information is critical. It requires capturing and integrating information from sources such as satellite images, local sensors and social media content generated by local people. A bold obstacle to capturing, representing and integrating such heterogeneous and diverse information is lack of a proper ontology which properly conceptualizes this domain, aggregates and unifies datasets. Thus, in this paper, we introduce empathi ontology which conceptualizes the core concepts concerning with the domain of emergency managing and planning of hazard crises. Although empathi has a coarse-grained view, it considers the necessary concepts and relations being essential in this domain. This ontology is available at", "label": 1, "source": "scigen_human", "idx": 6, "lang": "en"}
{"text": "Audio-based cover song detection has received much attention in the MIR community in the recent years. To date, the most popular formulation of the problem has been to compare the audio signals of two tracks and to make a binary decision based on this information only. However, leveraging additional signals might be key if one wants to solve the problem at an industrial scale. In this paper, we introduce an ensemble-based method that approaches the problem from a many-to-many perspective. Instead of considering pairs of tracks in isolation, we consider larger sets of potential versions for a given composition, and create and exploit the graph of relationships between these tracks. We show that this can result in a significant improvement in performance, in particular when the number of existing versions of a given composition is large.", "label": 1, "source": "scigen_human", "idx": 7, "lang": "en"}
{"text": "Underwater imagery has enabled numerous civilian applications in various domains, ranging from academia to industry, and from industrial surveillance and maintenance to environmental protection and behavior of marine creatures studies. The accumulation of litter and plastic debris at the seafloor and the bottom of rivers are extremely harmful for the aquatic life. We propose a solution for monitoring this problem using a team of Autonomous Underwater Vehicles (AUVs) to exchange the recorded video in order to reconstruct the map of regions of interest. However, underwater video transmission is a challenge in the harsh environment in which radio-frequency waves are absorbed for distances above a few tens of meters, optical waves require narrow laser beams and suffer from scattering and ocean wave motion, and acoustic waves - while long range - provide a very low bandwidth and unreliable channel for communication. In our solution, the scalable coded video of each vehicle is shared in-network with a selected group of receiving vehicles through the underwater acoustic channel. Presented evaluations, including both simulations and experiments, confirm the efficiency and flexibility of the proposed solution using acoustic software-defined modems.", "label": 1, "source": "scigen_human", "idx": 8, "lang": "en"}
{"text": "The paper presents a new, robust control algorithm for position trajectory tracking in a 3D space, dedicated to underactuated airships. In order to take into account real characteristics of such vehicles, and to reflect practically motivated constraints, the algorithm assumes a highly uncertain system dynamics model. The tracking problem is solved in a uniform way, without dividing it into subtasks considered in 2D spaces, thanks to the introduction of an auxiliary tracking error. The proposed controller is based on the sliding mode approach. Its stability is investigated using Lyapunov theorem. Numerical simulations are conducted in order to verify properties of a closed-loop system for a generic model of the airship. Performance of the control system is examined via experiments in various scenarios using a prototype airship. The obtained results indicate that the control objectives are satisfied in practice with a reasonable accuracy. Moreover, it is shown that the controller is robust to some bounded additive measurement perturbations and delays in the control loop.", "label": 1, "source": "scigen_human", "idx": 9, "lang": "en"}
{"text": "Object detection has been vigorously investigated for years but fast accurate detection for real-world scenes remains a very challenging problem. Overcoming drawbacks of single-stage detectors, we take aim at precisely detecting objects for static and temporal scenes in real time. Firstly, as a dual refinement mechanism, a novel anchor-offset detection is designed, which includes an anchor refinement, a feature location refinement, and a deformable detection head. This new detection mode is able to simultaneously perform two-step regression and capture accurate object features. Based on the anchor-offset detection, a dual refinement network (DRNet) is developed for high-performance static detection, where a multi-deformable head is further designed to leverage contextual information for describing objects. As for temporal detection in videos, temporal refinement networks (TRNet) and temporal dual refinement networks (TDRNet) are developed by propagating the refinement information across time. We also propose a soft refinement strategy to temporally match object motion with the previous refinement. Our proposed methods are evaluated on PASCAL VOC, COCO, and ImageNet VID datasets. Extensive comparisons on static and temporal detection verify the superiority of DRNet, TRNet, and TDRNet. Consequently, our developed approaches run in a fairly fast speed, and in the meantime achieve a significantly enhanced detection accuracy, i.e., 84.4 mAP on VOC 2007, 83.6 mAP on VOC 2012, 69.4 mAP on VID 2017, and 42.4 AP on COCO. Ultimately, producing encouraging results, our methods are applied to online underwater object detection and grasping with an autonomous system. Codes are publicly available at", "label": 1, "source": "scigen_human", "idx": 10, "lang": "en"}
{"text": "This paper presents a tool for addressing a key component in many algorithms for planning robot trajectories under uncertainty: evaluation of the safety of a robot whose actions are governed by a closed-loop feedback policy near a nominal planned trajectory. We describe an adaptive importance sampling Monte Carlo framework that enables the evaluation of a given control policy for satisfaction of a probabilistic collision avoidance constraint which also provides an associated certificate of accuracy (in the form of a confidence interval). In particular this adaptive technique is well-suited to addressing the complexities of rigid-body collision checking applied to non-linear robot dynamics. As a Monte Carlo method it is amenable to parallelization for computational tractability, and is generally applicable to a wide gamut of simulatable systems, including alternative noise models. Numerical experiments demonstrating the effectiveness of the adaptive importance sampling procedure are presented and discussed.", "label": 1, "source": "scigen_human", "idx": 12, "lang": "en"}
{"text": "Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors.", "label": 1, "source": "scigen_human", "idx": 13, "lang": "en"}
{"text": "We develop a general framework for proving rigorous guarantees on the performance of the EM algorithm and a variant known as gradient EM. Our analysis is divided into two parts: a treatment of these algorithms at the population level (in the limit of infinite data), followed by results that apply to updates based on a finite set of samples. First, we characterize the domain of attraction of any global maximizer of the population likelihood. This characterization is based on a novel view of the EM updates as a perturbed form of likelihood ascent, or in parallel, of the gradient EM updates as a perturbed form of standard gradient ascent. Leveraging this characterization, we then provide non-asymptotic guarantees on the EM and gradient EM algorithms when applied to a finite set of samples. We develop consequences of our general theory for three canonical examples of incomplete-data problems: mixture of Gaussians, mixture of regressions, and linear regression with covariates missing completely at random. In each case, our theory guarantees that with a suitable initialization, a relatively small number of EM (or gradient EM) steps will yield (with high probability) an estimate that is within statistical error of the MLE. We provide simulations to confirm this theoretically predicted behavior.", "label": 1, "source": "scigen_human", "idx": 14, "lang": "en"}
{"text": "Video-based person re-identification matches video clips of people across non-overlapping cameras. Most existing methods tackle this problem by encoding each video frame in its entirety and computing an aggregate representation across all frames. In practice, people are often partially occluded, which can corrupt the extracted features. Instead, we propose a new spatiotemporal attention model that automatically discovers a diverse set of distinctive body parts. This allows useful information to be extracted from all frames without succumbing to occlusions and misalignments. The network learns multiple spatial attention models and employs a diversity regularization term to ensure multiple models do not discover the same body part. Features extracted from local image regions are organized by spatial attention model and are combined using temporal attention. As a result, the network learns latent representations of the face, torso and other body parts using the best available image patches from the entire video sequence. Extensive evaluations on three datasets show that our framework outperforms the state-of-the-art approaches by large margins on multiple metrics.", "label": 1, "source": "scigen_human", "idx": 15, "lang": "en"}
{"text": "We propose a compressive sensing algorithm that exploits geometric properties of images to recover images of high quality from few measurements. The image reconstruction is done by iterating the two following steps: 1) estimation of normal vectors of the image level curves and 2) reconstruction of an image fitting the normal vectors, the compressed sensing measurements and the sparsity constraint. The proposed technique can naturally extend to non local operators and graphs to exploit the repetitive nature of textured images in order to recover fine detail structures. In both cases, the problem is reduced to a series of convex minimization problems that can be efficiently solved with a combination of variable splitting and augmented Lagrangian methods, leading to fast and easy-to-code algorithms. Extended experiments show a clear improvement over related state-of-the-art algorithms in the quality of the reconstructed images and the robustness of the proposed method to noise, different kind of images and reduced measurements.", "label": 1, "source": "scigen_human", "idx": 16, "lang": "en"}
{"text": "Quantum memories are a fundamental of any global-scale quantum Internet, high-performance quantum networking and near-term quantum computers. A main problem of quantum memories is the low retrieval efficiency of the quantum systems from the quantum registers of the quantum memory. Here, we define a novel quantum memory called high-retrieval-efficiency (HRE) quantum memory for near-term quantum devices. An HRE quantum memory unit integrates local unitary operations on its hardware level for the optimization of the readout procedure and utilizes the advanced techniques of quantum machine learning. We define the integrated unitary operations of an HRE quantum memory, prove the learning procedure, and evaluate the achievable output signal-to-noise ratio values. We prove that the local unitaries of an HRE quantum memory achieve the optimization of the readout procedure in an unsupervised manner without the use of any labeled data or training sequences. We show that the readout procedure of an HRE quantum memory is realized in a completely blind manner without any information about the input quantum system or about the unknown quantum operation of the quantum register. We evaluate the retrieval efficiency of an HRE quantum memory and the output SNR (signal-to-noise ratio). The results are particularly convenient for gate-model quantum computers and the near-term quantum devices of the quantum Internet.", "label": 1, "source": "scigen_human", "idx": 17, "lang": "en"}
{"text": "Achieving transparency in black-box deep learning algorithms is still an open challenge. High dimensional features and decisions given by deep neural networks (NN) require new algorithms and methods to expose its mechanisms. Current state-of-the-art NN interpretation methods (e.g. Saliency maps, DeepLIFT, LIME, etc.) focus more on the direct relationship between NN outputs and inputs rather than the NN structure and operations itself. In current deep NN operations, there is uncertainty over the exact role played by neurons with fixed activation functions. In this paper, we achieve partially explainable learning model by symbolically explaining the role of activation functions (AF) under a scalable topology. This is carried out by modelling the AFs as adaptive Gaussian Processes (GP), which sit within a novel scalable NN topology, based on the Kolmogorov-Arnold Superposition Theorem (KST). In this scalable NN architecture, the AFs are generated by GP interpolation between control points and can thus be tuned during the back-propagation procedure via gradient descent. The control points act as the core enabler to both local and global adjustability of AF, where the GP interpolation constrains the intrinsic autocorrelation to avoid over-fitting. We show that there exists a trade-off between the NN's expressive power and interpretation complexity, under linear KST topology scaling. To demonstrate this, we perform a case study on a binary classification dataset of banknote authentication. Our model converge at better precision rate than state-of-the-art SVM algorithms which indicates that we do not make performance sacrifices in our approach. Meanwhile, by quantitatively and qualitatively investigating the mapping relationship between inputs and output, our explainable model can provide interpretation over each of the one-dimensional attributes. These early results suggest that our model has the potential to act as the final interpretation layer for deep neural networks.", "label": 1, "source": "scigen_human", "idx": 18, "lang": "en"}
{"text": "We study the sensitivity to noise of permanent (X) 2 for random real and complex x n n Gaussian matrices X, and show that asymptotically the correlation between the noisy and noiseless outcomes tends to zero when the noise level is o (1) n. This suggests that, under certain reasonable noise models, the probability distributions produced by noisy BosonSampling are very sensitive to noise. We also show that when the amount of noise is constant the noisy value of permanent (X) 2 can be approximated efficiently on a classical computer. These results seem to weaken the possibility of demonstrating quantum-speedup via BosonSampling without quantum fault-tolerance.", "label": 1, "source": "scigen_human", "idx": 19, "lang": "en"}
{"text": "Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first highlight both clinical needs and technical challenges in medical imaging and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.", "label": 1, "source": "scigen_human", "idx": 20, "lang": "en"}
{"text": "Most theoretical frameworks that focus on data errors and inconsistencies follow logic-based reasoning. Yet, practical data cleaning tools need to incorporate statistical reasoning to be effective in real-world data cleaning tasks. Motivated by empirical successes, we propose a formal framework for unclean databases, where two types of statistical knowledge are incorporated: The first represents a belief of how intended (clean) data is generated, and the second represents a belief of how noise is introduced in the actual observed database. To capture this noisy channel model, we introduce the concept of a Probabilistic Unclean Database (PUD), a triple that consists of a probabilistic database that we call the intention, a probabilistic data transformator that we call the realization and captures how noise is introduced, and an observed unclean database that we call the observation. We define three computational problems in the PUD framework: cleaning (infer the most probable intended database, given a PUD), probabilistic query answering (compute the probability of an answer tuple over the unclean observed database), and learning (estimate the most likely intention and realization models of a PUD, given examples as training data). We illustrate the PUD framework on concrete representations of the intention and realization, show that they generalize traditional concepts of repairs such as cardinality and value repairs, draw connections to consistent query answering, and prove tractability results. We further show that parameters can be learned in some practical instantiations, and in fact, prove that under certain conditions we can learn a PUD directly from a single dirty database without any need for clean examples.", "label": 1, "source": "scigen_human", "idx": 21, "lang": "en"}
{"text": "GPU accelerators have had a notable impact on high-performance computing across many disciplines. They provide high performance with low costpower, and therefore have become a primary compute resource on many of the largest supercomputers. Here, we implement multi-GPU acceleration into our Solar MHD code (MAS) using OpenACC in a fully portable, single-source manner. Our preliminary implementation is focused on MAS running in a reduced physics \"zero-beta\" mode. While valuable on its own, our main goal is to pave the way for a full physics, thermodynamic MHD implementation. We describe the OpenACC implementation methodology and challenges. \"Time-to-solution\" performance results of a production-level flux rope eruption simulation on multi-CPU and multi-GPU systems are shown. We find that the GPU-accelerated MAS code has the ability to run \"zero-beta\" simulations on a single multi-GPU server at speeds previously requiring multiple CPU server-nodes of a supercomputer.", "label": 1, "source": "scigen_human", "idx": 22, "lang": "en"}
{"text": "When we try to solve a system of linear equations, we can consider a simple iterative algorithm in which an equation including only one variable is chosen at each step, and the variable is fixed to the value satisfying the equation. The dynamics of this algorithm is captured by the peeling algorithm. Analyses of the peeling algorithm on random hypergraphs are required for many problems, e.g., the decoding threshold of low-density parity check codes, the inverting threshold of Goldreich's pseudorandom generator, the load threshold of cuckoo hashing, etc. In this work, we deal with random hypergraphs including superlinear number of hyperedges, and derive the tight threshold for the succeeding of the peeling algorithm. For the analysis, Wormald's method of differential equations, which is commonly used for analyses of the peeling algorithm on random hypergraph with linear number of hyperedges, cannot be used due to the superlinear number of hyperedges. A new method called the evolution of the moment generating function is proposed in this work.", "label": 1, "source": "scigen_human", "idx": 23, "lang": "en"}
{"text": "We propose a flexible framework for clustering hypergraph-structured data based on recently proposed random walks utilizing edge-dependent vertex weights. When incorporating edge-dependent vertex weights (EDVW), a weight is associated with each vertex-hyperedge pair, yielding a weighted incidence matrix of the hypergraph. Such weightings have been utilized in term-document representations of text data sets. We explain how random walks with EDVW serve to construct different hypergraph Laplacian matrices, and then develop a suite of clustering methods that use these incidence matrices and Laplacians for hypergraph clustering. Using several data sets from real-life applications, we compare the performance of these clustering algorithms experimentally against a variety of existing hypergraph clustering methods. We show that the proposed methods produce higher-quality clusters and conclude by highlighting avenues for future work.", "label": 1, "source": "scigen_human", "idx": 24, "lang": "en"}
{"text": "In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not require major hardware or software investments. We design, implement, and experiment a force-directed algorithm in Giraph, a popular open source framework for distributed computing, based on a vertex-centric design paradigm. The algorithm is tested both on real and artificial graphs with up to million edges, by using a rather inexpensive PaaS (Platform as a Service) infrastructure of Amazon. The experiments show the scalability and effectiveness of our technique when compared to a centralized implementation of the same force-directed model. We show that graphs with about one million edges can be drawn in less than 8 minutes, by spending about 1 per drawing in the cloud computing infrastructure.", "label": 1, "source": "scigen_human", "idx": 26, "lang": "en"}
{"text": "The rapid growth of multimedia consumption has triggered technical, economic, and business innovations that improve the quality and accessibility of content. It has also opened new markets, promising large revenues for industry players. However, new technologies also pose new questions regarding the legal aspects of content delivery, which are often resolved through litigation between copyright owners and content distributors. The precedents set by these cases will act as a game changer in the content delivery industry and will shape the existing offerings in the market in terms of how new technologies can be deployed and what kind of pricing strategies can be associated with them. In this paper, we offer a tutorial on key copyright and communications laws and decisions related to storage and transmission of video content over the Internet. We summarize legal limitations on the deployment of new technologies and pricing mechanisms, and explain the implications of recent lawsuits. Understanding these concerns is essential for engineers engaged in designing the technical and economic aspects of video delivery systems.", "label": 1, "source": "scigen_human", "idx": 27, "lang": "en"}
{"text": "We study the power and limits of optimal dynamic pricing in combinatorial markets; i.e., dynamic pricing that leads to optimal social welfare. Previous work by Cohen-Addad et al. [EC'16] demonstrated the existence of optimal dynamic prices for unit-demand buyers, and showed a market with coverage valuations that admits no such prices. However, finding the frontier of markets (i.e., valuation functions) that admit optimal dynamic prices remains an open problem. In this work we establish positive and negative results that narrow the existing gap. On the positive side, we provide tools for handling markets beyond unit-demand valuations. In particular, we characterize all optimal allocations in multi-demand markets. This characterization allows us to partition the items into equivalence classes according to the role they play in achieving optimality. Using these tools, we provide a poly-time optimal dynamic pricing algorithm for up to 3 multi-demand buyers. On the negative side, we establish a maximal domain theorem, showing that for every non-gross substitutes valuation, there exist unit-demand valuations such that adding them yields a market that does not admit an optimal dynamic pricing. This result is reminiscent of the seminal maximal domain theorem by Gul and Stacchetti [JET'99] for Walrasian equilibrium. Yang [JET'17] discovered an error in their original proof, and established a different, incomparable version of their maximal domain theorem. En route to our maximal domain theorem for optimal dynamic pricing, we provide the first complete proof of the original theorem by Gul and Stacchetti.", "label": 1, "source": "scigen_human", "idx": 28, "lang": "en"}
{"text": "We study robust PCA for the fully observed setting, which is about separating a low rank matrix L and a sparse matrix S from their sum D L S. In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which significantly improves the computational efficiency of the existing alternating projections proposed in when updating the low rank factor. The acceleration is achieved by first projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-the-art algorithms for robust PCA.", "label": 1, "source": "scigen_human", "idx": 29, "lang": "en"}
{"text": "Neural program embedding has shown potential in aiding the analysis of large-scale, complicated software. Newly proposed deep neural architectures pride themselves on learning program semantics rather than superficial syntactic features. However, by considering the source code only, the vast majority of neural networks do not capture a deep, precise representation of program semantics. In this paper, we present DyPro, a novel deep neural network that learns from program execution traces. Compared to the prior dynamic models, not only is DyPro capable of generalizing across multiple executions for learning a program's dynamic semantics in its entirety, but DyPro is also more efficient when dealing with programs yielding long execution traces. For evaluation, we task DyPro with semantics classification (i.e. categorizing programs based on their semantics) and compared it against two prominent static models: Gated Graph Neural Network and TreeLSTM. We find that DyPro achieves the highest prediction accuracy among all models. To further reveal the capacity of all aforementioned deep neural architectures, we examine if the models can learn to detect deeper semantic properties of a program. In particular given a task of recognizing loop invariants, we find that DyPro outperforms all static models by a wide margin.", "label": 1, "source": "scigen_human", "idx": 31, "lang": "en"}
{"text": "As inertial and visual sensors are becoming ubiquitous, visual-inertial navigation systems (VINS) have prevailed in a wide range of applications from mobile augmented reality to aerial navigation to autonomous driving, in part because of the complementary sensing capabilities and the decreasing costs and size of the sensors. In this paper, we survey thoroughly the research efforts taken in this field and strive to provide a concise but complete review of the related work - which is unfortunately missing in the literature while being greatly demanded by researchers and engineers - in the hope to accelerate the VINS research and beyond in our society as a whole.", "label": 1, "source": "scigen_human", "idx": 32, "lang": "en"}
{"text": "Matrix Product States (MPS), also known as Tensor Train (TT) decomposition in mathematics, has been proposed originally for describing an (especially one-dimensional) quantum system, and recently has found applications in various applications such as compressing high-dimensional data, supervised kernel linear classifier, and unsupervised generative modeling. However, when applied to systems which are not defined on one-dimensional lattices, a serious drawback of the MPS is the exponential decay of the correlations, which limits its power in capturing long-range dependences among variables in the system. To alleviate this problem, we propose to introduce long-range interactions, which act as shortcuts, to MPS, resulting in a new model Shortcut Matrix Product States (SMPS). When chosen properly, the shortcuts can decrease significantly the correlation length of the MPS, while preserving the computational efficiency. We develop efficient training methods of SMPS for various tasks, establish some of their mathematical properties, and show how to find a good location to add shortcuts. Finally, using extensive numerical experiments we evaluate its performance in a variety of applications, including function fitting, partition function calculation of 2 - d Ising model, and unsupervised generative modeling of handwritten digits, to illustrate its advantages over vanilla matrix product states.", "label": 1, "source": "scigen_human", "idx": 33, "lang": "en"}
{"text": "Most of the current action recognition algorithms are based on deep networks which stack multiple convolutional, pooling and fully connected layers. While convolutional and fully connected operations have been widely studied in the literature, the design of pooling operations that handle action recognition, with different sources of temporal granularity in action categories, has comparatively received less attention, and existing solutions rely mainly on max or averaging operations. The latter are clearly powerless to fully exhibit the actual temporal granularity of action categories and thereby constitute a bottleneck in classification performances. In this paper, we introduce a novel hierarchical pooling design that captures different levels of temporal granularity in action recognition. Our design principle is coarse-to-fine and achieved using a tree-structured network; as we traverse this network top-down, pooling operations are getting less invariant but timely more resolute and well localized. Learning the combination of operations in this network - which best fits a given ground-truth - is obtained by solving a constrained minimization problem whose solution corresponds to the distribution of weights that capture the contribution of each level (and thereby temporal granularity) in the global hierarchical pooling process. Besides being principled and well grounded, the proposed hierarchical pooling is also video-length and resolution agnostic. Extensive experiments conducted on the challenging UCF-101, HMDB-51 and JHMDB-21 databases corroborate all these statements.", "label": 1, "source": "scigen_human", "idx": 34, "lang": "en"}
{"text": "The problem of mesh matching is addressed in this work. For a given n -sided planar region bounded by one loop of n polylines we are selecting optimal quadrilateral mesh from existing catalogue of meshes. The formulation of matching between planar shape and quadrilateral mesh from the catalogue is based on the problem of finding longest common subsequence (LCS). Theoretical foundation of mesh matching method is provided. Suggested method represents a viable technique for selecting best mesh for planar region and stepping stone for further parametrization of the region.", "label": 1, "source": "scigen_human", "idx": 35, "lang": "en"}
{"text": "Partially answering a question of Paul Seymour, we obtain a sufficient eigenvalue condition for the existence of k edge-disjoint spanning trees in a regular graph, when k {2, 3 }. More precisely, we show that if the second largest eigenvalue of a d -regular graph G is less than - d - 2 k 1 d 1, then G contains at least k edge-disjoint spanning trees, when k {2, 3 }. We construct examples of graphs that show our bounds are essentially best possible. We conjecture that the above statement is true for any k d 2.", "label": 1, "source": "scigen_human", "idx": 36, "lang": "en"}
{"text": "For mobile robots navigating on sidewalks, it is essential to be able to safely cross street intersections. Most existing approaches rely on the recognition of the traffic light signal to make an informed crossing decision. Although these approaches have been crucial enablers for urban navigation, the capabilities of robots employing such approaches are still limited to navigating only on streets that contain signalized intersections. In this paper, we address this challenge and propose a multimodal convolutional neural network framework to predict the safety of a street intersection for crossing. Our architecture consists of two subnetworks; an interaction-aware trajectory estimation stream IA-TCNN, that predicts the future states of all observed traffic participants in the scene, and a traffic light recognition stream AtteNet. Our IA-TCNN utilizes dilated causal convolutions to model the behavior of all the observable dynamic agents in the scene without explicitly assigning priorities to the interactions among them. While AtteNet utilizes Squeeze-Excitation blocks to learn a content-aware mechanism for selecting the relevant features from the data, thereby improving the noise robustness. Learned representations from the traffic light recognition stream are fused with the estimated trajectories from the motion prediction stream to learn the crossing decision. Incorporating the uncertainty information from both modules enables our architecture to learn a likelihood function that is robust to noise and mispredictions from either subnetworks. Simultaneously, by learning to estimate motion trajectories of the surrounding traffic participants and incorporating knowledge of the traffic light signal, our network learns a robust crossing procedure that is invariant to the type of street intersection. Furthermore, we extend our previously introduced Freiburg Street Crossing dataset with sequences captured at multiple intersections of varying types, demonstrating complex interactions among the traffic participants as well as various lighting and weather conditions. We perform comprehensive experimental evaluations on public datasets as well as our Freiburg Street Crossing dataset, which demonstrate that our network achieves state-of-the-art performance for each of the subtasks, as well as for the crossing safety prediction. Moreover, we deploy the proposed architectural framework on a robotic platform and conduct real-world experiments which demonstrate the suitability of the approach for real-time deployment and robustness to various environments.", "label": 1, "source": "scigen_human", "idx": 37, "lang": "en"}
{"text": "We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: we combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.", "label": 1, "source": "scigen_human", "idx": 38, "lang": "en"}
{"text": "We present a novel algorithm for instrumental variable (IV) regression, DualIV, which simplifies traditional two-stage methods via a dual formulation. Inspired by problems in stochastic programming, we show that the two-stage procedure for nonlinear IV regression can be reformulated as a convex-concave saddle-point problem. Our formulation circumvents the first-stage regression which is a potential bottleneck in real-world applications. Based on this new approach, we develop a simple kernel-based algorithm with a closed-form solution. Empirical results show that we are competitive to existing, more complicated algorithms for instrumental variable regression.", "label": 1, "source": "scigen_human", "idx": 39, "lang": "en"}
{"text": "We consider data transmission over a network where each edge is an erasure channel and where the inner nodes transmit a random linear combination of their incoming information. We distinguish two channel models in this setting, the row and the column erasure channel model. For both models we derive the symbol erasure correction capabilities of spread codes and compare them to other known codes suitable for those models. Furthermore, we explain how to decode these codes in the two channel models and compare their decoding complexities. The results show that, depending on the application and the to-be-optimized aspect, any combination of codes and channel models can be the best choice.", "label": 1, "source": "scigen_human", "idx": 40, "lang": "en"}
{"text": "Move blocking (MB) is a widely used strategy to reduce the degrees of freedom of the Optimal Control Problem (OCP) arising in receding horizon control. The size of the OCP is reduced by forcing the input variables to be constant over multiple discretization steps. In this paper, we focus on developing computationally efficient MB schemes for multiple shooting based nonlinear model predictive control (NMPC). The degrees of freedom of the OCP is reduced by introducing MB in the shooting step, resulting in a smaller but sparse OCP. Therefore, the discretization accuracy and level of sparsity is maintained. A condensing algorithm that exploits the sparsity structure of the OCP is proposed, that allows to reduce the computation complexity of condensing from quadratic to linear in the number of discretization nodes. As a result, active-set methods with warm-start strategy can be efficiently employed, thus allowing the use of a longer prediction horizon. A detailed comparison between the proposed scheme and the nonuniform grid NMPC is given. Effectiveness of the algorithm in reducing computational burden while maintaining optimization accuracy and constraints fulfillment is shown by means of simulations with two different problems.", "label": 1, "source": "scigen_human", "idx": 41, "lang": "en"}
{"text": "Legged robots traversing in confined environments could find their only path is blocked by obstacles. In circumstances where the obstacles are movable, a multilegged robot can manipulate the obstacles using its legs to allow it to continue on its path. We present a method for a hexapod robot to autonomously generate manipulation trajectories for detected obstacles. Using a RGB-D sensor as input, the obstacle is extracted from the environment and filtered to provide key contact points for the manipulation algorithm to calculate a trajectory to move the obstacle out of the path. Experiments on a 30 degree of freedom hexapod robot show the effectiveness of the algorithm in manipulating a range of obstacles in a 3D environment using its front legs.", "label": 1, "source": "scigen_human", "idx": 42, "lang": "en"}
{"text": "We give an approximate formula of the distribution of the largest eigenvalue of real Wishart matrices by the expected Euler characteristic method for the general dimension. The formula is expressed in terms of a definite integral with parameters. We derive a differential equation satisfied by the integral for the x 2 2 matrix case and perform a numerical analysis of it.", "label": 1, "source": "scigen_human", "idx": 43, "lang": "en"}
{"text": "We introduce novel dynamic oracles for training two of the most accurate known shift-reduce algorithms for constituent parsing: the top-down and in-order transition-based parsers. In both cases, the dynamic oracles manage to notably increase their accuracy, in comparison to that obtained by performing classic static training. In addition, by improving the performance of the state-of-the-art in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained by a fully-supervised single-model greedy shift-reduce constituent parser on the WSJ benchmark.", "label": 1, "source": "scigen_human", "idx": 44, "lang": "en"}
{"text": "Feature extraction from financial data is one of the most important problems in market prediction domain for which many approaches have been suggested. Among other modern tools, convolutional neural networks (CNN) have recently been applied for automatic feature selection and market prediction. However, in experiments reported so far, less attention has been paid to the correlation among different markets as a possible source of information for extracting features. In this paper, we suggest a CNN-based framework with specially designed CNNs, that can be applied on a collection of data from a variety of sources, including different markets, in order to extract features for predicting the future of those markets. The suggested framework has been applied for predicting the next day's direction of movement for the indices of SP 500, NASDAQ, DJI, NYSE, and RUSSELL markets based on various sets of initial features. The evaluations show a significant improvement in prediction's performance compared to the state of the art baseline algorithms.", "label": 1, "source": "scigen_human", "idx": 45, "lang": "en"}
{"text": "Skills like computational thinking, problem solving, handling complexity, team-work and project management are essential for future careers and needs to be taught to students at the elementary level itself. Computer programming knowledge and skills, experiencing technology and conducting science and engineering experiments are also important for students at elementary level. However, teaching such skills effectively through active learning can be challenging for educators. In this paper, we present our approach and experiences in teaching such skills to several elementary level children using Lego Mindstorms EV3 robotics education kit. We describe our learning environment consisting of lessons, worksheets, hands-on activities and assessment. We taught students how to design, construct and program robots using components such as motors, sensors, wheels, axles, beams, connectors and gears. Students also gained knowledge on basic programming constructs such as control flow, loops, branches and conditions using a visual programming environment. We carefully observed how students performed various tasks and solved problems. We present experimental results which demonstrates that our teaching methodology consisting of both the course content and pedagogy was effective in imparting the desired skills and knowledge to elementary level children. The students also participated in a competitive World Robot Olympiad India event and qualified during the regional round which is an evidence of the effectiveness of the approach.", "label": 1, "source": "scigen_human", "idx": 46, "lang": "en"}
{"text": "This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab , Python , R etc. are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples for 1D (dummy) spatial data.", "label": 1, "source": "scigen_human", "idx": 47, "lang": "en"}
{"text": "Automotive companies increasingly adopt scaled agile methods to allow them to deal with their organisational and product complexity. Suitable methods are needed to ensure safety when developing automotive systems. On a small scale, R-Scrum and SafeScrum are two concrete suggestions for how to develop safety-critical systems using agile methods. However, for large-scale environments, existing frameworks like SAFe or LeSS do not support the development of safety-critical systems out of the box. We, therefore, aim to understand which challenges exist when developing safety-critical systems within large-scale agile industrial settings, in particular in the automotive domain. Based on an analysis of R-Scrum and SafeScrum , we conducted a focus group with three experts from industry to collect challenges in their daily work. We found challenges in the areas of living traceability, continuous compliance, and organisational flexibility. Among others, organisations struggle with defining a suitable traceability strategy, performing incremental safety analysis, and with integrating safety practices into their scaled way of working. Our results indicate a need to provide practical approaches to integrate safety work into large-scale agile development and point towards possible solutions, e.g., modular safety cases.", "label": 1, "source": "scigen_human", "idx": 48, "lang": "en"}
{"text": "The discriminator from generative adversarial nets (GAN) has been used by some researchers as a feature extractor in transfer learning and appeared worked well. However, there are also some studies that believe this is the wrong research direction because intuitively the task of the discriminator focuses on separating the real samples from the generated ones, making features extracted in this way useless for most of the downstream tasks. To avoid this dilemma, we first conducted a thorough theoretical analysis of the relationship between the discriminator task and the characteristics of the features extracted. We found that the connection between the task of the discriminator and the feature is not as strong as was thought, for that the main factor restricting the feature learned by the discriminator is not the task of the discriminator itself, but the need to prevent the entire GAN model from mode collapse during the training. From this perspective and combined with further analyses, we found that to avoid mode collapse in the training process of GAN, the features extracted by the discriminator are not guided to be different for the real samples, but divergence without noise is indeed allowed and occupies a large proportion of the feature space. This makes the features learned more robust and helps answer the question as to why the discriminator can succeed as a feature extractor in related research. Consequently, to expose the essence of the discriminator extractor as different from other extractors, we analyze the counterpart of the discriminator extractor, the classifier extractor that assigns the target samples to different categories. We found the performance of the discriminator extractor may be inferior to the classifier based extractor when the source classification task is similar to the target task, which is the common case, but the ability to avoid noise prevents the discriminator from being replaced by the classifier. Last but not least, as our research also revealed a ratio playing an important role in GAN's training to prevent mode collapse, it contributes to the basic GAN study.", "label": 1, "source": "scigen_human", "idx": 49, "lang": "en"}
{"text": "Modern pattern recognition methods are based on convolutional networks since they are able to learn complex patterns that benefit the classification. However, convolutional networks are computationally expensive and require a considerable amount of memory, which limits their deployment on low-power and resource-constrained systems. To handle these problems, recent approaches have proposed pruning strategies that find and remove unimportant neurons (i.e., filters) in these networks. Despite achieving remarkable results, existing pruning approaches are ineffective since the accuracy of the original network is degraded. In this work, we propose a novel approach to efficiently remove filters from convolutional networks. Our approach estimates the filter importance based on its relationship with the class label on a low-dimensional space. This relationship is computed using Partial Least Squares (PLS) and Variable Importance in Projection (VIP). Our method is able to reduce up to 67 of the floating point operations (FLOPs) without penalizing the network accuracy. With a negligible drop in accuracy, we can reduce up to 90 of FLOPs. Additionally, sometimes the method is even able to improve the accuracy compared to original, unpruned, network. We show that employing PLSVIP as the criterion for detecting the filters to be removed is better than recent feature selection techniques, which have been employed by state-of-the-art pruning methods. Finally, we show that the proposed method achieves the highest FLOPs reduction and the smallest drop in accuracy when compared to state-of-the-art pruning approaches. Codes are available at:", "label": 1, "source": "scigen_human", "idx": 50, "lang": "en"}
{"text": "We consider an extension of the massive unsourced random access originally proposed in to the case where the receiver has a very large number of antennas (a massive MIMO base station) and no channel state information is given to the receiver (fully non-coherent detection). Our coding approach borrows the concatenated coding idea from, combined with a novel non-Bayesian \"activity detection\" algorithm for massive MIMO random access channels, that outperforms currently proposed Bayesian vector AMP (VAMP) schemes currently proposed for activity detection, and does not suffer from the numerical instabilities and requirement for accurate a priori statistics as VAMP. We show that the required transmit E b N 0 for reliable communication can be made arbitrarily small as the number of receiver antennas M grows sufficiently large.", "label": 1, "source": "scigen_human", "idx": 51, "lang": "en"}
{"text": "We consider the problem of fitting variational posterior approximations using stochastic optimization methods. The performance of these approximations depends on (1) how well the variational family matches the true posterior distribution, (2) the choice of divergence, and (3) the optimization of the variational objective. We show that even in the best-case scenario when the exact posterior belongs to the assumed variational family, common stochastic optimization methods lead to poor variational approximations if the problem dimension is moderately large. We also demonstrate that these methods are not robust across diverse model types. Motivated by these findings, we develop a more robust and accurate stochastic optimization framework by viewing the underlying optimization algorithm as producing a Markov chain. Our approach is theoretically motivated and includes a diagnostic for convergence and a novel stopping rule, both of which are robust to noisy evaluations of the objective function. We show empirically that the proposed framework works well on a diverse set of models: it can automatically detect stochastic optimization failure or inaccurate variational approximation.", "label": 1, "source": "scigen_human", "idx": 52, "lang": "en"}
{"text": "Bitcoin introduced delegation of control over a monetary system from a select few to all who participate in that system. This delegation is known as the decentralization of controlling power and is a powerful security mechanism for the ecosystem. After the introduction of Bitcoin, the field of cryptocurrency has seen widespread attention from industry and academia, so much so that the original novel contribution of Bitcoin, i.e., decentralization, may be overlooked, due to decentralizations' assumed fundamental existence for the functioning of such crypto-assets. However, recent studies have observed a trend of increased centralization in cryptocurrencies such as Bitcoin and Ethereum. As this increased centralization has an impact the security of the blockchain, it is crucial that it is measured, towards adequate control. This research derives an initial taxonomy of centralization present in decentralized blockchains through rigorous synthesis using a systematic literature review. This is followed by iterative refinement through expert interviews. We systematically analyzed 89 research papers published between 2009 and 2019. Our study contributes to the existing body of knowledge by highlighting the multiple definitions and measurements of centralization in the literature. We identify different aspects of centralization and propose an encompassing taxonomy of centralization concerns. This taxonomy is based on empirically observable and measurable characteristics. It consists of 13 aspects of centralization, classified over six architectural layers: Governance, Network, Consensus, Incentive, Operational, and Application. We also discuss how the implications of centralization can vary depending on the aspects studied. We believe that this review and taxonomy provides a comprehensive overview of centralization in decentralized blockchains involving various conceptualizations and measures.", "label": 1, "source": "scigen_human", "idx": 53, "lang": "en"}
{"text": "The main limitation of visible light communication (VLC) is the narrow modulation bandwidth, which reduces the achievable data rates. In this paper, we apply the non-orthogonal multiple access (NOMA) scheme to enhance the achievable throughput in high-rate VLC downlink networks. We first propose a novel gain ratio power allocation (GRPA) strategy that takes into account the users' channel conditions to ensure efficient and fair power allocation. Our results indicate that GRPA significantly enhances system performance compared to the static power allocation. We also study the effect of tuning the transmission angles of the light emitting diodes (LEDs) and the field of views (FOVs) of the receivers, and demonstrate that these parameters can offer new degrees of freedom to boost NOMA performance. Simulation results reveal that NOMA is a promising multiple access scheme for the downlink of VLC networks.", "label": 1, "source": "scigen_human", "idx": 54, "lang": "en"}
{"text": "This paper develops a mechanical tool as well as its manipulation policies for 2-finger parallel robotic grippers. It primarily focuses on a mechanism that converts the gripping motion of 2-finger parallel grippers into a continuous rotation to realize tasks like fastening screws. The essential structure of the tool comprises a Scissor-Like Element (SLE) mechanism and a double-ratchet mechanism. They together convert repeated linear motion into continuous rotating motion. At the joints of the SLE mechanism, elastic elements are attached to provide resisting force for holding the tool as well as for producing torque output when a gripper releases the tool. The tool is entirely mechanical, allowing robots to use the tool without any peripherals and power supply. The paper presents the details of the tool design, optimizes its dimensions and effective stroke lengths, and studies the contacts and forces to achieve stable grasping and screwing. Besides the design, the paper develops manipulation policies for the tool. The policies include visual recognition, picking-up and manipulation, and exchanging tooltips. The developed tool produces clockwise rotation at the front end and counter-clockwise rotation at the back end. Various tooltips can be installed at both two ends. Robots may employ the developed manipulation policies to exchange the tooltips and rotating directions following the needs of specific fastening or loosening tasks. Robots can also reorient the tool using pick-and-place or handover, and move the tool to work poses using the policies. The designed tool, together with the developed manipulation policies, are analyzed and verified in several real-world applications. The tool is small, cordless, convenient, and has good robustness and adaptability.", "label": 1, "source": "scigen_human", "idx": 55, "lang": "en"}
{"text": "As of September 2020, the COVID-19 pandemic continues to devastate the health and well-being of the global population. With more than 33 million confirmed cases and over a million deaths, global health organizations are still a long way from fully containing the pandemic. This pandemic has raised serious questions about the emergency preparedness of health agencies, not only in terms of treatment of an unseen disease, but also in identifying its early symptoms. In the particular case of COVID-19, several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such as COVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of infected peoples' data could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate this problem within a one-class classification framework, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present COVIDomaly, a convolutional autoencoder framework to detect unseen COVID-19 cases from the chest radiographs. We tested two settings on a publicly available dataset (COVIDx) by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. After performing 3-fold cross validation, we obtain a pooled ROC-AUC of 0.7652 and 0.6902 in the two settings respectively. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays.", "label": 1, "source": "scigen_human", "idx": 58, "lang": "en"}
{"text": "While first-order optimization methods such as stochastic gradient descent (SGD) are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, in contrast to SGD with momentum, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.", "label": 1, "source": "scigen_human", "idx": 59, "lang": "en"}
{"text": "This paper proposes a parallel optimization algorithm for cooperative automation of large-scale connected vehicles. The task of cooperative automation is formulated as a centralized optimization problem taking the whole decision space of all vehicles into account. Considering the uncertainty of the environment, the problem is solved in a receding horizon fashion. Then, we employ the alternating direction method of multipliers (ADMM) to solve the centralized optimization in a parallel way, which scales more favorably to large-scale instances. Also, Taylor series is used to linearize nonconvex constraints caused by coupling collision avoidance constraints among interactive vehicles. Simulations with two typical traffic scenes for multiple vehicles demonstrate the effectiveness and efficiency of our method.", "label": 1, "source": "scigen_human", "idx": 60, "lang": "en"}
{"text": "This essay argues that a new form of democracy - an \"Emergent Democracy\" - will develop as a result of the use of Internet communication tools and platforms such as blogs. The essay explores a variety of tools available and explores the history of democracy, modern experiments with democracy and how these tools might support democracy. The essay also explores concerns as these new tools emerge. These issues include concerns such as privacy and the societally negative use of these tools by corporations, totalitarian regimes and terrorists.", "label": 1, "source": "scigen_human", "idx": 62, "lang": "en"}
{"text": "Segregating an audio mixture containing multiple simultaneous bird sounds is a challenging task. However, birdsong often contains rapid pitch modulations, and these modulations carry information which may be of use in automatic recognition. In this paper we demonstrate that an improved spectrogram representation, based on the distribution derivative method, leads to improved performance of a segregation algorithm which uses a Markov renewal process model to track vocalisation patterns consisting of singing and silences.", "label": 1, "source": "scigen_human", "idx": 63, "lang": "en"}
{"text": "Music recommender systems (MRS) have experienced a boom in recent years, thanks to the emergence and success of online streaming services, which nowadays make available almost all music in the world at the user's fingertip. While today's MRS considerably help users to find interesting music in these huge catalogs, MRS research is still facing substantial challenges. In particular when it comes to build, incorporate, and evaluate recommendation strategies that integrate information beyond simple user-item interactions or content-based descriptors, but dig deep into the very essence of listener needs, preferences, and intentions, MRS research becomes a big endeavor and related publications quite sparse. The purpose of this trends and survey article is twofold. We first identify and shed light on what we believe are the most pressing challenges MRS research is facing, from both academic and industry perspectives. We review the state of the art towards solving these challenges and discuss its limitations. Second, we detail possible future directions and visions we contemplate for the further evolution of the field. The article should therefore serve two purposes: giving the interested reader an overview of current challenges in MRS research and providing guidance for young researchers by identifying interesting, yet under-researched, directions in the field.", "label": 1, "source": "scigen_human", "idx": 64, "lang": "en"}
{"text": "Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to \"drinking from the firehose,\" as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains. In this work we present CacheOut, a new microarchitectural attack that is capable of bypassing Intel's buffer overwrite countermeasures. We observe that as data is being evicted from the CPU's L1 cache, it is often transferred back to the leaky CPU buffers where it can be recovered by the attacker. CacheOut improves over previous MDS attacks by allowing the attacker to choose which data to leak from the CPU's L1 cache, as well as which part of a cache line to leak. We demonstrate that CacheOut can leak information across multiple security boundaries, including those between processes, virtual machines, user and kernel space, and from SGX enclaves.", "label": 1, "source": "scigen_human", "idx": 65, "lang": "en"}
{"text": "This work introduces Conditional Image Retrieval (CIR) systems: IR methods that can efficiently specialize to specific subsets of images on the fly. These systems broaden the class of queries IR systems support, and eliminate the need for expensive re-fitting to specific subsets of data. Specifically, we adapt tree-based K-Nearest Neighbor (KNN) data-structures to the conditional setting by introducing additional inverted-index data-structures. This speeds conditional queries and does not slow queries without conditioning. We present two new datasets for evaluating the performance of CIR systems and evaluate a variety of design choices. As a motivating application, we present an algorithm that can explore shared semantic content between works of art of vastly different media and cultural origin. Finally, we demonstrate that CIR data-structures can identify Generative Adversarial Network (GAN) \"blind spots\": areas where GANs fail to properly model the true data distribution.", "label": 1, "source": "scigen_human", "idx": 66, "lang": "en"}
{"text": "We introduce a general and simple structural design called \"Multiplicative Integration\" (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.", "label": 1, "source": "scigen_human", "idx": 67, "lang": "en"}
{"text": "With the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the COVID-19 pandemic, there is a growing interest in finding solutions to help address the problem. A solution to this problem would be to use neurotechnology to provide them augmented cognition, senses and action for optimal diagnosis and treatment. Consequently, doing so can negatively impact them and others. We argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices, and harm to them and patients. In this paper, we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons. We will then review selected ethical concerns discussed within literature, discuss the neuroengineering behind using neurotechnology for augmentation purposes, then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice.", "label": 1, "source": "scigen_human", "idx": 68, "lang": "en"}
{"text": "In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is 2.8 x larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset.", "label": 1, "source": "scigen_human", "idx": 69, "lang": "en"}
{"text": "We introduce the new task of Acoustic Question Answering (AQA) to promote research in acoustic reasoning. The AQA task consists of analyzing an acoustic scene composed by a combination of elementary sounds and answering questions that relate the position and properties of these sounds. The kind of relational questions asked, require that the models perform non-trivial reasoning in order to answer correctly. Although similar problems have been extensively studied in the domain of visual reasoning, we are not aware of any previous studies addressing the problem in the acoustic domain. We propose a method for generating the acoustic scenes from elementary sounds and a number of relevant questions for each scene using templates. We also present preliminary results obtained with two models (FiLM and MAC) that have been shown to work for visual reasoning.", "label": 1, "source": "scigen_human", "idx": 70, "lang": "en"}
{"text": "A posteriori error estimates are constructed for the three-field variational formulation of the Biot problem involving the displacements, the total pressure and the fluid pressure. The discretization under focus is the H 1 (O) -conforming Taylor-Hood finite element combination, consisting of polynomial degrees k 1 for the displacements and the fluid pressure and k for the total pressure. An a posteriori error estimator is derived on the basis of H (div) -conforming reconstructions of the stress and flux approximations. The symmetry of the reconstructed stress is allowed to be satisfied only weakly. The reconstructions can be performed locally on a set of vertex patches and lead to a guaranteed upper bound for the error with a constant that depends only on local constants associated with the patches and thus on the shape regularity of the triangulation. Particular emphasis is given to nearly incompressible materials and the error estimates hold uniformly in the incompressible limit. Numerical results on the L-shaped domain confirm the theory and the suitable use of the error estimator in adaptive strategies.", "label": 1, "source": "scigen_human", "idx": 71, "lang": "en"}
{"text": "We propose to classify the power of algorithms by the complexity of the problems that they can be used to solve. Instead of restricting to the problem a particular algorithm was designed to solve explicitly, however, we include problems that, with polynomial overhead, can be solved ' implicitly'during the algorithm's execution. For example, we allow to solve a decision problem by suitably transforming the input, executing the algorithm, and observing whether a specific bit in its internal configuration ever switches during the execution. We show that the Simplex Method, the Network Simplex Method (both with Dantzig's original pivot rule), and the Successive Shortest Path Algorithm are NP-mighty, that is, each of these algorithms can be used to solve any problem in NP. This result casts a more favorable light on these algorithms' exponential worst-case running times. Furthermore, as a consequence of our approach, we obtain several novel hardness results. For example, for a given input to the Simplex Algorithm, deciding whether a given variable ever enters the basis during the algorithm's execution and determining the number of iterations needed are both NP-hard problems. Finally, we close a long-standing open problem in the area of network flows over time by showing that earliest arrival flows are NP-hard to obtain.", "label": 1, "source": "scigen_human", "idx": 72, "lang": "en"}
{"text": "Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website:", "label": 1, "source": "scigen_human", "idx": 73, "lang": "en"}
{"text": "Deep neuroevolution, that is evolutionary policy search methods based on deep neural networks, have recently emerged as a competitor to deep reinforcement learning algorithms due to their better parallelization capabilities. However, these methods still suffer from a far worse sample efficiency. In this paper we investigate whether a mechanism known as \"importance mixing\" can significantly improve their sample efficiency. We provide a didactic presentation of importance mixing and we explain how it can be extended to reuse more samples. Then, from an empirical comparison based on a simple benchmark, we show that, though it actually provides better sample efficiency, it is still far from the sample efficiency of deep reinforcement learning, though it is more stable.", "label": 1, "source": "scigen_human", "idx": 74, "lang": "en"}
{"text": "In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to maintain social connections. In this online digital world, people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations. Thus, social media has established itself to bear signals relating to human behavior. One can easily design user characteristic network by scraping through someone's social media profiles. In this paper, we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social, psychological and linguistic behavior as evident from the content generated by them. Our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters. We use Twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78 with very high precision and recall. Note that such an automatic classification can have far-reaching impact - (i) on health research related to addiction prevention and control, and (ii) in eliminating abusive and vulgar contents from Twitter, borne by the tweets of drunk texters.", "label": 1, "source": "scigen_human", "idx": 75, "lang": "en"}
{"text": "Wireless Sensor Networks (WSNs) with their dynamic applications gained a tremendous attention of researchers. Constant monitoring of critical situations attracted researchers to utilize WSNs at vast platforms. The main focus in WSNs is to enhance network life-time as much as one could, for efficient and optimal utilization of resources. Different approaches based upon clustering are proposed for optimum functionality. Network life-time is always related with energy of sensor nodes deployed at remote areas for constant and fault tolerant monitoring. In this work, we propose Quadrature-LEACH (Q-LEACH) for homogenous networks which enhances stability period, network life-time and throughput quiet significantly.", "label": 1, "source": "scigen_human", "idx": 76, "lang": "en"}
{"text": "This work investigates the consensus problem for multi-agent nonlinear systems through the distributed real-time nonlinear receding horizon control methodology. With this work, we develop a scheme to reach the consensus for nonlinear multi agent systems under fixed directedundirected graph (s) without the need of any linearization techniques. For this purpose, the problem of consensus is converted into an optimization problem and is directly solved by the backwards sweep Riccati method to generate the control protocol which results in a non-iterative algorithm. Stability analysis is conducted to provide convergence guarantees of proposed scheme. In addition, an extension to the leader-following consensus of nonlinear multi-agent systems is presented. Several examples are provided to validate and demonstrate the effectiveness of the presented scheme and the corresponding theoretical results.", "label": 1, "source": "scigen_human", "idx": 77, "lang": "en"}
{"text": "Variational Auto-Encoders have often been used for unsupervised pretraining, feature extraction and out-of-distribution and anomaly detection in the medical field. However, VAEs often lack the ability to produce sharp images and learn high-level features. We propose to alleviate these issues by adding a new branch to conditional hierarchical VAEs. This enforces a division between higher-level and lower-level features. Despite the additional computational overhead compared to a normal VAE it results in sharper and better reconstructions and can capture the data distribution similarly well (indicated by a similar or slightly better OoD detection performance).", "label": 1, "source": "scigen_human", "idx": 78, "lang": "en"}
{"text": "New cryptographic techniques such as homomorphic encryption (HE) allow computations to be outsourced to and evaluated blindfolded in a resourceful cloud. These computations often require private data owned by multiple participants, engaging in joint evaluation of some functions. For example, Genome-Wide Association Study (GWAS) is becoming feasible because of recent proliferation of genome sequencing technology. Due to the sensitivity of genomic data, these data should be encrypted using different keys. However, supporting computation on ciphertexts encrypted under multiple keys is a non-trivial task. In this paper, we present a comprehensive survey on different state-of-the-art cryptographic techniques and schemes that are commonly used. We review techniques and schemes including Attribute-Based Encryption (ABE), Proxy Re-Encryption (PRE), Threshold Homomorphic Encryption (ThHE), and Multi-Key Homomorphic Encryption (MKHE). We analyze them based on different system and security models, and examine their complexities. We share lessons learned and draw observations for designing better schemes with reduced overheads.", "label": 1, "source": "scigen_human", "idx": 79, "lang": "en"}
{"text": "Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.", "label": 1, "source": "scigen_human", "idx": 80, "lang": "en"}
{"text": "Linear logic and the linear l -calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on AEthel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear l -calculus with an accuracy of as high as 70.", "label": 1, "source": "scigen_human", "idx": 81, "lang": "en"}
{"text": "To support a freight carrier in a combinatorial transport auction, we proposes an exact and two heuristic strategies for bidding on subsets of requests. The exact bidding strategy is based on the concept of elementary request combinations. We show that it is sufficient and necessary for a carrier to bid on each elementary request combination in order to guarantee the same result as bidding on each element of the powerset of the set of tendered requests. Both heuristic bidding strategies identify promising request combinations. For this, pairwise synergies based on saving values as well as the capacitated p-median problem are used. The bidding strategies are evaluated by a computational study that simulates an auction. It is based on 174 benchmark instances and therefore easily extendable by other researchers. On average, the two heuristic strategies achieve 91 percent and 81 percent of the available sales potential while generating 36 and only 4 percent of the bundle bids of the exact strategy. Therefore, the proposed bidding strategies help a carrier to increase her chance to win and at the same time reduce the computational burden to participate in a combinatorial transport auction.", "label": 1, "source": "scigen_human", "idx": 82, "lang": "en"}
{"text": "An innovative 3-D radar imaging technique is developed for fast and efficient identification and characterization of radar backscattering components of complex objects, when the collected scattered field is made of polarization-diverse measurements. In this context, all the polarimetric information seems irretrievably mixed. A direct model, derived from a simple but original extension of the widespread \"multiple scattering model\" leads to a high dimensional linear inverse problem. It is solved by a fast dedicated imaging algorithm that performs to determine at a time three huge 3-D scatterer maps which correspond to HH, VV and HV polarizations at emission and reception. It is applied successfully to various mock-ups and data sets collected from an accurate and dedicated 3D spherical experimental layout that provides concentric polarization-diverse RCS measurements.", "label": 1, "source": "scigen_human", "idx": 83, "lang": "en"}
{"text": "We consider the age-old problem of allocating items among different agents in a way that is efficient and fair. Two papers, by Dolev et al. and Ghodsi et al., have recently studied this problem in the context of computer systems. Both papers had similar models for agent preferences, but advocated different notions of fairness. We formalize both fairness notions in economic terms, extending them to apply to a larger family of utilities. Noting that in settings with such utilities efficiency is easily achieved in multiple ways, we study notions of fairness as criteria for choosing between different efficient allocations. Our technical results are algorithms for finding fair allocations corresponding to two fairness notions: Regarding the notion suggested by Ghodsi et al., we present a polynomial-time algorithm that computes an allocation for a general class of fairness notions, in which their notion is included. For the other, suggested by Dolev et al., we show that a competitive market equilibrium achieves the desired notion of fairness, thereby obtaining a polynomial-time algorithm that computes such a fair allocation and solving the main open problem raised by Dolev et al.", "label": 1, "source": "scigen_human", "idx": 84, "lang": "en"}
{"text": "Retrieving videos of a particular person with face image as query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing face videos with some robust set modeling techniques (e.g. covariance matrices as exploited in this study, which reside on Riemannian manifold), has recently shown appealing advantages. This hence results in a thorny heterogeneous spaces matching problem. Moreover, hashing with handcrafted features as done in many existing works is clearly inadequate to achieve desirable performance for this task. To address such problems, we present an end-to-end Deep Heterogeneous Hashing (DHH) method that integrates three stages including image feature learning, video modeling, and heterogeneous hashing in a single framework, to learn unified binary codes for both face images and videos. To tackle the key challenge of hashing on manifold, a well-studied Riemannian kernel mapping is employed to project data (i.e. covariance matrices) into Euclidean space and thus enables to embed the two heterogeneous representations into a common Hamming space, where both intra-space discriminability and inter-space compatibility are considered. To perform network optimization, the gradient of the kernel mapping is innovatively derived via structured matrix backpropagation in a theoretically principled way. Experiments on three challenging datasets show that our method achieves quite competitive performance compared with existing hashing methods.", "label": 1, "source": "scigen_human", "idx": 85, "lang": "en"}
{"text": "This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation - implicit concurrency 1 footnote 1 1 footnote 1 for short - a broad and versatile computational learning efficiency thought to underlie general-purpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover (UGAs). We demonstrate that implicit concurrency is indeed a form of efficient learning by showing that it can be used to obtain close-to-optimal bounds on the time and queries required to approximately correctly solve a constrained version (k 7, e 1 5) of a recognizable computational learning problem: learning parities with noisy membership queries. We argue that a UGA that treats the noisy membership query oracle as a fitness function can be straightforwardly used to approximately correctly learn the essential attributes in O (log 1.585 n) queries and O (n log 1.585 n) time, where n is the total number of attributes. Our proof relies on an accessible symmetry argument and the use of statistical hypothesis testing to reject a global null hypothesis at the 10 - 100 level of significance. It is, to the best of our knowledge, the first relatively rigorous identification of efficient computational learning in an evolutionary algorithm on a non-trivial learning problem.", "label": 1, "source": "scigen_human", "idx": 86, "lang": "en"}
{"text": "The digital identity problem is a complex one in large part because it involves personal data, the algorithms which compute reputations on the data and the management of the identifiers that are linked to personal data. The reality of today is that personal data of an individual is distributed throughout the Internet, in both private and public institutions, and increasingly also on the user's devices. In order to empower individuals to have a say in who has access to their personal data and to enable individuals to make use of their data for their own purposes, a coherent and scalable access authorization architecture is required. Such an architecture must allow different data holders, data providers and user-content generators to respond to an individual's wishes with regards to consent in a federated fashion. This federation must allow an individual to easily manage access policies and provide consent as required by current and forthcoming data privacy regulations. This paper describes the User Managed Access (UMA) architecture and protocols that provide the foundation for scalable access authorization.", "label": 1, "source": "scigen_human", "idx": 87, "lang": "en"}
{"text": "The majority of works in distributed storage networks assume a simple network model with a collection of identical storage nodes with the same communication cost between the nodes. In this paper, we consider a realistic multi-rack distributed data storage network and present a code design framework for this model. Considering the cheaper data transmission within the racks, our code construction method is able to locally repair the nodes failure within the same rack by using only the survived nodes in the same rack. However, in the case of severe failure patterns when the information content of the survived nodes is not sufficient to repair the failures, other racks will participate in the repair process. By employing the criteria of our multi-rack storage code, we establish a linear programming bound on the size of the code in order to maximize the code rate.", "label": 1, "source": "scigen_human", "idx": 88, "lang": "en"}
{"text": "In this article, we investigate the transient behavior of a sequence of packetsbits traversing a multi-hop wireless network. Our work is motivated by novel applications from the domain of process automation, Machine-Type Communication (MTC) and cyber-physical systems, where short messages are communicated and statistical guarantees need to be provided on a per-message level. In order to optimize such a network, apart from understanding the stationary system dynamics, an understanding of the short-term dynamics (i.e., transient behavior) is also required. To this end, we derive novel Wireless Transient Bounds (WTB) for end-to-end delay and backlog in a multi-hop wireless network using stochastic network calculus approach. WTB depends on the initial backlog at each node as well as the instantaneous channel states. We numerically compare WTB with State-Of-The-Art Transient bounds (SOTAT), that can be obtained by adapting existing stationary bounds, as well as simulation of the network. While SOTAT and stationary bounds are not able to capture the short-term system dynamics well, WTB provides relatively tight upper bound and has a decay rate that closely matches the simulation. This is achieved by WTB only with a slight increase in the computational complexity, by a factor of O (T N), where T is the duration of the arriving sequence and N is the number of hops in the network. We believe that the presented analysis and the bounds can be used as base for future work on transient network optimization, e.g., in massive MTC, critical MTC, edge computing and autonomous vehicle.", "label": 1, "source": "scigen_human", "idx": 89, "lang": "en"}
{"text": "Recently, a tabletop molecular communication platform has been developed for transmitting short text messages across a room. The end-to-end system impulse response for this platform does not follow previously published theoretical works because of imperfect receiver, transmitter, and turbulent flows. Moreover, it is observed that this platform resembles a nonlinear system, which makes the rich body of theoretical work that has been developed by communication engineers not applicable to this platform. In this work, we first introduce corrections to the previous theoretical models of the end-to-end system impulse response based on the observed data from experimentation. Using the corrected impulse response models, we then formulate the nonlinearity of the system as noise and show that through simplifying assumptions it can be represented as Gaussian noise. Through formulating the system's nonlinearity as the output a linear system corrupted by noise, the rich toolbox of mathematical models of communication systems, most of which are based on linearity assumption, can be applied to this platform.", "label": 1, "source": "scigen_human", "idx": 90, "lang": "en"}
{"text": "Neural Architecture Search (NAS) has shown great potentials in finding a better neural network design than human design. Sample-based NAS is the most fundamental method aiming at exploring the search space and evaluating the most promising architecture. However, few works have focused on improving the sampling efficiency for a multi-objective NAS. Inspired by the nature of the graph structure of a neural network, we propose BOGCN-NAS, a NAS algorithm using Bayesian Optimization with Graph Convolutional Network (GCN) predictor. Specifically, we apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. For NAS-oriented tasks, we also design a weighted loss focusing on architectures with high performance. Our method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speedaccuracy trade-off. Extensive experiments are conducted to verify the effectiveness of our method over many competing methods, e.g. 128.4 x more efficient than Random Search and 7.8 x more efficient than previous SOTA LaNAS for finding the best architecture on the largest NAS dataset NASBench-101.", "label": 1, "source": "scigen_human", "idx": 91, "lang": "en"}
{"text": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models footnote footnote Interactive demos, video, code, and data are available at GitHub and gandissect.csail.mit.edu..", "label": 1, "source": "scigen_human", "idx": 92, "lang": "en"}
{"text": "Deep learning models, such as the fully convolutional network (FCN), have been widely used in 3D biomedical segmentation and achieved state-of-the-art performance. Multiple modalities are often used for disease diagnosis and quantification. Two approaches are widely used in the literature to fuse multiple modalities in the segmentation networks: early-fusion (which stacks multiple modalities as different input channels) and late-fusion (which fuses the segmentation results from different modalities at the very end). These fusion methods easily suffer from the cross-modal interference caused by the input modalities which have wide variations. To address the problem, we propose a novel deep learning architecture, namely OctopusNet, to better leverage and fuse the information contained in multi-modalities. The proposed framework employs a separate encoder for each modality for feature extraction and exploits a hyper-fusion decoder to fuse the extracted features while avoiding feature explosion. We evaluate the proposed OctopusNet on two publicly available datasets, i.e. ISLES-2018 and MRBrainS-2013. The experimental results show that our framework outperforms the commonly-used feature fusion approaches and yields the state-of-the-art segmentation accuracy.", "label": 1, "source": "scigen_human", "idx": 93, "lang": "en"}
{"text": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library 1 footnote 1 1 footnote 1 The library is called TensorFlow Fold and lives at . of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "label": 1, "source": "scigen_human", "idx": 94, "lang": "en"}
{"text": "Cascaded regression method is a fast and accurate method on finding 2D pose of objects in RGB images. It is able to find the accurate pose of objects in an image by a great number of corrections on the good initial guess of the pose of objects. This paper explains the algorithm and shows the result of two experiments carried by the researchers. The presented new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing. Finally, we generate confidence-scored 3D proposals of several body parts by re-projecting the classification result and finding local modes.", "label": 1, "source": "scigen_human", "idx": 95, "lang": "en"}
{"text": "In NLP, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because the attention in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into convolution). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text t x . In this work, we propose an attentive convolution network, AttConv . It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text t x that are distant or (ii) from extra (i.e., external) contexts t y . Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of AttConv in sentence representation learning with the incorporation of context. In particular, attentive convolution outperforms attentive pooling and is a strong competitor to popular attentive RNNs. 1 footnote 1 1 footnote 1", "label": 1, "source": "scigen_human", "idx": 96, "lang": "en"}
{"text": "In this paper, we study the problem of approximating the minimum cut in a distributed message-passing model, the CONGEST model. The minimum cut problem has been well-studied in the context of centralized algorithms. However, there were no known non-trivial algorithms in the distributed model until the recent work of Ghaffari and Kuhn. They gave algorithms for finding cuts of size O ( - 1 ) and ( 2 ) in O (D) O (n 1 2 ) rounds and O ( D n) rounds respectively, where is the size of the minimum cut. This matches the lower bound they provided up to a polylogarithmic factor. Yet, no scheme that achieves ( 1 ) -approximation ratio is known. We give a distributed algorithm that finds a cut of size ( 1 ) in O ( D n) time, which is optimal up to polylogarithmic factors.", "label": 1, "source": "scigen_human", "idx": 97, "lang": "en"}
{"text": "Convolutional neural networks (CNN) have had unprecedented success in medical imaging and, in particular, in medical image segmentation. However, despite the fact that segmentation results are closer than ever to the inter-expert variability, CNNs are not immune to producing anatomically inaccurate segmentations, even when built upon a shape prior. In this paper, we present a framework for producing cardiac image segmentation maps that are guaranteed to respect pre-defined anatomical criteria, while remaining within the inter-expert variability. The idea behind our method is to use a well-trained CNN, have it process cardiac images, identify the anatomically implausible results and warp these results toward the closest anatomically valid cardiac shape. This warping procedure is carried out with a constrained variational autoencoder (cVAE) trained to learn a representation of valid cardiac shapes through a smooth, yet constrained, latent space. With this cVAE, we can project any implausible shape into the cardiac latent space and steer it toward the closest correct shape. We tested our framework on short-axis MRI as well as apical two and four-chamber view ultrasound images, two modalities for which cardiac shapes are drastically different. With our method, CNNs can now produce results that are both within the inter-expert variability and always anatomically plausible without having to rely on a shape prior.", "label": 1, "source": "scigen_human", "idx": 98, "lang": "en"}
{"text": "We consider the estimation of a n -dimensional vector x from the knowledge of noisy and possibility non-linear element-wise measurements of x x T , a very generic problem that contains, e.g. stochastic 2 -block model, submatrix localization or the spike perturbation of random matrices. We use an interpolation method proposed by Guerra and later refined by Korada and Macris . We prove that the Bethe mutual information (related to the Bethe free energy and conjectured to be exact by Lesieur et al. on the basis of the non-rigorous cavity method) always yields an upper bound to the exact mutual information. We also provide a lower bound using a similar technique. For concreteness, we illustrate our findings on the sparse PCA problem, and observe that (a) our bounds match for a large region of parameters and (b) that it exists a phase transition in a region where the spectum remains uninformative. While we present only the case of rank-one symmetric matrix estimation, our proof technique is readily extendable to low-rank symmetric matrix or low-rank symmetric tensor estimation.", "label": 1, "source": "scigen_human", "idx": 99, "lang": "en"}
{"text": "We study Doob's martingale convergence theorem for computable continuous time martingales on Brownian motion, in the context of algorithmic randomness. A characterization of the class of sample points for which the theorem holds is given. Such points are given the name of Doob random points. It is shown that a point is Doob random if its tail is computably random in a certain sense. Moreover, Doob randomness is strictly weaker than computable randomness and is incomparable with Schnorr randomness.", "label": 1, "source": "scigen_human", "idx": 100, "lang": "en"}
{"text": "Inspired by recent advances in neural machine translation, that jointly align and translate using encoder-decoder networks equipped with attention, we propose an attention-based LSTM model for human activity recognition. Our model jointly learns to classify actions and highlight frames associated with the action, by attending to salient visual information through a jointly learned soft-attention networks. We explore attention informed by various forms of visual semantic features, including those encoding actions, objects and scenes. We qualitatively show that soft-attention can learn to effectively attend to important objects and scene information correlated with specific human actions. Further, we show that, quantitatively, our attention-based LSTM outperforms the vanilla LSTM and CNN models used by state-of-the-art methods. On a large-scale youtube video dataset, ActivityNet , our model outperforms competing methods in action classification.", "label": 1, "source": "scigen_human", "idx": 102, "lang": "en"}
{"text": "Consider the scattering of a time-harmonic elastic plane wave by a bi-periodic rigid surface. The displacement of elastic wave motion is modeled by the three-dimensional Navier equation in an open domain above the surface. Based on the Dirichlet-to-Neumann (DtN) operator, which is given as an infinite series, an exact transparent boundary condition is introduced and the scattering problem is formulated equivalently into a boundary value problem in a bounded domain. An a posteriori error estimate based adaptive finite element DtN method is proposed to solve the discrete variational problem where the DtN operator is truncated into a finite number of terms. The a posteriori error estimate takes account of the finite element approximation error and the truncation error of the DtN operator which is shown to decay exponentially with respect to the truncation parameter. Numerical experiments are presented to illustrate the effectiveness of the proposed method.", "label": 1, "source": "scigen_human", "idx": 103, "lang": "en"}
{"text": "We determine the cost of performing Shor's algorithm for integer factorization on a ternary quantum computer, using two natural models of universal fault-tolerant computing: (i) a model based on magic state distillation that assumes the availability of the ternary Clifford gates, projective measurements, classical control as its natural instrumentation set; (ii) a model based on a metaplectic topological quantum computer (MTQC). A natural choice to implement Shor's algorithm on a ternary quantum computer is to translate the entire arithmetic into a ternary form. However, it is also possible to emulate the standard binary version of the algorithm by encoding each qubit in a three-level system. We compare the two approaches and analyze the complexity of implementing Shor's period finding function in the two models. We also highlight the fact that the cost of achieving universality through magic states in MTQC architecture is asymptotically lower than in generic ternary case.", "label": 1, "source": "scigen_human", "idx": 104, "lang": "en"}
{"text": "Integrating mobile edge computing (MEC) and wireless power transfer (WPT) has been regarded as a promising technique to improve computation capabilities for self-sustainable Internet of Things (IoT) devices. This paper investigates a wireless powered multiuser MEC system, where a multi-antenna access point (AP) (integrated with an MEC server) broadcasts wireless power to charge multiple users for mobile computing. We consider a time-division multiple access (TDMA) protocol for multiuser computation offloading. Under this setup, we aim to maximize the weighted sum of the computation rates (in terms of the number of computation bits) across all the users, by jointly optimizing the energy transmit beamformer at the AP, the task partition for the users (for local computing and offloading, respectively), and the time allocation among the users. We derive the optimal solution in a semi-closed form via convex optimization techniques. Numerical results show the merit of the proposed design over alternative benchmark schemes.", "label": 1, "source": "scigen_human", "idx": 105, "lang": "en"}
{"text": "We present Task Bench, a parameterized benchmark designed to explore the performance of parallel and distributed programming systems under a variety of application scenarios. Task Bench lowers the barrier to benchmarking multiple programming systems by making the implementation for a given system orthogonal to the benchmarks themselves: every benchmark constructed with Task Bench runs on every Task Bench implementation. Furthermore, Task Bench's parameterization enables a wide variety of benchmark scenarios that distill the key characteristics of larger applications. We conduct a comprehensive study with implementations of Task Bench in 15 programming systems on up to 256 Haswell nodes of the Cori supercomputer. We introduce a novel metric, minimum effective task granularity to study the baseline runtime overhead of each system. We show that when running at scale, 100 s is the smallest granularity that even the most efficient systems can reliably support with current technologies. We also study each system's scalability, ability to hide communication and mitigate load imbalance.", "label": 1, "source": "scigen_human", "idx": 106, "lang": "en"}
{"text": "Even though many machine algorithms have been proposed for entity resolution, it remains very challenging to find a solution with quality guarantees. In this paper, we propose a novel HUman and Machine cOoperation (HUMO) framework for entity resolution (ER), which divides an ER workload between the machine and the human. HUMO enables a mechanism for quality control that can flexibly enforce both precision and recall levels. We introduce the optimization problem of HUMO, minimizing human cost given a quality requirement, and then present three optimization approaches: a conservative baseline one purely based on the monotonicity assumption of precision, a more aggressive one based on sampling and a hybrid one that can take advantage of the strengths of both previous approaches. Finally, we demonstrate by extensive experiments on real and synthetic datasets that HUMO can achieve high-quality results with reasonable return on investment (ROI) in terms of human cost, and it performs considerably better than the state-of-the-art alternatives in quality control.", "label": 1, "source": "scigen_human", "idx": 107, "lang": "en"}
{"text": "Cosmic dust particles effectively attenuate starlight. Their absorption of starlight produces emission spectra from the near- to far-infrared, which depends on the sizes and properties of the dust grains, and spectrum of the heating radiation field. The near- to mid-infrared is dominated by the emissions by very small grains. Modeling the absorption of starlight by these particles is, however, computationally expensive and a significant bottleneck for self-consistent radiation transport codes treating the heating of dust by stars. In this paper, we summarize the formalism for computing the stochastic emissivity of cosmic dust, which was developed in earlier works, and present a new library HEATCODE implementing this formalism for the calculation for arbitrary grain properties and heating radiation fields. Our library is highly optimized for general-purpose processors with multiple cores and vector instructions, with hierarchical memory cache structure. The HEATCODE library also efficiently runs on co-processor cards implementing the Intel Many Integrated Core (Intel MIC) architecture. We discuss in detail the optimization steps that we took in order to optimize for the Intel MIC architecture, which also significantly benefited the performance of the code on general-purpose processors, and provide code samples and performance benchmarks for each step. The HEATCODE library performance on a single Intel Xeon Phi coprocessor (Intel MIC architecture) is 2 times a general-purpose two-socket multicore processor system with approximately the same nominal power consumption. The library supports heterogeneous calculations employing host processors simultaneously with multiple coprocessors, and can be easily incorporated into existing radiation transport codes.", "label": 1, "source": "scigen_human", "idx": 108, "lang": "en"}
{"text": "In 2012, Barbulescu, Detrey, Estibals and Zimmermann proposed a new framework to exhaustively search for optimal formulae for evaluating bilinear maps over finite fields, such as Strassen or Karatsuba formulae. The main contribution of this work is a new criterion to aggressively prune useless branches in the exhaustive search, thus leading to the computation of new optimal formulae. We apply in particular our approach to the short product modulo X 5 and the circulant product modulo X 5 1). Moreover, we are able to prove that there is essentially only one optimal decomposition of the product of 3 2 by 2 3 matrices up to the action of some group of automorphisms.", "label": 1, "source": "scigen_human", "idx": 110, "lang": "en"}
{"text": "In this paper, we focus on how to dynamically allocate a divisible resource fairly among n players who arrive and depart over time. The players may have general heterogeneous valuations over the resource. It is known that exact envy-free and proportional allocations may not exist in the dynamic setting . Thus, we will study to what extent we can guarantee the fairness in the dynamic setting. We first design two algorithms which are O (log n) -proportional and O (n) -envy-free for the setting with general valuations, and by constructing the adversary instances such that all dynamic algorithms must be at least (1) -proportional and (n log n) -envy-free, we show that the bounds are tight up to a logarithmic factor. Moreover, we introduce the setting where the players' valuations are uniform on the resource but with different demands, which generalize the setting of . We prove an O (log n) upper bound and a tight lower bound for this case.", "label": 1, "source": "scigen_human", "idx": 111, "lang": "en"}
{"text": "The engineering of machine learning systems is still a nascent field; relying on a seemingly daunting collection of quickly evolving tools and best practices. It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques. We outline four example machine learning problems that can be solved using open source machine learning libraries, and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications.", "label": 1, "source": "scigen_human", "idx": 112, "lang": "en"}
{"text": "The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.", "label": 1, "source": "scigen_human", "idx": 113, "lang": "en"}
{"text": "Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at .", "label": 1, "source": "scigen_human", "idx": 114, "lang": "en"}
{"text": "Link prediction is a fundamental task in statistical network analysis. Recent advances have been made on learning flexible nonparametric Bayesian latent feature models for link prediction. In this paper, we present a max-margin learning method for such nonparametric latent feature relational models. Our approach attempts to unite the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction. It inherits the advances of nonparametric Bayesian methods to infer the unknown latent social dimension, while for discriminative link prediction, it adopts the max-margin learning principle by minimizing a hinge-loss using the linear expectation operator, without dealing with a highly nonlinear link likelihood function. For posterior inference, we develop an efficient stochastic variational inference algorithm under a truncated mean-field assumption. Our methods can scale up to large-scale real networks with millions of entities and tens of millions of positive links. We also provide a full Bayesian formulation, which can avoid tuning regularization hyper-parameters. Experimental results on a diverse range of real datasets demonstrate the benefits inherited from max-margin learning and Bayesian nonparametric inference.", "label": 1, "source": "scigen_human", "idx": 115, "lang": "en"}
{"text": "Long Short-Term Memory networks (LSTMs) can be trained to realize inverse control of physics-based sound synthesizers. Physics-based sound synthesizers simulate the laws of physics to produce output sound according to input gesture signals. When a user's gestures are measured in real time, she or he can use them to control physics-based sound synthesizers, thereby creating simulated virtual instruments. An intriguing question is how to program a computer to learn to play such physics-based models. This work demonstrates that LSTMs can be trained to accomplish this inverse control task with four physics-based sound synthesizers. Keywords: LSTM, physics-based models, sound synthesis", "label": 1, "source": "scigen_human", "idx": 116, "lang": "en"}
{"text": "Unlike many complex networks studied in the literature, social networks rarely exhibit unanimous behavior, or consensus . This requires a development of mathematical models that are sufficiently simple to be examined and capture, at the same time, the complex behavior of real social groups, where opinions and actions related to them may form clusters of different size. One such model, proposed by Friedkin and Johnsen, extends the idea of conventional consensus algorithm (also referred to as the iterative opinion pooling) to take into account the actors' prejudices, caused by some exogenous factors and leading to disagreement in the final opinions. In this paper, we offer a novel multidimensional extension, describing the evolution of the agents' opinions on several topics. Unlike the existing models, these topics are interdependent, and hence the opinions being formed on these topics are also mutually dependent. We rigorous examine stability properties of the proposed model, in particular, convergence of the agents' opinions. Although our model assumes synchronous communication among the agents, we show that the same final opinions may be reached \"on average\" via asynchronous gossip-based protocols.", "label": 1, "source": "scigen_human", "idx": 117, "lang": "en"}
{"text": "Dominators provide a general mechanism for identifying reconverging paths in graphs. This is useful for a number of applications in Computer-Aided Design (CAD) including signal probability computation in biased random simulation, switching activity estimation in power and noise analysis, and cut points identification in equivalence checking. However, traditional single-vertex dominators are too rare in circuit graphs. In order to handle reconverging paths more efficiently, we consider the case of double-vertex dominators which occur more frequently. First, we derive a number of specific properties of double-vertex dominators. Then, we describe a data structure for representing all double-vertex dominators of a given vertex in linear space. Finally, we present an algorithm for finding all double-vertex dominators of a given vertex in linear time. Our results provide an efficient systematic way of partitioning large graphs along the reconverging points of the signal flow.", "label": 1, "source": "scigen_human", "idx": 118, "lang": "en"}
{"text": "We propose a novel randomized incremental gradient algorithm, namely, VAriance-Reduced Accelerated Gradient (Varag), for finite-sum optimization. Equipped with a unified step-size policy that adjusts itself to the value of the conditional number, Varag exhibits the unified optimal rates of convergence for solving smooth convex finite-sum problems directly regardless of their strong convexity. Moreover, Varag is the first of its kind that benefits from the strong convexity of the data-fidelity term, and solves a wide class of problems only satisfying an error bound condition rather than strong convexity, both resulting in the optimal linear rate of convergence. Varag can also be extended to solve stochastic finite-sum problems.", "label": 1, "source": "scigen_human", "idx": 119, "lang": "en"}
{"text": "The fact that individuals will most likely behave differently in different situations begets the introduction of conditional strategies. Inspired by this, we study the evolution of cooperation in the spatial public goods game, where besides unconditional cooperators and defectors, also different types of conditional cooperators compete for space. Conditional cooperators will contribute to the public good only if other players within the group are likely to cooperate as well, but will withhold their contribution otherwise. Depending on the number of other cooperators that are required to elicit cooperation of a conditional cooperator, the latter can be classified in as many types as there are players within each group. We find that the most cautious cooperators, such that require all other players within a group to be conditional cooperators, are the undisputed victors of the evolutionary process, even at very low synergy factors. We show that the remarkable promotion of cooperation is due primarily to the spontaneous emergence of quarantining of defectors, which become surrounded by conditional cooperators and are forced into isolated convex \"bubbles\" from where they are unable to exploit the public good. This phenomenon can be observed only in structured populations, thus adding to the relevance of pattern formation for the successful evolution of cooperation.", "label": 1, "source": "scigen_human", "idx": 120, "lang": "en"}
{"text": "In a guessing game, players guess the value of a random real number selected using some probability density function. The winner may be determined in various ways; for example, a winner can be a player whose guess is closest in magnitude to the target or a winner can be a player coming closest without guessing higher than the target. We study optimal strategies for players in these games and determine some of them for two, three, and four players.", "label": 1, "source": "scigen_human", "idx": 121, "lang": "en"}
{"text": "We propose a novel network pruning approach by information preserving of pre-trained network weights (filters). Network pruning with the information preserving is formulated as a matrix sketch problem, which is efficiently solved by the off-the-shelf Frequent Direction method. Our approach, referred to as FilterSketch, encodes the second-order information of pre-trained weights, which enables the representation capacity of pruned networks being recovered with a simple fine-tuning procedure. FilterSketch requires neither training from scratch nor data-driven iterative optimization, leading to a several-orders-of-magnitude reduction of time cost in the optimization of pruning. Experiments on CIFAR-10 show that FilterSketch reduces 63.3 of FLOPs and prunes 59.9 of network parameters with negligible accuracy cost for ResNet-110. On ILSVRC-2012, it reduces 45.5 of FLOPs and removes 43.0 of parameters with only 0.69 accuracy drop for ResNet-50. Codes and experimental results can be available at .", "label": 1, "source": "scigen_human", "idx": 123, "lang": "en"}
{"text": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture.", "label": 1, "source": "scigen_human", "idx": 125, "lang": "en"}
{"text": "We combine momentum from machine learning with evolutionary dynamics, where momentum can be viewed as a simple mechanism of intergenerational memory. Using information divergences as Lyapunov functions, we show that momentum accelerates the convergence of evolutionary dynamics including the replicator equation and Euclidean gradient descent on populations. When evolutionarily stable states are present, these methods prove convergence for small learning rates or small momentum, and yield an analytic determination of the relative decrease in time to converge that agrees well with computations. The main results apply even when the evolutionary dynamic is not a gradient flow. We also show that momentum can alter the convergence properties of these dynamics, for example by breaking the cycling associated to the rock-paper-scissors landscape, leading to either convergence to the ordinarily non-absorbing equilibrium, or divergence, depending on the value and mechanism of momentum.", "label": 1, "source": "scigen_human", "idx": 126, "lang": "en"}
{"text": "The arXiv has collected 1.5 million pre-print articles over 28 years, hosting literature from scientific fields including Physics, Mathematics, and Computer Science. Each pre-print features text, figures, authors, citations, categories, and other metadata. These rich, multi-modal features, combined with the natural graph structure - created by citation, affiliation, and co-authorship - makes the arXiv an exciting candidate for benchmarking next-generation models. Here we take the first necessary steps toward this goal, by providing a pipeline which standardizes and simplifies access to the arXiv's publicly available data. We use this pipeline to extract and analyze a 6.7 million edge citation graph, with an 11 billion word corpus of full-text research articles. We present some baseline classification results, and motivate application of more exciting generative graph models.", "label": 1, "source": "scigen_human", "idx": 127, "lang": "en"}
{"text": "Point clouds provide a flexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervised learning tasks such as object classification and semantic segmentation. While massive point cloud datasets can be captured using modern scanning technology, manually labelling such large 3D point clouds for supervised learning tasks is a cumbersome process. This necessitates methods that can learn from unlabelled data to significantly reduce the number of annotated samples needed in supervised learning. We propose a self-supervised learning task for deep learning on raw point cloud data in which a neural network is trained to reconstruct point clouds whose parts have been randomly rearranged. While solving this task, representations that capture semantic properties of the point cloud are learned. Our method is agnostic of network architecture and outperforms current unsupervised learning approaches in downstream object classification tasks. We show experimentally, that pre-training with our method before supervised training improves the performance of state-of-the-art models and significantly improves sample efficiency.", "label": 1, "source": "scigen_human", "idx": 129, "lang": "en"}
{"text": "In cost sharing games with delays, a set of agents jointly allocates a finite subset of resources. Each resource has a fixed cost that has to be shared by the players, and each agent has a non-shareable player-specific delay for each resource. A prominent example is uncapacitated facility location (UFL), where facilities need to be opened (at a shareable cost) and clients want to connect to opened facilities. Each client pays a cost share and his non-shareable physical connection cost. Given any profile of subsets allocated by the agents, a separable cost sharing protocol determines cost shares that satisfy budget balance on every resource and separability over the resources. Moreover, a separable protocol guarantees existence of pure Nash equilibria in the induced strategic game for the agents. In this paper, we study separable cost sharing protocols in several general combinatorial domains. We provide black-box reductions to reduce the design of a separable cost-sharing protocol to the design of an approximation algorithm for the underlying cost minimization problem. In this way, we obtain new separable cost-sharing protocols in games based on arbitrary player-specific matroids, single-source connection games without delays, and connection games on n -series-parallel graphs with delays. All these reductions are efficiently computable - given an initial allocation profile, we obtain a cheaper profile and separable cost shares turning the profile into a pure Nash equilibrium. Hence, in these domains any approximation algorithm can be used to obtain a separable cost sharing protocol with a price of stability bounded by the approximation factor.", "label": 1, "source": "scigen_human", "idx": 130, "lang": "en"}
{"text": "This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learning Environment using deep reinforcement learning. The proposed method, called human checkpoint replay , consists in using checkpoints sampled from human gameplay as starting points for the learning process. This is meant to compensate for the difficulties of current exploration strategies, such as -greedy, to find successful control policies in games with sparse rewards. Like other deep reinforcement learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma's Revenge and Private Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human experience replay.", "label": 1, "source": "scigen_human", "idx": 131, "lang": "en"}
{"text": "This paper presents a distance-based discriminative framework for learning with probability distributions. Instead of using kernel mean embeddings or generalized radial basis kernels, we introduce embeddings based on dissimilarity of distributions to some reference distributions denoted as templates. Our framework extends the theory of similarity of to the population distribution case and we show that, for some learning problems, some dissimilarity on distribution achieves low-error linear decision functions with high probability. Our key result is to prove that the theory also holds for empirical distributions. Algorithmically, the proposed approach consists in computing a mapping based on pairwise dissimilarity where learning a linear decision function is amenable. Our experimental results show that the Wasserstein distance embedding performs better than kernel mean embeddings and computing Wasserstein distance is far more tractable than estimating pairwise Kullback-Leibler divergence of empirical distributions.", "label": 1, "source": "scigen_human", "idx": 132, "lang": "en"}
{"text": "Aim: In contrast to studies of defects found during code review, we aim to clarify whether code reviews measures can explain the prevalence of post-release defects. Method: We replicate McIntosh et al.'s study that uses additive regression to model the relationship between defects and code reviews. To increase external validity, we apply the same methodology on a new software project. We discuss our findings with the first author of the original study, McIntosh. We then investigate how to reduce the impact of correlated predictors in the variable selection process and how to increase understanding of the inter-relationships among the predictors by employing Bayesian Network (BN) models. Context: As in the original study, we use the same measures authors obtained for Qt project in the original study. We mine data from version control and issue tracker of Google Chrome and operationalize measures that are close analogs to the large collection of code, process, and code review measures used in the replicated the study. Results: Both the data from the original study and the Chrome data showed high instability of the influence of code review measures on defects with the results being highly sensitive to variable selection procedure. Models without code review predictors had as good or better fit than those with review predictors. Replication, however, confirms with the bulk of prior work showing that prior defects, module size, and authorship have the strongest relationship to post-release defects. The application of BN models helped explain the observed instability by demonstrating that the review-related predictors do not affect post-release defects directly and showed indirect effects. For example, changes that have no review discussion tend to be associated with files that have had many prior defects which in turn increase the number of post-release defects. We hope that similar analyses of other software engineering techniques may also yield a more nuanced view of their impact. Our replication package including our data and scripts is publicly available .", "label": 1, "source": "scigen_human", "idx": 133, "lang": "en"}
{"text": "Population synthesis is concerned with the generation of synthetic yet realistic representations of populations. It is a fundamental problem in the modeling of transport where the synthetic populations of micro-agents represent a key input to most agent-based models. In this paper, a new methodological framework for how to 'grow' pools of micro-agents is presented. The model framework adopts a deep generative modeling approach from machine learning based on a Variational Autoencoder (VAE). Compared to the previous population synthesis approaches, including Iterative Proportional Fitting (IPF), Gibbs sampling and traditional generative models such as Bayesian Networks or Hidden Markov Models, the proposed method allows fitting the full joint distribution for high dimensions. The proposed methodology is compared with a conventional Gibbs sampler and a Bayesian Network by using a large-scale Danish trip diary. It is shown that, while these two methods outperform the VAE in the low-dimensional case, they both suffer from scalability issues when the number of modeled attributes increases. It is also shown that the Gibbs sampler essentially replicates the agents from the original sample when the required conditional distributions are estimated as frequency tables. In contrast, the VAE allows addressing the problem of sampling zeros by generating agents that are virtually different from those in the original data but have similar statistical properties. The presented approach can support agent-based modeling at all levels by enabling richer synthetic populations with smaller zones and more detailed individual characteristics.", "label": 1, "source": "scigen_human", "idx": 134, "lang": "en"}
{"text": "The likelihood model of high dimensional data X n can often be expressed as p ( X n Z n , ), where : ( k) k [ K ] is a collection of hidden features shared across objects, indexed by n , and Z n is a non-negative factor loading vector with K entries where Z n k indicates the strength of k used to express X n . In this paper, we introduce random function priors for Z n for modeling correlations among its K dimensions Z n 1 through Z n K , which we call population random measure embedding (PRME). Our model can be viewed as a generalized paintbox model (,) using random functions, and can be learned efficiently with neural networks via amortized variational inference. We derive our Bayesian nonparametric method by applying a representation theorem on separately exchangeable discrete random measures.", "label": 1, "source": "scigen_human", "idx": 135, "lang": "en"}
{"text": "We study the metric facility location problem with client insertions and deletions. This setting differs from the classic dynamic facility location problem, where the set of clients remains the same, but the metric space can change over time. We show a deterministic algorithm that maintains a constant factor approximation to the optimal solution in worst-case time O (2 O ( 2 per client insertion or deletion in metric spaces while answering queries about the cost in O (1) time, where denotes the doubling dimension of the metric. For metric spaces with bounded doubling dimension, the update time is polylogarithmic in the parameters of the problem.", "label": 1, "source": "scigen_human", "idx": 136, "lang": "en"}
{"text": "Although shill bidding is a common auction fraud, it is however very tough to detect. Due to the unavailability and lack of training data, in this study, we build a high-quality labeled shill bidding dataset based on recently collected auctions from eBay. Labeling shill biding instances with multidimensional features is a critical phase for the fraud classification task. For this purpose, we introduce a new approach to systematically label the fraud data with the help of the hierarchical clustering CURE that returns remarkable results as illustrated in the experiments.", "label": 1, "source": "scigen_human", "idx": 137, "lang": "en"}
{"text": "Compressive sensing (CS) is a promising technology for realizing energy-efficient wireless sensors for long-term health monitoring. However, conventional model-driven CS frameworks suffer from limited compression ratio and reconstruction quality when dealing with physiological signals due to inaccurate models and the overlook of individual variability. In this paper, we propose a data-driven CS framework that can learn signal characteristics and personalized features from any individual recording of physiologic signals to enhance CS performance with a minimized number of measurements. Such improvements are accomplished by a co-training approach that optimizes the sensing matrix and the dictionary towards improved restricted isometry property and signal sparsity, respectively. Experimental results upon ECG signals show that the proposed method, at a compression ratio of 10x, successfully reduces the isometry constant of the trained sensing matrices by 86 against random matrices and improves the overall reconstructed signal-to-noise ratio by 15dB over conventional model-driven approaches.", "label": 1, "source": "scigen_human", "idx": 138, "lang": "en"}
{"text": "Despite being popularly referred to as the ultimate solution for all problems of our current electric power system, smart grid is still a growing and unstable concept. It is usually considered as a set of advanced features powered by promising technological solutions. In this paper, we describe smart grid as a socio-technical transition and illustrate the evolutionary path on which a smart grid can be realized. Through this conceptual lens, we revealed the role of big data, and how it can fuel the organic growth of smart grid. We also provided a rough estimate of how much data will be potentially generated from different data sources, which helps clarify the big data challenge during the evolutionary process.", "label": 1, "source": "scigen_human", "idx": 139, "lang": "en"}
{"text": "An instance of the Connected Maximum Cut problem consists of an undirected graph G (V , E) and the goal is to find a subset of vertices S V that maximizes the number of edges in the cut (S) such that the induced graph G [ S ] is connected. We present the first non-trivial (1 log n) approximation algorithm for the Connected Maximum Cut problem in general graphs using novel techniques. We then extend our algorithm to edge weighted case and obtain a poly-logarithmic approximation algorithm. Interestingly, in contrast to the classical Max-Cut problem that can be solved in polynomial time on planar graphs, we show that the Connected Maximum Cut problem remains NP-hard on unweighted, planar graphs. On the positive side, we obtain a polynomial time approximation scheme for the Connected Maximum Cut problem on planar graphs and more generally on bounded genus graphs.", "label": 1, "source": "scigen_human", "idx": 141, "lang": "en"}
{"text": "Given a social network modeled as a weighted graph G , the influence maximization problem seeks k vertices to become initially influenced, to maximize the expected number of influenced nodes under a particular diffusion model. The influence maximization problem has been proven to be NP-hard, and most proposed solutions to the problem are approximate greedy algorithms, which can guarantee a tunable approximation ratio for their results with respect to the optimal solution. The state-of-the-art algorithms are based on Reverse Influence Sampling (RIS) technique, which can offer both computational efficiency and non-trivial 1 1 e ) -approximation ratio guarantee for any 0 . RIS-based algorithms, despite their lower computational cost compared to other methods, still require long running times to solve the problem in large-scale graphs with low values of . In this paper, we present a novel and efficient parallel implementation of a RIS-based algorithm, namely IMM, on GPGPU. The proposed solution can significantly reduce the running time on large-scale graphs with low values of . Furthermore, we show that our proposed parallel algorithm can solve other variations of the IM problem, only by applying minor modifications. Experimental results show that the proposed solution reduces the runtime by a factor up to 220 .", "label": 1, "source": "scigen_human", "idx": 142, "lang": "en"}
{"text": "Graph-specific computing with the support of dedicated accelerator has greatly boosted the graph processing in both efficiency and energy. Nevertheless, their data conflict management is still sequential in essential when some vertex needs a large number of conflicting updates at the same time, leading to prohibitive performance degradation. This is particularly true for processing natural graphs. In this paper, we have the insight that the atomic operations for the vertex updating of many graph algorithms (e.g., BFS, PageRank and WCC) are typically incremental and simplex. This hence allows us to parallelize the conflicting vertex updates in an accumulative manner. We architect a novel graph-specific accelerator that can simultaneously process atomic vertex updates for massive parallelism on the conflicting data access while ensuring the correctness. A parallel accumulator is designed to remove the serialization in atomic protection for conflicting vertex updates through merging their results in parallel. Our implementation on Xilinx Virtex UltraScale XCVU9P with a wide variety of typical graph algorithms shows that our accelerator achieves an average throughput by 2.36 GTEPS as well as up to 3.14x performance speedup in comparison with state-of-the-art ForeGraph (with single-chip version).", "label": 1, "source": "scigen_human", "idx": 143, "lang": "en"}
{"text": "Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with writing software. We propose TEASPN 1 footnote 1 1 footnote 1 See for the screencast and for more general info about TEASPN. , a protocol and an open-source framework for achieving integrated writing assistance environments. The protocol standardizes the way writing software communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in natural language processing (NLP) with low cost. As a result, users can enjoy the integrated experience in their favorite writing software. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text.", "label": 1, "source": "scigen_human", "idx": 144, "lang": "en"}
{"text": "We have shown previously that our parameter-reduced variants of Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) are comparable in performance to the standard LSTM RNN on the MNIST dataset. In this study, we show that this is also the case for two diverse benchmark datasets, namely, the review sentiment IMDB and the 20 Newsgroup datasets. Specifically, we focus on two of the simplest variants, namely LSTM6 (i.e., standard LSTM with three constant fixed gates) and LSTMC6 (i.e., LSTM6 with further reduced cell body input block). We demonstrate that these two aggressively reduced-parameter variants are competitive with the standard LSTM when hyper-parameters, e.g., learning parameter, number of hidden units and gate constants are set properly. These architectures enable speeding up training computations and hence, these networks would be more suitable for online training and inference onto portable devices with relatively limited computational resources.", "label": 1, "source": "scigen_human", "idx": 145, "lang": "en"}
{"text": "In this paper, we show that any scaled-up version of any discrete self-similar tree fractal does not strictly self-assemble, at any temperature, in Winfree's abstract Tile Assembly Model.", "label": 1, "source": "scigen_human", "idx": 146, "lang": "en"}
{"text": "In this paper, we uncover a new off-path TCP hijacking attack that can be used to terminate victim TCP connections or inject forged data into victim TCP connections by manipulating the new mixed IPID assignment method, which is widely used in Linux kernel version 4.18 and beyond to help defend against TCP hijacking attacks. The attack has three steps. First, an off-path attacker can downgrade the IPID assignment for TCP packets from the more secure per-socket-based policy to the less secure hash-based policy, building a shared IPID counter that forms a side channel on the victim. Second, the attacker detects the presence of TCP connections by observing the shared IPID counter on the victim. Third, the attacker infers the sequence number and the acknowledgment number of the detected connection by observing the side channel of the shared IPID counter. Consequently, the attacker can completely hijack the connection, i.e., resetting the connection or poisoning the data stream. We evaluate the impacts of this off-path TCP attack in the real world. Our case studies of SSH DoS, manipulating web traffic, and poisoning BGP routing tables show its threat on a wide range of applications. Our experimental results show that our off-path TCP attack can be constructed within 215 seconds and the success rate is over 88. Finally, we analyze the root cause of the exploit and develop a new IPID assignment method to defeat this attack. We prototype our defense in Linux 4.18 and confirm its effectiveness through extensive evaluation over real applications on the Internet.", "label": 1, "source": "scigen_human", "idx": 148, "lang": "en"}
{"text": "In this paper, we first propose a method that can efficiently compute the maximal robust controlled invariant set for discrete-time linear systems with pure delay in input. The key to this method is to construct an auxiliary linear system (without delay) with the same state-space dimension of the original system in consideration and to relate the maximal invariant set of the auxiliary system to that of the original system. When the system is subject to disturbances, guaranteeing safety is harder for systems with input delays. Ability to incorporate any additional information about the disturbance becomes more critical in these cases. Motivated by this observation, in the second part of the paper, we generalize the proposed method to take into account additional preview information on the disturbances, while maintaining computational efficiency. Compared with the naive approach of constructing a higher dimensional system by appending the state-space with the delayed inputs and previewed disturbances, the proposed approach is demonstrated to scale much better with the increasing delay time.", "label": 1, "source": "scigen_human", "idx": 149, "lang": "en"}
{"text": "Increasing technological sophistication and widespread use of smartphones and wearable devices provide opportunities for innovative and highly personalized health interventions. A Just-In-Time Adaptive Intervention (JITAI) uses real-time data collection and communication capabilities of modern mobile devices to deliver interventions in real-time that are adapted to the in-the-moment needs of the user. The lack of methodological guidance in constructing data-based JITAIs remains a hurdle in advancing JITAI research despite the increasing popularity of JITAIs among clinical scientists. In this article, we make a first attempt to bridge this methodological gap by formulating the task of tailoring interventions in real-time as a contextual bandit problem. Interpretability requirements in the domain of mobile health lead us to formulate the problem differently from existing formulations intended for web applications such as ad or news article placement. Under the assumption of linear reward function, we choose the reward function (the \"critic parameterization separately from a lower dimensional parameterization of stochastic policies (the \"actor. We provide an online actor-critic algorithm that guides the construction and refinement of a JITAI. Asymptotic properties of the actor-critic algorithm are developed and backed up by numerical experiments. Additional numerical experiments are conducted to test the robustness of the algorithm when idealized assumptions used in the analysis of contextual bandit algorithm are breached.", "label": 1, "source": "scigen_human", "idx": 150, "lang": "en"}
{"text": "We describe a way of assigning labels to the vertices of any undirected graph on up to n vertices, each composed of n 2 O (1) bits, such that given the labels of two vertices, and no other information regarding the graph, it is possible to decide whether or not the vertices are adjacent in the graph. This is optimal, up to an additive constant, and constitutes the first improvement in almost 50 years of an n 2 O (log n) bound of Moon. As a consequence, we obtain an induced-universal graph for n -vertex graphs containing only O (2 n 2) vertices, which is optimal up to a multiplicative constant, solving an open problem of Vizing from 1968. We obtain similar tight results for directed graphs, tournaments and bipartite graphs.", "label": 1, "source": "scigen_human", "idx": 151, "lang": "en"}
{"text": "Laminated glass structures are formed by stiff layers of glass connected with a compliant plastic interlayer. Due to their slenderness and heterogeneity, they exhibit a complex mechanical response that is difficult to capture by single-layer models even in the elastic range. The purpose of this paper is to introduce an efficient and reliable finite element approach to the simulation of the immediate response of laminated glass beams. It proceeds from a refined plate theory due to Mau (1973), as we treat each layer independently and enforce the compatibility by the Lagrange multipliers. At the layer level, we adopt the finite-strain shear deformable formulation of Reissner (1972) and the numerical framework by Ibrahimbegovic and Frey (1993). The resulting system is solved by the Newton method with consistent linearization. By comparing the model predictions against available experimental data, analytical methods and two-dimensional finite element simulations, we demonstrate that the proposed formulation is reliable and provides accuracy comparable to the detailed two-dimensional finite element analyzes. As such, it offers a convenient basis to incorporate more refined constitutive description of the interlayer.", "label": 1, "source": "scigen_human", "idx": 152, "lang": "en"}
{"text": "If a Micro Processor Unit (MPU) receives an external electric signal as noise, the system function will freeze or malfunction easily. A new resilience strategy is implemented in order to reset the MPU automatically and stop the MPU from freezing or malfunctioning. The technique is useful for embedded systems which work in non-human environments. However, evaluating resilience strategies is difficult because their effectiveness depends on numerous, complex, interacting factors. In this paper, we use probabilistic model checking to evaluate the embedded systems installed with the above mentioned new resilience strategy. Qualitative evaluations are implemented with 6 PCTL formulas, and quantitative evaluations use two kinds of evaluation. One is system failure reduction, and the other is ADT (Average Down Time), the industry standard. Our work demonstrates the benefits brought by the resilience strategy. Experimental results indicate that our evaluation is cost-effective and reliable.", "label": 1, "source": "scigen_human", "idx": 153, "lang": "en"}
{"text": "We present goal-oriented a posteriori error estimates for the automatic variationally stable finite element (AVS-FE) method for scalar-valued convection-diffusion problems. The AVS-FE method is a Petrov-Galerkin method in which the test space is broken, whereas the trial space consists of classical FE basis functions, e.g., C 0 or Raviart-Thomas functions. We employ the concept of optimal test functions of the discontinuous Petrov-Galerkin (DPG) method by Demkowicz and Gopalakrishnan , leading to unconditionally stable FE approximations. Remarkably, by using C 0 or Raviart-Thomas trial spaces, the optimal discontinuous test functions can be computed in a completely decoupled element-by-element fashion. To establish the error estimators we present two approaches: i) following the classical approach of Becker and Rannacher , i.e., the dual solution is sought in the (broken) test space, and i i) introducing an alternative approach in which we seek C 0 , or Raviart-Thomas, AVS-FE approximations of the dual solution by using the underlying strong form of the dual boundary value problem (BVP). Various numerical verifications for 2D convection-dominated diffusion BVPs show that the estimates of the approximation error by the new alternative method are highly accurate, while the classical approach leads to error estimates of poor quality. Lastly, we present an algorithm for h - adaptive processes based on control of the numerical approximation error via the new alternative approach. Numerical verifications show that the estimator maintains high accuracy as the error converges to zero.", "label": 1, "source": "scigen_human", "idx": 155, "lang": "en"}
{"text": "Session types have been proposed as a means of statically verifying implementations of communication protocols. Although prior work has been successful in verifying some classes of protocols, it does not cope well with parameterized, multi-actor scenarios with inherent asynchrony. For example, the sliding window protocol is inexpressible in previously proposed session type systems. This paper describes System-A, a new typing language which overcomes many of the expressiveness limitations of prior work. System-A explicitly supports asynchrony and parallelism, as well as multiple forms of parameterization. We define System-A and show how it can be used for the static verification of a large class of asynchronous communication protocols.", "label": 1, "source": "scigen_human", "idx": 157, "lang": "en"}
{"text": "We propose an improved discriminative model prediction method for robust long-term tracking based on a pre-trained short-term tracker. The baseline pre-trained short-term tracker is SuperDiMP which combines the bounding-box regressor of PrDiMP with the standard DiMP classifier. Our tracker RLT-DiMP improves SuperDiMP in the following three aspects: (1) Uncertainty reduction using random erasing: To make our model robust, we exploit an agreement from multiple images after erasing random small rectangular areas as a certainty. And then, we correct the tracking state of our model accordingly. (2) Random search with spatio-temporal constraints: we propose a robust random search method with a score penalty applied to prevent the problem of sudden detection at a distance. (3) Background augmentation for more discriminative feature learning: We augment various backgrounds that are not included in the search area to train a more robust model in the background clutter. In experiments on the VOT-LT2020 benchmark dataset, the proposed method achieves comparable performance to the state-of-the-art long-term trackers. The source code is available at: .", "label": 1, "source": "scigen_human", "idx": 158, "lang": "en"}
{"text": "We show that for every integer k 2 , the Res (k) propositional proof system does not have the weak feasible disjunction property. Next, we generalize a recent result of Atserias and Muller to Res (k). We show that if NP is not included in P (resp. QP, SUBEXP) then for every integer k 1 , Res (k) is not automatable in polynomial (resp. quasi-polynomial, subexponential) time.", "label": 1, "source": "scigen_human", "idx": 159, "lang": "en"}
{"text": "Coronavirus or COVID-19, which has been declared pandemic by the World Health Organization, has incurred huge losses to the lives of people throughout the world. Although, the scientists, researchers and doctors are working round the clock to develop a vaccine for COVID-19, it may take a year or two to make a safe and effective vaccine available for the world. In current circumstances, a solution must be developed to control or stop the spread of the virus. For this purpose, a novel technique based on call data record analysis (CDRA) and contact tracing is proposed that can effectively control the coronavirus outbreak. A positive coronavirus patient can be traced through CDRA and contact tracing. The technique can track the path traversed by the patient and collect the cell numbers of all those people who have met with the patient. Keeping in tact the privacy of this group of people, who are contacted through their cell numbers so that they can isolate themselves till the result of their coronavirus test arrives. If a test result of a person comes positive among the group, then heshe must be isolated and same CDRA and contact tracing procedures are adopted for that person. A COVID-19 patient is geo tagged and alerts are sent if any violation of isolation is done by the patient. Moreover, the general public is informed in advance to avoid the path followed by the patients. This cost effective mechanism is not only capable to control the coronavirus outbreak but also helps in isolating the patient in hisher house.", "label": 1, "source": "scigen_human", "idx": 160, "lang": "en"}
{"text": "We are experiencing an unprecedented healthcare crisis caused by the newly-discovered corona-virus disease (COVID-19). The outbreaks of COVID-19 reveal the frailties of existing healthcare systems. Therefore, the digital transformation of healthcare systems becomes an inevitable trend. During this process, the Internet of Medical Things (IoMT) plays a crucial role while intrinsic vulnerabilities of security and privacy deter the wide adoption of IoMT. In this article, we present a blockchain-enabled IoMT to address the security and privacy concerns of IoMT systems. We also discuss the solutions brought by blockchain-enabled IoMT to COVID-19 from five different perspectives. Moreover, we outline the open challenges and future directions of blockchain-enabled IoMT.", "label": 1, "source": "scigen_human", "idx": 161, "lang": "en"}
{"text": "In an article written five years ago, we described a method for predicting which scientific papers will be highly cited in the future, even if they are currently not highly cited. Applying the method to real citation data we made predictions about papers we believed would end up being well cited. Here we revisit those predictions, five years on, to see how well we did. Among the over 2000 papers in our original data set, we examine the fifty that, by the measures of our previous study, were predicted to do best and we find that they have indeed received substantially more citations in the intervening years than other papers, even after controlling for the number of prior citations. On average these top fifty papers have received 23 times as many citations in the last five years as the average paper in the data set as a whole, and 15 times as many as the average paper in a randomly drawn control group that started out with the same number of citations. Applying our prediction technique to current data, we also make new predictions of papers that we believe will be well cited in the next few years.", "label": 1, "source": "scigen_human", "idx": 162, "lang": "en"}
{"text": "The estimation of the motor torque and friction parameters are crucial for implementing an efficient low level joint torque control. In a set of coupled joints, the actuators torques are mapped to the output joint torques through a coupling matrix, such that the motor torque and friction parameters appear entangled from the point of view of the joints. As a result, their identification is problematic when using the same methodology as for single joints. This paper proposes an identification method with an improved accuracy with respect to classical closed loop methods on coupled joints. The method stands out through the following key points: it is a direct open loop identification; it addresses separately each motor in the coupling; it accounts for the static friction in the actuation elements. The identified parameters should significantly improve the contribution of the feed-forward terms in the low level control of coupled joints with static friction.", "label": 1, "source": "scigen_human", "idx": 163, "lang": "en"}
{"text": "We study the problem of estimating a p -dimensional s -sparse vector in a linear model with Gaussian design and additive noise. In the case where the labels are contaminated by at most o adversarial outliers, we prove that the 1 -penalized Huber's M -estimator based on n samples attains the optimal rate of convergence ( s n) 1 2 ( o n), up to a logarithmic factor. For more general design matrices, our results highlight the importance of two properties: the transfer principle and the incoherence property. These properties with suitable constants are shown to yield the optimal rates, up to log-factors, of robust estimation with adversarial contamination.", "label": 1, "source": "scigen_human", "idx": 164, "lang": "en"}
{"text": "We consider the problem of determining the existence of a sequence of matrices driving a discrete-time multi-agent consensus system to consensus. We transform this problem into the problem of the existence of a product of the (stochastic) transition matrices that has a positive column. This allows us to make use of results from automata theory to sets of stochastic matrices. Our main result is a polynomial-time algorithm to decide the existence of a sequence of matrices achieving consensus.", "label": 1, "source": "scigen_human", "idx": 165, "lang": "en"}
{"text": "An Intrusion Detection System (IDS) is a key cybersecurity tool for network administrators as it identifies malicious traffic and cyberattacks. With the recent successes of machine learning techniques such as deep learning, more and more IDS are now using machine learning algorithms to detect attacks faster. However, these systems lack robustness when facing previously unseen types of attacks. With the increasing number of new attacks, especially against Internet of Things devices, having a robust IDS able to spot unusual and new attacks becomes necessary. This work explores the possibility of leveraging generative adversarial models to improve the robustness of machine learning based IDS. More specifically, we propose a new method named SIGMA, that leverages adversarial examples to strengthen IDS against new types of attacks. Using Generative Adversarial Networks (GAN) and metaheuristics, SIGMA generates adversarial examples, iteratively, and uses it to retrain a machine learning-based IDS, until a convergence of the detection rate (i.e. until the detection system is not improving anymore). A round of improvement consists of a generative phase, in which we use GANs and metaheuristics to generate instances; an evaluation phase in which we calculate the detection rate of those newly generated attacks; and a training phase, in which we train the IDS with those attacks. We have evaluated the SIGMA method for four standard machine learning classification algorithms acting as IDS, with a combination of GAN and a hybrid local-search and genetic algorithm, to generate new datasets of attacks. Our results show that SIGMA can successfully generate adversarial attacks against different machine learning based IDS. Also, using SIGMA, we can improve the performance of an IDS to up to 100 after as little as two rounds of improvement.", "label": 1, "source": "scigen_human", "idx": 166, "lang": "en"}
{"text": "This paper addresses the problem of monocular 3D human shape and pose estimation from an RGB image. Despite great progress in this field in terms of pose prediction accuracy, state-of-the-art methods often predict inaccurate body shapes. We suggest that this is primarily due to the scarcity of in-the-wild training data with diverse and accurate body shape labels. Thus, we propose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system that utilises proxy representations, such as silhouettes and 2D joints, as inputs to a shape and pose regression neural network, which is trained with synthetic training data (generated on-the-fly during training using the SMPL statistical body model) to overcome data scarcity. We bridge the gap between synthetic training inputs and noisy real inputs, which are predicted by keypoint detection and segmentation CNNs at test-time, by using data augmentation and corruption during training. In order to evaluate our approach, we curate and provide a challenging evaluation dataset for monocular human shape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via multi-frame optimisation. We show that STRAPS outperforms other state-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets and metrics.", "label": 1, "source": "scigen_human", "idx": 167, "lang": "en"}
{"text": "This paper addresses the problem of designing an optimal output feedback controller with a specified controller structure for linear time-invariant (LTI) systems to maximize the passivity level for the closed-loop system, in both continuous-time (CT) and discrete-time (DT). Specifically, the set of controllers under consideration is linearly parameterized with constrained parameters. Both input feedforward passivity (IFP) and output feedback passivity (OFP) indices are used to capture the level of passivity. Given a set of stabilizing controllers, a necessary and sufficient condition is proposed for the existence of such fixed-structured output feedback controllers that can passivate the closed-loop system. Moreover, it is shown that the condition can be used to obtain the controller that maximizes the IFP or the OFP index by solving a convex optimization problem.", "label": 1, "source": "scigen_human", "idx": 168, "lang": "en"}
{"text": "In this paper, a multi-scale approach to spectrum sensing in cognitive cellular networks is proposed. In order to overcome the huge cost incurred in the acquisition of full network state information, a hierarchical scheme is proposed, based on which local state estimates are aggregated up the hierarchy to obtain aggregate state information at multiple scales, which are then sent back to each cell for local decision making. Thus, each cell obtains fine-grained estimates of the channel occupancies of nearby cells, but coarse-grained estimates of those of distant cells. The performance of the aggregation scheme is studied in terms of the trade-off between the throughput achievable by secondary users and the interference generated by the activity of these secondary users to primary users. In order to account for the irregular structure of interference patterns arising from path loss, shadowing, and blockages, which are especially relevant in millimeter wave networks, a greedy algorithm is proposed to find a multi-scale aggregation tree to optimize the performance. It is shown numerically that this tailored hierarchy outperforms a regular tree construction by 60.", "label": 1, "source": "scigen_human", "idx": 169, "lang": "en"}
{"text": "Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.", "label": 1, "source": "scigen_human", "idx": 171, "lang": "en"}
{"text": "We adapt the rectangular splitting technique of Paterson and Stockmeyer to the problem of evaluating terms in holonomic sequences that depend on a parameter. This approach allows computing the n -th term in a recurrent sequence of suitable type using O (n 1 2) \"expensive\" operations at the cost of an increased number of \"cheap\" operations. Rectangular splitting has little overhead and can perform better than either naive evaluation or asymptotically faster algorithms for ranges of n encountered in applications. As an example, fast numerical evaluation of the gamma function is investigated. Our work generalizes two previous algorithms of Smith.", "label": 1, "source": "scigen_human", "idx": 172, "lang": "en"}
{"text": "Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments - as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.", "label": 1, "source": "scigen_human", "idx": 173, "lang": "en"}
{"text": "The Full Dimension-MIMO (FD-MIMO) technology is capable of achieving huge improvements in network throughput with simultaneous connectivity of a large number of mobile wireless devices, unmanned aerial vehicles, and the Internet of Things (IoT). In FD-MIMO, with a large number of antennae at the base station and the ability to perform beamforming, the capacity of the physical downlink shared channel (PDSCH) has increased a lot. However, the current specifications of the 3 r d Generation Partnership Project (3GPP) does not allow the base station to perform beamforming techniques for the physical downlink control channel (PDCCH), and hence, PDCCH has neither the capacity nor the coverage of PDSCH. Therefore, PDCCH capacity will still limit the performance of a network as it dictates the number of users that can be scheduled at a given time instant. In Release 11, 3GPP introduced enhanced PDCCH (EPDCCH) to increase the PDCCH capacity at the cost of sacrificing the PDSCH resources. The problem of enhancing the PDCCH capacity within the available control channel resources has not been addressed yet in the literature. Hence, in this paper, we propose a novel beamformed PDCCH (BF-PDCCH) design which is aligned to the 3GPP specifications and requires simple software changes at the base station. We rely on the sounding reference signals transmitted in the uplink to decide the best beam for a user and ingeniously schedule the users in PDCCH. We perform system level simulations to evaluate the performance of the proposed design and show that the proposed BF-PDCCH achieves larger network throughput when compared with the current state of art algorithms, PDCCH and EPDCCH schemes. Keywords: Beamforming, blind decoding, control channel, rate matching, search space.", "label": 1, "source": "scigen_human", "idx": 174, "lang": "en"}
{"text": "Individual identification is essential to animal behavior and ecology research and is of significant importance for protecting endangered species. Red pandas, among the world's rarest animals, are currently identified mainly by visual inspection and microelectronic chips, which are costly and inefficient. Motivated by recent advancement in computer-vision-based animal identification, in this paper, we propose an automatic framework for identifying individual red pandas based on their face images. We implement the framework by exploring well-established deep learning models with necessary adaptation for effectively dealing with red panda images. Based on a database of red panda images constructed by ourselves, we evaluate the effectiveness of the proposed automatic individual red panda identification method. The evaluation results show the promising potential of automatically recognizing individual red pandas from their faces. We are going to release our database and model in the public domain to promote the research on automatic animal identification and particularly on the technique for protecting red pandas.", "label": 1, "source": "scigen_human", "idx": 175, "lang": "en"}
{"text": "Nowadays, ubiquitous network access has become a reality thanks to Unmanned Aerial Vehicles (UAVs) that have gained extreme popularity due to their flexible deployment and higher chance of Line-of-Sight (LoS) links to ground users. Telecommunication service providers deploy UAVs to provide flying network access in remote rural areas, disaster-affected areas or massive-attended events (sport venues, festivals, etc.) where full set-up to provide temporary wireless coverage would be very expensive. Of course, a UAV i s battery-powered which means limited energy budget for both mobility aspect and communication aspect. An efficient solution is to allow UAVs swhiching their radio modules to sleep mode in order to extend battery lifetime. This results in temporary unavailability of communication feature. Within such a situation, the ultimate deal for a UAV operator is to provide a cost effective service with acceptable availability. This would allow to meet some target Quality of Service (QoS) while having a good market share granting satisfactory benefits. In this article, we exhibit a new framework with many interesting insights on how to jointly define the availability and the access cost in UAV-empowered flying access networks to opportunistically cover a target geographical area. Yet, we construct a duopoly model to capture the adversarial behavior of service providers in terms of their pricing policies and their respective availability probabilities. Optimal periodic beaconing (small messages advertising existence of a UAV) is a vital issue that needs to be addressed, given the UAVs limited battery capacity and their recharging constraints. A full analysis of the game outcome, both in terms of equilibrium pricing and equilibrium availability, is derived. We show that the availability-pricing game exhibits some nice features as it is sub-modular with respect to the availability policy, whereas it is super-modular with respect to the service fee. Furthermore, we implement a learning scheme using best-response dynamics that allows operators to learn their joint pricing-availability strategies in a fast, accurate and distributed fashion. Extensive simulations show convergence of the proposed scheme to the joint pricing-availability equilibrium and offer promising insights on how the game parameters should be chosen to efficiently control the duopoly game. Index Terms:UAV, Coverage probability; Service probability; Cournot Duopoly; Non-cooperative Game; Pricing Game; Availability Game; Sub-Modular Game; Super-Modular Game; Nash Equilibrium; Best Response Dynamics.", "label": 1, "source": "scigen_human", "idx": 176, "lang": "en"}
{"text": "We propose a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. Inspired by the classic intrinsic image decomposition, our learning signal builds upon two insights: 1) combining the disentangled factors should reconstruct the original image, and 2) the permanent factors should stay constant across multiple temporal samples of the same scene. To facilitate training, we assemble a city-scale dataset of outdoor timelapse imagery from Google Street View, where the same locations are captured repeatedly through time. This data represents an unprecedented scale of spatio-temporal outdoor imagery. We show that our learned disentangled factors can be used to manipulate novel images in realistic ways, such as changing lighting effects and scene geometry. Please visit factorize-a-city.github.io for animated results.", "label": 1, "source": "scigen_human", "idx": 177, "lang": "en"}
{"text": "This paper proposes a vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles. Different from previous sky editing methods that either focus on static photos or require inertial measurement units integrated in smartphones on shooting videos, our method is purely vision-based, without any requirements on the capturing devices, and can be well applied to either online or offline processing scenarios. Our method runs in real-time and is free of user interactions. We decompose this artistic creation process into a couple of proxy tasks including sky matting, motion estimation, and image blending. Experiments are conducted on videos diversely captured in the wild by handheld smartphones and dash cameras, and show high fidelity and good generalization of our method in both visual quality and lightingmotion dynamics. Our code and animated results are available at .", "label": 1, "source": "scigen_human", "idx": 178, "lang": "en"}
{"text": "In this article a DNN-based system for detection of three common voice disorders (vocal nodules, polyps and cysts; laryngeal neoplasm; unilateral vocal paralysis) is presented. The input to the algorithm is (at least 3-second long) audio recording of sustained vowel sound a:. The algorithm was developed as part of the \"2018 FEMH Voice Data Challenge\" organized by Far Eastern Memorial Hospital and obtained score value (defined in the challenge specification) of 77.44. This was the second best result before final submission. Final challenge results are not yet known during writing of this document. The document also reports changes that were made for the final submission which improved the score value in cross-validation by 0.6 points.", "label": 1, "source": "scigen_human", "idx": 179, "lang": "en"}
{"text": "Can an adversary exploit model explanations to infer sensitive information about the models' training set? To investigate this question, we first focus on membership inference attacks: given a data point and a model explanation, the attacker's goal is to decide whether or not the point belongs to the training data. We study this problem for two popular transparency methods: gradient-based attribution methods and record-based influence measures. We develop membership inference attacks based on these model explanations, and extensively test them on a variety of datasets. For gradient-based methods, we show that the explanations can leak a significant amount of information about the individual data points in the training set, much beyond what is leaked through the predicted labels. We also show that record-based measures can be effectively, and even more significantly, exploited for membership inference attacks. More importantly, we design reconstruction attacks against this class of model explanations. We demonstrate that they can be exploited to recover significant parts of the training set. Finally, our results indicate that minorities and outliers are more vulnerable to these type of attacks than the rest of the population. Thus, there is a significant disparity for the privacy risks of model explanations across different groups.", "label": 1, "source": "scigen_human", "idx": 180, "lang": "en"}
{"text": "Variational Bayes (VB) is a recent approximate method for Bayesian inference. It has the merit of being a fast and scalable alternative to Markov Chain Monte Carlo (MCMC) but its approximation error is often unknown. In this paper, we derive the approximation error of VB in terms of mean, mode, variance, predictive density and KL divergence for the linear Gaussian multi-equation regression. Our results indicate that VB approximates the posterior mean perfectly. Factors affecting the magnitude of underestimation in the posterior variance and mode are revealed. Importantly, We demonstrate that VB estimates predictive densities accurately.", "label": 1, "source": "scigen_human", "idx": 181, "lang": "en"}
{"text": "Given the rise of a new approach to MT, Neural MT (NMT), and its promising performance on different text types, we assess the translation quality it can attain on what is perceived to be the greatest challenge for MT: literary text. Specifically, we target novels, arguably the most popular type of literary text. We build a literary-adapted NMT system for the English-to-Catalan translation direction and evaluate it against a system pertaining to the previous dominant paradigm in MT: statistical phrase-based MT (PBSMT). To this end, for the first time we train MT systems, both NMT and PBSMT, on large amounts of literary text (over 100 million words) and evaluate them on a set of twelve widely known novels spanning from the the 1920s to the present day. According to the BLEU automatic evaluation metric, NMT is significantly better than PBSMT ( p 0.01) on all the novels considered. Overall, NMT results in a 11 relative improvement (3 points absolute) over PBSMT. A complementary human evaluation on three of the books shows that between 17 and 34 of the translations, depending on the book, produced by NMT (versus 8 and 20 with PBSMT) are perceived by native speakers of the target language to be of equivalent quality to translations produced by a professional human translator.", "label": 1, "source": "scigen_human", "idx": 182, "lang": "en"}
{"text": "For testing goodness of fit it is very popular to use either the 2 -statistic or G 2 -statistics (information divergence). Asymptotically both are 2 -distributed so an obvious question is which of the two statistics that has a distribution that is closest to the 2 -distribution. Surprisingly, when there is only one degree of freedom it seems like the distribution of information divergence is much better approximated by a 2 -distribution than the 2 -statistic. For random variables we introduce a new transformation that transform several important distributions into new random variables that are almost Gaussian. For the binomial distributions and the Poisson distributions we formulate a general conjecture about how close their transform are to the Gaussian. The conjecture is proved for Poisson distributions.", "label": 1, "source": "scigen_human", "idx": 183, "lang": "en"}
{"text": "How are the meanings of linguistic expressions related to their use in concrete cognitive tasks? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain quantifiers. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network - a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) - on the \"most\" verification task from , manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as cognitive models of this and other psychosemantic tasks.", "label": 1, "source": "scigen_human", "idx": 184, "lang": "en"}
{"text": "We propose a computational framework for ranking images (group photos in particular) taken at the same event within a short time span. The ranking is expected to correspond with human perception of overall appeal of the images. We hypothesize and provide evidence through subjective analysis that the factors that appeal to humans are its emotional content, aesthetics and image quality. We propose a network which is an ensemble of three information channels, each predicting a score corresponding to one of the three visual appeal factors. For group emotion estimation, we propose a convolutional neural network (CNN) based architecture for predicting group emotion from images. This new architecture enforces the network to put emphasis on the important regions in the images, and achieves comparable results to the state-of-the-art. Next, we develop a network for the image ranking task that combines group emotion, aesthetics and image quality scores. Owing to the unavailability of suitable databases, we created a new database of manually annotated group photos taken during various social events. We present experimental results on this database and other benchmark databases whenever available. Overall, our experiments show that the proposed framework can reliably predict the overall appeal of images with results closely corresponding to human ranking.", "label": 1, "source": "scigen_human", "idx": 185, "lang": "en"}
{"text": "In Internet of Things (IoT) systems with security demands, there is often a need to distribute sensitive information (such as encryption keys, digital signatures, or login credentials, etc.) among the devices, so that it can be retrieved for confidential purposes at a later moment. However, this information cannot be entrusted to any one device, since the failure of that device or an attack on it will jeopardize the security of the entire network. Even if the information is divided among devices, there is still the danger that an attacker can compromise a group of devices and expose the sensitive information. In this work, we design and implement a secure and robust scheme to enable the distribution of sensitive information in IoT networks. The proposed approach has two important properties: (1) it uses Threshold Secret Sharing (TSS) to split the information into pieces distributed among all devices in the system - and so the information can only be retrieved collaboratively by groups of devices; and (2) it ensures the privacy and integrity of the information, even when attackers hijack a large number of devices and use them in concert - specifically, all the compromised devices can be identified, the confidentiality of information is kept, and authenticity of the secret can be guaranteed.", "label": 1, "source": "scigen_human", "idx": 186, "lang": "en"}
{"text": "During the Coincheck incident, which recorded the largest damages in cryptocurrency history in 2018, it was demonstrated that using Mosaic token can have a certain effect. Although it seems attractive to employ tokens as countermeasures for cryptocurrency leakage, Mosaic is a specific token for the New Economy Movement (NEM) cryptocurrency and is not employed for other blockchain systems or cryptocurrencies. Moreover, although some volunteers tracked leaked NEM using Mosaic in the CoinCheck incident, it would be better to verify that the volunteers can be trusted. Simultaneously, if someone (e.g., who stole cryptocurrencies) can identify the volunteers, then that person or organization may be targets of them. In this paper, we propose an anonymous trust-marking scheme on blockchain systems that is universally applicable to any cryptocurrency. In our scheme, entities called token admitters are allowed to generate tokens adding trustworthiness or untrustworthiness to addresses. Anyone can anonymously verify whether these tokens were issued by a token admitter. Simultaneously, only the designated auditor and no one else, including nondesignated auditors, can identify the token admitters. Our scheme is based on accountable ring signatures and commitment, and is implemented on an elliptic curve called Curve25519, and we confirm that both cryptographic tools are efficient. Moreover, we also confirm that our scheme is applicable to Bitcoin, Ethereum, and NEM.", "label": 1, "source": "scigen_human", "idx": 187, "lang": "en"}
{"text": "Unlike the traditional dock-based systems, dockless bike-sharing systems are more convenient for users in terms of flexibility. However, the flexibility of these dockless systems comes at the cost of management and operation complexity. Indeed, the imbalanced and dynamic use of bikes leads to mandatory rebalancing operations, which impose a critical need for effective bike traffic flow prediction. While efforts have been made in developing traffic flow prediction models, existing approaches lack interpretability, and thus have limited value in practical deployment. To this end, we propose an Interpretable Bike Flow Prediction (IBFP) framework, which can provide effective bike flow prediction with interpretable traffic patterns. Specifically, by dividing the urban area into regions according to flow density, we first model the spatio-temporal bike flows between regions with graph regularized sparse representation, where graph Laplacian is used as a smooth operator to preserve the commonalities of the periodic data structure. Then, we extract traffic patterns from bike flows using subspace clustering with sparse representation to construct interpretable base matrices. Moreover, the bike flows can be predicted with the interpretable base matrices and learned parameters. Finally, experimental results on real-world data show the advantages of the IBFP method for flow prediction in dockless bike sharing systems. In addition, the interpretability of our flow pattern exploitation is further illustrated through a case study where IBFP provides valuable insights into bike flow analysis.", "label": 1, "source": "scigen_human", "idx": 188, "lang": "en"}
{"text": "Let 1 p . In this paper, we consider solving a nonlinear functional equation f (x) y , where x , y belong to p and f has continuous bounded gradient in an inverse-closed subalgebra of B ( 2), the Banach algebra of all bounded linear operators on the Hilbert space 2 . We introduce strict monotonicity property for functions f on Banach spaces p so that the above nonlinear functional equation is solvable and the solution x depends continuously on the given data y in p . We show that the Van-Cittert iteration converges in p with exponential rate and hence it could be used to locate the true solution of the above nonlinear functional equation. We apply the above theory to handle two problems in signal processing: nonlinear sampling termed with instantaneous companding and subsequently average sampling; and local identification of innovation positions and qualification of amplitudes of signals with finite rate of innovation.", "label": 1, "source": "scigen_human", "idx": 189, "lang": "en"}
{"text": "The dynamic complexity of the reachability query is studied in the dynamic complexity framework of Patnaik and Immerman, restricted to quantifier-free update formulas. It is shown that, with this restriction, the reachability query cannot be dynamically maintained, neither with binary auxiliary relations nor with unary auxiliary functions, and that ternary auxiliary relations are more powerful with respect to graph queries than binary auxiliary relations. Further inexpressibility results are given for the reachability query in a different setting as well as for a syntactical restriction of quantifier-free update formulas. Moreover inexpressibility results for some other queries are presented.", "label": 1, "source": "scigen_human", "idx": 190, "lang": "en"}
{"text": "Simulating dynamic rupture propagation is challenging due to the uncertainties involved in the underlying physics of fault slip, stress conditions, and frictional properties of the fault. A trial and error approach is often used to determine the unknown parameters describing rupture, but running many simulations usually requires human review to determine how to adjust parameter values and is thus not very efficient. To reduce the computational cost and improve our ability to determine reasonable stress and friction parameters, we take advantage of the machine learning approach. We develop two models for earthquake rupture propagation using the artificial neural network (ANN) and the random forest (RF) algorithms to predict if a rupture can break a geometric heterogeneity on a fault. We train the models using a database of 1600 dynamic rupture simulations computed numerically. Fault geometry, stress conditions, and friction parameters vary in each simulation. We cross-validate and test the predictive power of the models using an additional 400 simulated ruptures, respectively. Both RF and ANN models predict rupture propagation with more than 81 accuracy and model parameters can be used to infer the underlying factors most important for rupture propagation. Both of the models are computationally efficient such that the 400 testings require a fraction of a second, leading to potential applications of dynamic rupture that have previously not been possible due to the computational demands of physics-based rupture simulations.", "label": 1, "source": "scigen_human", "idx": 191, "lang": "en"}
{"text": "Accurate mobile traffic forecast is important for efficient network planning and operations. However, existing traffic forecasting models have high complexity, making the forecasting process slow and costly. In this paper, we analyze some characteristics of mobile traffic such as periodicity, spatial similarity and short term relativity. Based on these characteristics, we propose a Block Regression (BR) model for mobile traffic forecasting. This model employs seasonal differentiation so as to take into account of the temporally repetitive nature of mobile traffic. One of the key features of our BR model lies in its low complexity since it constructs a single model for all base stations. We evaluate the accuracy of BR model based on real traffic data and compare it with the existing models. Results show that our BR model offers equal accuracy to the existing models but has much less complexity.", "label": 1, "source": "scigen_human", "idx": 192, "lang": "en"}
{"text": "Deep convolutional neural networks have demonstrated promising performance on image classification tasks, but the manual design process becomes more and more complex due to the fast depth growth and the increasingly complex topologies of convolutional neural networks. As a result, neural architecture search has emerged to automatically design convolutional neural networks that outperform handcrafted counterparts. However, the computational cost is immense, e.g. 22,400 GPU-days and 2,000 GPU-days for two outstanding neural architecture search works named NAS and NASNet, respectively, which motivates this work. A new effective and efficient surrogate-assisted particle swarm optimisation algorithm is proposed to automatically evolve convolutional neural networks. This is achieved by proposing a novel surrogate model, a new method of creating a surrogate dataset and a new encoding strategy to encode variable-length blocks of convolutional neural networks, all of which are integrated into a particle swarm optimisation algorithm to form the proposed method. The proposed method shows its effectiveness by achieving competitive error rates of 3.49 on the CIFAR-10 dataset, 18.49 on the CIFAR-100 dataset, and 1.82 on the SVHN dataset. The convolutional neural network blocks are efficiently learned by the proposed method from CIFAR-10 within 3 GPU-days due to the acceleration achieved by the surrogate model and the surrogate dataset to avoid the training of 80.1 of convolutional neural network blocks represented by the particles. Without any further search, the evolved blocks from CIFAR-10 can be successfully transferred to CIFAR-100 and SVHN, which exhibits the transferability of the block learned by the proposed method.", "label": 1, "source": "scigen_human", "idx": 193, "lang": "en"}
{"text": "For fear of retribution, the victim of a crime may be willing to report it only if other victims of the same perpetrator also step forward. Common examples include 1) identifying oneself as the victim of sexual harassment, especially by a person in a position of authority or 2) accusing an influential politician, an authoritarian government, or ones own employer of corruption. To handle such situations, legal literature has proposed the concept of an allegation escrow: a neutral third-party that collects allegations anonymously, matches them against each other, and de-anonymizes allegers only after de-anonymity thresholds (in terms of number of co-allegers), pre-specified by the allegers, are reached. An allegation escrow can be realized as a single trusted third party; however, this party must be trusted to keep the identity of the alleger and content of the allegation private. To address this problem, this paper introduces Secure Allegation Escrows (SAE, pronounced \"say. A SAE is a group of parties with independent interests and motives, acting jointly as an escrow for collecting allegations from individuals, matching the allegations, and de-anonymizing the allegations when designated thresholds are reached. By design, SAEs provide a very strong property: No less than a majority of parties constituting a SAE can de-anonymize or disclose the content of an allegation without a sufficient number of matching allegations (even in collusion with any number of other allegers). Once a sufficient number of matching allegations exist, the join escrow discloses the allegation with the allegers' identities. We describe how SAEs can be constructed using a novel authentication protocol and a novel allegation matching and bucketing algorithm, provide formal proofs of the security of our constructions, and evaluate a prototype implementation, demonstrating feasibility in practice.", "label": 1, "source": "scigen_human", "idx": 194, "lang": "en"}
{"text": "The technology in the area of automated vehicles is gaining speed and promises many advantages. However, with the recent introduction of conditionally automated driving, we have also seen accidents. Test protocols for both, conditionally automated (e.g., on highways) and automated vehicles do not exist yet and leave researchers and practitioners with different challenges. For instance, current test procedures do not suffice for fully automated vehicles, which are supposed to be completely in charge for the driving task and have no driver as a back up. This paper presents current challenges of testing the functionality and safety of automated vehicles derived from conducting focus groups and interviews with 26 participants from five countries having a background related to testing automotive safety-related topics. We provide an overview of the state-of-practice of testing active safety features as well as challenges that needs to be addressed in the future to ensure safety for automated vehicles. The major challenges identified through the interviews and focus groups, enriched by literature on this topic are related to 1) virtual testing and simulation, 2) safety, reliability, and quality, 3) sensors and sensor models, 4) required scenario complexity and amount of test cases, and 5) handover of responsibility between the driver and the vehicle.", "label": 1, "source": "scigen_human", "idx": 195, "lang": "en"}
{"text": "Attention is an increasingly popular mechanism used in a wide range of neural architectures. Because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures for natural language processing, with a focus on architectures designed to work with vector representation of the textual data. We discuss the dimensions along which proposals differ, the possible uses of attention, and chart the major research activities and open challenges in the area.", "label": 1, "source": "scigen_human", "idx": 196, "lang": "en"}
{"text": "Automatic cell segmentation in microscopy images works well with the support of deep neural networks trained with full supervision. Collecting and annotating images, though, is not a sustainable solution for every new microscopy database and cell type. Instead, we assume that we can access a plethora of annotated image data sets from different domains (sources) and a limited number of annotated image data sets from the domain of interest (target), where each domain denotes not only different image appearance but also a different type of cell segmentation problem. We pose this problem as meta-learning where the goal is to learn a generic and adaptable few-shot learning model from the available source domain data sets and cell segmentation tasks. The model can be afterwards fine-tuned on the few annotated images of the target domain that contains different image appearance and different cell type. In our meta-learning training, we propose the combination of three objective functions to segment the cells, move the segmentation results away from the classification boundary using cross-domain tasks, and learn an invariant representation between tasks of the source domains. Our experiments on five public databases show promising results from 1- to 10-shot meta-learning using standard segmentation neural network architectures.", "label": 1, "source": "scigen_human", "idx": 197, "lang": "en"}
{"text": "With the ratification of the IEEE 802.15.3d amendment to the 802.15.3, a first step has been made to standardize consumer wireless communications in the sub-THz frequency band. The IEEE 802.15.3d offers switched point-to-point connectivity with the data rates of 100Gbits and higher at distances ranging from tens of centimeters up to a few hundred meters. In this article, we provide a detailed introduction to the IEEE 802.15.3d and the key design principles beyond the developed standard. We particularly describe the target applications and usage scenarios, as well as the specifics of the IEEE 802.15.3d physical and medium access layers. Later, we present the results of the initial performance evaluation of IEEE 802.15.3d wireless communications. The obtained first-order performance predictions show non-incremental benefits compared to the characteristics of the fifth-generation wireless systems, thus paving the way towards the six-generation (6G) THz networks. We conclude the article by outlining the further standardization and regulatory activities on wireless networking in the THz frequency band.", "label": 1, "source": "scigen_human", "idx": 198, "lang": "en"}
{"text": "We study three-way joins on MapReduce. Joins are very useful in a multitude of applications from data integration and traversing social networks, to mining graphs and automata-based constructions. However, joins are expensive, even for moderate data sets; we need efficient algorithms to perform distributed computation of joins using clusters of many machines. MapReduce has become an increasingly popular distributed computing system and programming paradigm. We consider a state-of-the-art MapReduce multi-way join algorithm by Afrati and Ullman and show when it is appropriate for use on very large data sets. By providing a detailed experimental study, we demonstrate that this algorithm scales much better than what is suggested by the original paper. However, if the join result needs to be summarized or aggregated, as opposed to being only enumerated, then the aggregation step can be integrated into a cascade of two-way joins, making it more efficient than the other algorithm, and thus becomes the preferred solution.", "label": 1, "source": "scigen_human", "idx": 199, "lang": "en"}
{"text": "Advertising is a primary means for revenue generation for millions of websites and smartphone apps. Naturally, a fraction abuse ad networks to systematically defraud advertisers of their money. Modern defences have matured to overcome some forms of click fraud but measurement studies have reported that a third of clicks supplied by ad networks could be clickspam. Our work develops novel inference techniques which can isolate click fraud attacks using their fundamental properties. We propose two defences, mimicry and bait-click , which provide clickspam detection with substantially improved results over current approaches. Mimicry leverages the observation that organic clickfraud involves the reuse of legitimate click traffic, and thus isolates clickspam by detecting patterns of click reuse within ad network clickstreams. The bait-click defence leverages the vantage point of an ad network to inject a pattern of bait clicks into a user's device. Any organic clickspam generated involving the bait clicks will be subsequently recognisable by the ad network. Our experiments show that the mimicry defence detects around 81 of fake clicks in stealthy (low rate) attacks, with a false-positive rate of 110 per hundred thousand clicks. Similarly, the bait-click defence enables further improvements in detection, with rates of 95 and a reduction in false-positive rates of between 0 and 30 clicks per million - a substantial improvement over current approaches.", "label": 1, "source": "scigen_human", "idx": 200, "lang": "en"}
{"text": "Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Te mporal M essage P assing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments 1 footnote 1 1 footnote 1 Code and data are published at on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7 average relative improvement in Hits10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.", "label": 1, "source": "scigen_human", "idx": 201, "lang": "en"}
{"text": "Separating a singing voice from its music accompaniment remains an important challenge in the field of music information retrieval. We present a unique neural network approach inspired by a technique that has revolutionized the field of vision: pixel-wise image classification, which we combine with cross entropy loss and pretraining of the CNN as an autoencoder on singing voice spectrograms. The pixel-wise classification technique directly estimates the sound source label for each time-frequency (T-F) bin in our spectrogram image, thus eliminating common pre- and postprocessing tasks. The proposed network is trained by using the Ideal Binary Mask (IBM) as the target output label. The IBM identifies the dominant sound source in each T-F bin of the magnitude spectrogram of a mixture signal, by considering each T-F bin as a pixel with a multi-label (for each sound source). Cross entropy is used as the training objective, so as to minimize the average probability error between the target and predicted label for each pixel. By treating the singing voice separation problem as a pixel-wise classification task, we additionally eliminate one of the commonly used, yet not easy to comprehend, postprocessing steps: the Wiener filter postprocessing. The proposed CNN outperforms the first runner up in the Music Information Retrieval Evaluation eXchange (MIREX) 2016 and the winner of MIREX 2014 with a gain of 2.2702 5.9563 dB global normalized source to distortion ratio (GNSDR) when applied to the iKala dataset. An experiment with the DSD100 dataset on the full-tracks song evaluation task also shows that our model is able to compete with cutting-edge singing voice separation systems which use multi-channel modeling, data augmentation, and model blending.", "label": 1, "source": "scigen_human", "idx": 202, "lang": "en"}
{"text": "Dense 3D shape acquisition of swimming human or live fish is an important research topic for sports, biological science and so on. For this purpose, active stereo sensor is usually used in the air, however it cannot be applied to the underwater environment because of refraction, strong light attenuation and severe interference of bubbles. Passive stereo is a simple solution for capturing dynamic scenes at underwater environment, however the shape with textureless surfaces or irregular reflections cannot be recovered. Recently, the stereo camera pair with a pattern projector for adding artificial textures on the objects is proposed. However, to use the system for underwater environment, several problems should be compensated, i.e. , disturbance by fluctuation and bubbles. Simple solution is to use convolutional neural network for stereo to cancel the effects of bubbles andor water fluctuation. Since it is not easy to train CNN with small size of database with large variation, we develop a special bubble generation device to efficiently create real bubble database of multiple size and density. In addition, we propose a transfer learning technique for multi-scale CNN to effectively remove bubbles and projected-patterns on the object. Further, we develop a real system and actually captured live swimming human, which has not been done before. Experiments are conducted to show the effectiveness of our method compared with the state of the art techniques.", "label": 1, "source": "scigen_human", "idx": 203, "lang": "en"}
{"text": "One key use of k-means clustering is to identify cluster prototypes which can serve as representative points for a dataset. However, a drawback of using k-means cluster centers as representative points is that such points distort the distribution of the underlying data. This can be highly disadvantageous in problems where the representative points are subsequently used to gain insights on the data distribution, as these points do not mimic the distribution of the data. To this end, we propose a new clustering method called \"distributional clustering,\" which ensures cluster centers capture the distribution of the underlying data. We first prove the asymptotic convergence of the proposed cluster centers to the data generating distribution, then present an efficient algorithm for computing these cluster centers in practice. Finally, we demonstrate the effectiveness of distributional clustering on synthetic and real datasets.", "label": 1, "source": "scigen_human", "idx": 204, "lang": "en"}
{"text": "An important problem on graph-structured data is that of quantifying similarity between graphs. Graph kernels are an established technique for such tasks; in particular, those based on random walks and return probabilities have proven to be effective in wide-ranging applications, from bioinformatics to social networks to computer vision. However, random walk kernels generally suffer from slowness and tottering , an effect which causes walks to overemphasize local graph topology, undercutting the importance of global structure. To correct for these issues, we recast return probability graph kernels under the more general framework of density of states - a framework which uses the lens of spectral analysis to uncover graph motifs and properties hidden within the interior of the spectrum - and use our interpretation to construct scalable, composite density of states based graph kernels which balance local and global information, leading to higher classification accuracies on a host of benchmark datasets.", "label": 1, "source": "scigen_human", "idx": 205, "lang": "en"}
{"text": "Generating training examples for supervised tasks is a long sought after goal in AI. We study the problem of heart signal electrocardiogram (ECG) synthesis for improved heartbeat classification. ECG synthesis is challenging: the generation of training examples for such biological-physiological systems is not straightforward, due to their dynamic nature in which the various parts of the system interact in complex ways. However, an understanding of these dynamics has been developed for years in the form of mathematical process simulators. We study how to incorporate this knowledge into the generative process by leveraging a biological simulator for the task of ECG classification. Specifically, we use a system of ordinary differential equations (ODE) representing heart dynamics, and incorporate this ODE system into the optimization process of a generative adversarial network to create biologically plausible ECG training examples. We perform empirical evaluation and show that heart simulation knowledge during the generation process improves ECG classification.", "label": 1, "source": "scigen_human", "idx": 206, "lang": "en"}
{"text": "We study a constrained contextual linear bandit setting, where the goal of the agent is to produce a sequence of policies, whose expected cumulative reward over the course of T rounds is maximum, and each has an expected cost below a certain threshold . We propose an upper-confidence bound algorithm for this problem, called optimistic pessimistic linear bandit (OPLB), and prove an O ( d T - c 0) bound on its T -round regret, where the denominator is the difference between the constraint threshold and the cost of a known feasible action. We further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting. We prove a regret bound of O ( K T - c 0) for this algorithm in K -armed bandits, which is a K improvement over the regret bound we obtain by simply casting multi-armed bandits as an instance of contextual linear bandits and using the regret bound of OPLB. We also prove a lower-bound for the problem studied in the paper and provide simulations to validate our theoretical results.", "label": 1, "source": "scigen_human", "idx": 207, "lang": "en"}
{"text": "When analyzing the statistical and topological characteristics of complex networks, an effective and convenient way is to compute the centralities for recognizing influential and significant nodes or structures, yet most of them are restricted to local environment or some specific configurations. In this paper we propose a new centrality for nodes based on the von Neumann entropy, which allows us to investigate the importance of nodes in the view of spectrum eigenvalues distribution. By presenting the performances of this centrality with network examples in reality, it is shown that the von Neumann entropy node centrality is an excellent index for selecting crucial nodes as well as classical ones. Then to lower down the computational complexity, an approximation calculation to this centrality is given which only depends on its first and second neighbors. Furthermore, in the optimal spreader problem and reducing average clustering coefficients, this entropy centrality presents excellent efficiency and unveil topological structure features of networks accurately. The entropy centrality could reduce the scales of giant connected components fastly in Erdos-Renyi and scale-free networks, and break down the cluster structures efficiently in random geometric graphs. This new methodology reveals the node importance in the perspective of spectrum, which provides a new insight into networks research and performs great potentials to discover essential structural features in networks.", "label": 1, "source": "scigen_human", "idx": 208, "lang": "en"}
{"text": "We study approximation algorithms for variants of the median string problem, which asks for a string that minimizes the sum of edit distances from a given set of m strings of length n . Only the straightforward 2 -approximation is known for this NP-hard problem. This problem is motivated e.g. by computational biology, and belongs to the class of median problems (over different metric spaces), which are fundamental tasks in data analysis. Our main result is for the Ulam metric, where all strings are permutations over [ n ] and each edit operation moves a symbol (deletion plus insertion). We devise for this problem an algorithms that breaks the 2 -approximation barrier, i.e., computes a 2 ) -approximate median permutation for some constant 0 in time O ( n m 2 n 3). We further use these techniques to achieve a 2 ) approximation for the median string problem in the special case where the median is restricted to length n and the optimal objective is large ( m n). We also design an approximation algorithm for the following probabilistic model of the Ulam median: the input consists of m perturbations of an (unknown) permutation x , each generated by moving every symbol to a random position with probability (a parameter) 0 . Our algorithm computes with high probability a ( 1 o ( 1 -approximate median permutation in time O ( m n 2 n 3).", "label": 1, "source": "scigen_human", "idx": 209, "lang": "en"}
{"text": "Predicting which patients are more likely to be readmitted to a hospital within 30 days after discharge is a valuable piece of information in clinical decision-making. Building a successful readmission risk classifier based on the content of Electronic Health Records (EHRs) has proved, however, to be a challenging task. Previously explored features include mainly structured information, such as sociodemographic data, comorbidity codes and physiological variables. In this paper we assess incorporating additional clinically interpretable NLP-based features such as topic extraction and clinical sentiment analysis to predict early readmission risk in psychiatry patients.", "label": 1, "source": "scigen_human", "idx": 210, "lang": "en"}
{"text": "Mobility on Demand (MoD) services, like Uber and Lyft, are revolutionizing the way people move in cities around the world and are often considered a convenient alternative to public transit, since they offer higher Quality of Service (QoS - less waiting time, door-to-door service) at a cheap price. In the next decades, these advantages are expected to be further amplified by Automated MoD (AMoD), in which drivers will be replaced by automated vehicles, with a big gain in terms of cost-efficiency. MoD is usually intended as a door-to-door service. However, there has been recent interest toward consolidating, e.g., aggregating, the travel demand by limiting the number of admitted stop locations. This implies users have to walk fromto their intended origindestination. The contribution of this paper is a systematic study the impact of consolidation on the operator cost and on user QoS. We introduce a MoD system where pick-ups and drop-offs can only occur in a limited subset of admitted stop locations. The density of such locations is a system parameter: the less the density, the more the user demand is consolidated. We show that, by decreasing stop density, we can increase system capacity (number of passengers we are able to serve). On the contrary, increasing it, we can improve QoS. The system is tested in AMoDSim, an open-source simulator. The code to reproduce the results presented here is available on-line. This work is a first step toward flexible mobility services that are able to autonomously re-configure themselves, favoring capacity or QoS, depending on the amount of travel demand coming from users. In other words, the services we envisage in this work shift their operational mode to any intermediate point in the range from a taxi-like door-to-door service to a bus-like service, with few served stops and more passengers on-board.", "label": 1, "source": "scigen_human", "idx": 211, "lang": "en"}
{"text": "Motivation. Diffusion-based network models are widely used for protein function prediction using protein network data and have been shown to outperform neighborhood-based and module-based methods. Recent studies have shown that integrating the hierarchical structure of the Gene Ontology (GO) data dramatically improves prediction accuracy. However, previous methods usually either used the GO hierarchy to refine the prediction results of multiple classifiers, or flattened the hierarchy into a function-function similarity kernel. No study has taken the GO hierarchy into account together with the protein network as a two-layer network model. Results. We first construct a Bi-relational graph (Birg) model comprised of both protein-protein association and function-function hierarchical networks. We then propose two diffusion-based methods, BirgRank and AptRank, both of which use PageRank to diffuse information on this two-layer graph model. BirgRank is a direct application of traditional PageRank with fixed decay parameters. In contrast, AptRank utilizes an adaptive diffusion mechanism to improve the performance of BirgRank. We evaluate the ability of both methods to predict protein function on yeast, fly, and human protein datasets, and compare with four previous methods: GeneMANIA, TMC, ProteinRank and clusDCA. We design three different validation strategies: missing function prediction, de novo function prediction, and guided function prediction to comprehensively evaluate predictability of all six methods. We find that both BirgRank and AptRank outperform the previous methods, especially in missing function prediction when using only 10 of the data for training. Conclusion. AptRank naturally combines protein-protein associations and the GO function-function hierarchy into a two-layer network model without flattening the hierarchy into a similarity kernel. Introducing an adaptive mechanism to the traditional, fixed-parameter model of PageRank greatly improves the accuracy of protein function prediction. Code. . Contact.", "label": 1, "source": "scigen_human", "idx": 212, "lang": "en"}
{"text": "Drawing an analogy with automatic image completion systems, we propose Music SketchNet, a neural network framework that allows users to specify partial musical ideas guiding automatic music generation. We focus on generating the missing measures in incomplete monophonic musical pieces, conditioned on surrounding context, and optionally guided by user-specified pitch and rhythm snippets. First, we introduce SketchVAE, a novel variational autoencoder that explicitly factorizes rhythm and pitch contour to form the basis of our proposed model. Then we introduce two discriminative architectures, SketchInpainter and SketchConnector, that in conjunction perform the guided music completion, filling in representations for the missing measures conditioned on surrounding context and user-specified snippets. We evaluate SketchNet on a standard dataset of Irish folk music and compare with models from recent works. When used for music completion, our approach outperforms the state-of-the-art both in terms of objective metrics and subjective listening tests. Finally, we demonstrate that our model can successfully incorporate user-specified snippets during the generation process.", "label": 1, "source": "scigen_human", "idx": 213, "lang": "en"}
{"text": "In this paper, we investigate the encoding circuit size of Hamming codes and Hadamard codes. To begin with, we prove the exact lower bound of circuit size required in the encoding of (punctured) Hadamard codes and (extended) Hamming codes. Then the encoding algorithms for (punctured) Hadamard codes are presented to achieve the derived lower bounds. For (extended) Hamming codes, we also propose encoding algorithms that achieve the lower bounds.", "label": 1, "source": "scigen_human", "idx": 214, "lang": "en"}
{"text": "Many photography websites such as Flickr, 500px, Unsplash, and Adobe Behance are used by amateur and professional photography enthusiasts. Unlike content-based image search, such users of photography websites are not just looking for photos with certain content, but more generally for photos with a certain photographic \"aesthetic.\" In this context, we explore personalized photo recommendation and propose two aesthetic feature extraction methods based on (i) color space and (ii) deep style transfer embeddings. Using a dataset from 500px, we evaluate how these features can be best leveraged by collaborative filtering methods and show that (ii) provides a significant boost in photo recommendation performance.", "label": 1, "source": "scigen_human", "idx": 216, "lang": "en"}
{"text": "We present SmartLoc , a localization system to estimate the location and the traveling distance by leveraging the lower-power inertial sensors embedded in smartphones as a supplementary to GPS. To minimize the negative impact of sensor noises, SmartLoc exploits the intermittent strong GPS signals and uses the linear regression to build a prediction model which is based on the trace estimated from inertial sensors and the one computed from the GPS. Furthermore, we utilize landmarks (e.g. , bridge, traffic lights) detected automatically and special driving patterns (e.g. , turning, uphill, and downhill) from inertial sensory data to improve the localization accuracy when the GPS signal is weak. Our evaluations of SmartLoc in the city demonstrates its technique viability and significant localization accuracy improvement compared with GPS and other approaches: the error is approximately 20 m for 90 of time while the known mean error of GPS is 42.22 m.", "label": 1, "source": "scigen_human", "idx": 217, "lang": "en"}
{"text": "我们考虑使用从漫射壁反射的光对物体进行非视距（NLOS）成像。墙壁散射入射光，使得透镜不再用于形成图像。相反，我们利用4D空间相干函数来重建被遮挡物体的2D投影。该方法是完全被动的，因为假设不控制照射物体的光，并且与室内和室外环境中普遍存在的部分相干场兼容。我们提出了一个用于重建的多准则凸优化问题，该问题融合了不同尺度上反射场的强度和空间相干信息。我们的公式利用了已建立的光传播和散射的光学模型，并利用了不同基底中许多图像的共同稀疏性。我们还开发了一种基于乘法器交替方向法的算法来有效地求解所提出的凸规划。提供了一种用于分析测量矩阵的零空间的装置以及一种用于加权各个测量对重建的贡献的装置。本文有望在具有挑战性的非直瞄系统中推进被动成像，在这种系统中，强度不一定保持可区分的特征，并为有效的场景重建提供了多模态信息融合的框架。", "label": 1, "source": "scigen_human", "idx": 250, "lang": "zh"}
{"text": "面向服务将应用程序视为独立软件服务的编排，这些服务（1）实现可在许多应用程序中重复使用的功能，（2）可以远程调用，（3）打包以将潜在调用方与其实现技术解耦。因此，它使组织能够比没有服务更快地开发高质量的应用程序。传统应用程序不是面向服务的。然而，它们实现了许多可重用的功能，这些功能可以作为服务公开。当重新设计遗留应用程序以（重新）使用服务时，组织面临三个主要问题：（1）挖掘现有应用程序中可成为服务的可重用功能，（2）将这些功能打包到服务中，以及（3）重构遗留应用程序，以调用这些服务，从而简化未来的维护。在本文中，我们探讨了这三个问题，并提出了解决这些问题的研究方向。出于几个原因，我们选择将重点放在最近遗留的面向对象应用程序的面向服务的重新设计上，更具体地说，放在JEE应用程序上。首先，我们希望专注于架构挑战，因此我们选择不必处理源系统和目标系统之间的编程语言差异。我们特别选择了JEE应用程序，因为它们体现了在最近的遗留应用程序中可能遇到的一系列复杂性，即多语言系统、多层应用程序、对外部配置文件的依赖，以及在运行时对框架和容器服务的依赖。这些特点对上述三个问题提出了独特的挑战。", "label": 1, "source": "scigen_human", "idx": 251, "lang": "zh"}
{"text": "在回答一个问题时，除了特定的背景之外，人们还经常利用他们丰富的世界知识。最近的工作主要集中在回答一些相关文件或背景下的问题，几乎不需要一般背景。为了研究具有先验知识的问答，我们提出了CommonsenseQA：一个具有挑战性的常识问答新数据集。为了捕捉超越关联的常识，我们从ConceptNet中提取了多个与单个源概念具有相同语义关系的目标概念。人群工作者被要求提出多项选择题，这些选择题提到源概念，并依次区分每个目标概念。这鼓励员工创建具有复杂语义的问题，这些问题通常需要先验知识。我们通过这个程序创建了12247个问题，并用大量强大的基线来展示我们任务的难度。我们的最佳基线是基于大BERT，获得56的准确度，远低于人类表现89。", "label": 1, "source": "scigen_human", "idx": 252, "lang": "zh"}
{"text": "深度神经网络广泛应用于各个领域。然而，深度神经网络所需的大量数据收集揭示了潜在的隐私问题，也消耗了大量的通信带宽。为了解决这个问题，我们为联合学习分布式系统提出了一种隐私保护方法，该方法在Intel Software Guard Extensions上运行，这是一组提高应用程序代码和数据安全性的指令。同时，加密模型使传输开销更大。因此，我们通过稀疏化降低了换向成本，并在不同的模型架构下实现了合理的精度。在我们的隐私保护框架下的实验结果表明，对于LeNet-5，我们在IID数据上获得了98.78的准确度，在非IID数据中获得了97.60的准确度。节省了34.85的通信时间，总运行时间加速了1.8。对于MobileNetV2，我们在IID数据上获得了85.40的准确度，在非IID数据中获得了81.66的准确度——节省了15.85的通信，总运行时间加速为1.2。", "label": 1, "source": "scigen_human", "idx": 253, "lang": "zh"}
{"text": "人声分离是音乐信息检索中的一个基本问题，它试图将音乐录音中的声乐部分和器乐部分分离。近年来对歌声分离的研究表明，低秩表示和知情分离方法都能提高分离质量。然而，由于使用奇异值分解，低秩优化在计算上效率低下。因此，在本文中，我们提出了一种新的线性时间算法，称为知情群稀疏表示，并将其用于使用音高注释作为辅助信息来将人声与音乐分离。iKala数据集上的实验结果证实了我们方法的有效性，表明在预先训练的乐器词典的情况下，音乐伴奏遵循组稀疏结构。我们还展示了如何使用DSD100数据集轻松扩展我们的工作以容纳多个词典。", "label": 1, "source": "scigen_human", "idx": 254, "lang": "zh"}
{"text": "在这项工作中，我们提出了一种基于两种新方法的人脸对齐管道：K聚类回归森林的加权分割和人脸形状初始化的三维仿射姿态回归。我们的人脸对齐方法基于局部二元特征框架，其中，我们使用具有加权分裂（KRFWS）和金字塔HOG特征的K-cluster回归森林，而不是原始方法中使用的标准回归森林和像素差特征。我们还使用KRFWS来执行仿射姿势回归（APR）和三维仿射姿势回归，以改进人脸形状的初始化。APR对初始面形状应用刚性2D变换，以补偿初始面位置、尺寸和平面内旋转的不精确性。3D-APR估计额外补偿平面外旋转的3D变换的参数。由此产生的管道，包括APR和3D-APR，然后是人脸对齐，在具有挑战性的IBUG数据集上比标准LBF提高了20，在整个300-W数据集上显示了最先进的精度。", "label": 1, "source": "scigen_human", "idx": 255, "lang": "zh"}
{"text": "今天，无人机携带的毫米波接入点（AP）的按需部署被认为是提高5G网络性能的潜在解决方案。然而，现代无人机的电池寿命限制了此类系统的飞行时间。在这封信中，我们评估了在用户需求高度波动的地区临时提高容量的可行部署方案。该方法是将基于无人机的毫米波AP降落在附近的建筑物上，而不是在该地区上空盘旋。在开发的数学框架内，我们通过考虑所用无人机的整个运行周期，比较了机载和着陆部署的系统级性能。我们的数值结果表明，无人机部署方案的选择是由服务区和无人机充电站之间的间隔距离、无人机电池寿命和使用的空中AP数量的相互作用决定的。所提出的方法和结果可以支持在未来的5G网络中高效按需部署基于无人机的毫米波接入点。", "label": 1, "source": "scigen_human", "idx": 256, "lang": "zh"}
{"text": "Chimera图定义了最早商用量子计算机之一的拓扑结构。各种优化问题已映射到该拓扑结构，以评估量子增强优化启发式相对于其他优化器的行为，从而能够有效地解决经典问题，将其用作量子机器的基准。在本文中，我们首次研究了进化算法（EA）在Chimera拓扑上定义的Ising自旋玻璃实例上的使用。在由Sidon集合构建的Ising自旋玻璃的1000个硬实例上，评估了三种遗传算法（GA）和三种分布估计算法（EDA）。我们专注于确定关于图的拓扑结构的信息是否可以用于改进EA的结果，并确定影响GA和EDA成功率的Ising实例的特征。", "label": 1, "source": "scigen_human", "idx": 257, "lang": "zh"}
{"text": "深度神经网络（DNN）已被证明容易受到对抗性攻击，其中，模型通过对输入施加轻微扰动而被愚弄。随着物联网的出现，以及在手机、智能手表等嵌入式设备中实现智能的必要性，DNN的低功耗和安全硬件实现至关重要。在本文中，我们研究了使用量化来潜在地抵抗对抗性攻击。最近的几项研究报道了通过量化降低DNN的能量需求的显著结果。然而，先前没有任何工作考虑DNN的对抗敏感性与其对量化的影响之间的关系。我们提出了QUANOS——一种基于对抗性噪声敏感性（ANS）执行层特定混合量化的框架。我们为DNN确定了一种新的噪声稳定性度量（ANS），即每层计算对对抗性噪声的敏感性。ANS允许一种原则性的方法来确定每层的最佳比特宽度，该方法具有对抗性的鲁棒性以及能量效率，同时精度损失最小。本质上，QUANOS根据其对对抗性扰动的贡献来分配层重要性，并相应地缩放层的精度。QUANOS的一个关键优势是它不依赖于预先训练的模型，并且可以应用于训练的初始阶段。我们评估了QUANOS在具有数据门控和子字并行能力的精确可扩展乘法和累加（MAC）硬件架构上的优势。我们在CIFAR10和CIFAR100数据集上的实验表明，QUANOS在对抗性鲁棒性方面优于均匀量化的8位精度基线（高3.4），同时在等精度下产生改进的压缩（5）和节能（2）。在等压缩率下，QUANOS对强白盒攻击的对抗性鲁棒性（10）明显高于类似大小的基线。我们还发现，将QUANOS与最先进的防御方法相结合，在对抗非常强的攻击时，其鲁棒性优于最先进的方法（高-5 16）。", "label": 1, "source": "scigen_human", "idx": 258, "lang": "zh"}
{"text": "场景图旨在忠实地揭示人类对图像内容的感知。当人们分析场景时，通常倾向于首先描述图像的要点，即场景图中的主要对象和关键关系。这种人类固有的感知习惯意味着在场景解析过程中，人类的偏好存在一种层次结构。因此，我们认为还应该分层构建一个理想的场景图，并引入一种新的场景图建模方案。具体地说，场景由一系列图像区域组成的仿人层次实体树（HET）表示。为了生成基于HET的场景图，我们使用混合长短期存储器（Hybrid LSTM）解析HET，该存储器专门对层次结构和兄弟上下文进行编码，以捕获嵌入在HET中的结构化信息。为了进一步区分场景图中的关键关系的优先级，我们设计了一个关系排名模块（RRM），通过学习从客观实体的显著性和大小中捕捉人类的主观感知习惯来动态调整它们的排名。实验表明，我们的方法不仅在场景图生成方面取得了最先进的性能，而且在挖掘图像特定关系方面也是专家，这些关系在为下游任务服务方面发挥着重要作用。", "label": 1, "source": "scigen_human", "idx": 259, "lang": "zh"}
{"text": "印度古典舞蹈是一种有5000多年历史的表达情感的多模态语言。通过多媒体技术保护舞蹈是一项具有挑战性的任务。在本文中，我们开发了一个系统来生成舞蹈表演的可解析表示。该系统将有助于保护非物质遗产，注释表演以更好地指导，并综合舞蹈表演。我们首先试图在本体论模型中捕捉印度古典舞蹈形式Bharatanatyam Adavu s的基本步骤的概念。接下来，我们构建了一个基于事件的底层模型，将Adavu的本体与多模式数据流的本体（在本例中为Kinect的RGB-D）联系起来，作为一个可计算实现的框架。我们还提供了一个转录工具，用于将Bharatanatyam Adavu的性能编码为Labanonotation，并在我们记录的数据集上进行测试。我们的主要目的是使用本体论，用腊八符号来记录舞蹈的复杂动作。", "label": 1, "source": "scigen_human", "idx": 260, "lang": "zh"}
{"text": "在本文中，我们感兴趣的是在一阶信息中同时存在多个目标和随机性的情况下，开发凸优化问题的有效算法。我们通过选择一个函数作为目标，将随机多目标优化问题转化为约束优化问题，并试图通过适当的阈值约束其他目标。我们首先研究了一种基于两阶段探索开发的算法，该算法首先通过采样逼近随机目标，然后通过投影梯度法求解约束随机优化问题。即使在对目标的强假设下，该方法也能达到次优收敛速度。我们的第二种方法是一种有效的原对偶随机算法。它利用约束优化中拉格朗日方法的理论，对于一般的Lipschitz连续目标，在高概率下达到了O（1T）的最优收敛速度。", "label": 1, "source": "scigen_human", "idx": 261, "lang": "zh"}
{"text": "机械设备，如发动机、车辆、飞机等，通常配备有许多传感器，以捕捉机器的行为和健康状况。然而，通常存在传感器无法捕捉的外部因素或变量，从而导致固有的不可预测的时间序列。例如，手动控制和或不受监控的环境条件或负载可能导致固有的不可预测的时间序列。使用基于依赖于平稳性的数学模型或利用预测误差来检测异常的预测模型的标准方法，在这种情况下检测异常变得具有挑战性。我们提出了一种基于长短期记忆网络的Encoder-Dec码器方案，用于正常D检测（EncDec-AD），该方案学习重建“正常”时间序列行为，然后使用重建误差来检测异常。我们实验了三个公开可用的准可预测时间序列数据集：电力需求、航天飞机和心电图，以及两个具有预测和不可预测行为的真实世界发动机数据集。我们证明了EncDec AD是稳健的，可以从可预测、不可预测、周期性、非周期性和准周期性的时间序列中检测异常。此外，我们证明EncDec AD能够从短时间序列（长度小到30）和长时间序列（长到500）中检测异常。", "label": 1, "source": "scigen_human", "idx": 262, "lang": "zh"}
{"text": "本文研究了具有乘性噪声的标量状态随机系统的一类约束线性二次型最优控制问题，该问题具有多种应用，特别是在金融风险管理中。我们的模型中考虑的控制和状态变量的线性约束破坏了传统LQ公式的优雅结构，并阻碍了迄今为止文献中明确控制策略的推导。本文利用这类问题的结构所引起的状态分离性质，成功地导出了它的分析控制策略。我们揭示了最优控制策略是状态的分段仿射函数，并且可以通过求解两个耦合的Riccati方程来有效地离线计算。在某些温和条件下，我们还得到了无限时间域的平稳控制策略。我们通过一些示例演示了我们的方法的实现，并展示了如何校准我们的模型来解决动态约束的投资组合优化问题。", "label": 1, "source": "scigen_human", "idx": 263, "lang": "zh"}
{"text": "本文关注以下问题：给定一个在有噪声信道上控制的随机非线性系统，存在编码和控制策略使得闭环系统随机稳定的最大一类信道是什么？所考虑的随机稳定性概念是平稳性、遍历性或渐近平均平稳性。我们不将状态空间限制为紧凑的，例如，所考虑的系统可以由无界噪声驱动。得到了一大类系统和信道的充要条件。得到了一大类非线性系统和信息通道的Bode积分公式的推广。这些发现推广了线性系统的现有结果。", "label": 1, "source": "scigen_human", "idx": 264, "lang": "zh"}
{"text": "网络科学领域是一个高度跨学科的领域；对于网络数据的实证分析，它借鉴了几个研究领域的算法方法。因此，研究程序和对技术结果的描述往往存在差异，有时差异很大。在本文中，我们专注于网络分析算法工程实验部分的方法，这是以实证为重点的研究领域的重要组成部分。更准确地说，我们统一和调整了来自不同领域的现有建议，并为网络分析算法的系统评估提出了通用指南，包括统计分析。这样，可以正确评估新提出的算法的行为，并与现有解决方案进行比较。此外，作为主要的技术贡献，我们提供了SimexPal，这是一种高度自动化的工具，可以根据我们的指导方针执行和分析实验。为了说明SimexPal和我们的指导方针的优点，我们将其应用于一个案例研究：我们设计、执行、可视化和评估最近一种近似介数中心性算法的实验，这是网络分析中的一个重要问题。总之，我们的指导方针和SimexPal都将使之前在实验算法方面的努力现代化并加以补充；它们不仅对网络分析有用，而且在相关的上下文中也有用。", "label": 1, "source": "scigen_human", "idx": 265, "lang": "zh"}
{"text": "自动评估对话一致性是开发高质量开放领域对话系统的一项具有挑战性但要求很高的能力。然而，目前的评估指标只考虑表面特征或话语层面的语义，而没有明确考虑对话流的细粒度主题转换动态。在这里，我们首先考虑对话中主题构成的图结构可以准确地描述底层的沟通逻辑，这是一种更自然的产生说服力指标的方式。在主题级对话图的基础上，我们提出了一个新的评估度量GRADE，它代表自动对话E评估的G图增强R表示。具体来说，GRADE结合了粗粒度的话语级上下文表示和细粒度的主题级图表示来评估对话连贯性。图表示是通过对主题级对话图进行推理而获得的，该对话图用常识图的证据增强，包括k跳相邻表示和跳注意力权重。实验结果表明，就Pearson和Spearman与人类判断的相关性而言，我们的GRADE在衡量不同对话模型方面显著优于其他最先进的指标。此外，我们还发布了一个新的大规模人类评估基准，以促进未来对自动度量的研究。", "label": 1, "source": "scigen_human", "idx": 267, "lang": "zh"}
{"text": "随着物联网的广泛采用，大量加密加速器正在部署中。至关重要的是，这些加速器和其他安全硬件IP是可证明安全的。安全性是一项额外的功能要求，因此许多安全验证工具并不成熟。我们提出了一种方法流PASCAL，它适用于RTL设计，并发现其中潜在的定时侧信道攻击（SCA）漏洞。基于信息流分析，这能够识别可能导致信息泄露的定时差异安全路径。该流程还（自动）消除了由定时通道引起的信息泄漏。插入轻量级补偿器块作为平衡或顺应FSM去除了时序通道，对设计的修改最小，对电路中关键路径的时钟周期时间或组合延迟没有影响。", "label": 1, "source": "scigen_human", "idx": 268, "lang": "zh"}
{"text": "机器学习的可解释性被定义为人类能够理解决策原因的程度。然而，由于神经网络决策过程中的模糊性，它不被认为是可解释的。因此，在本研究中，我们提出了一种新的算法，该算法揭示了经过训练的神经网络认为哪些特征值是重要的，以及在决策过程中主要跟踪哪些路径。在所提出的算法中，定义了由神经网络层之间的相关系数估计的分数，该分数可以通过应用对copula的概念来计算。我们在实验中将估计的分数与随机森林的特征重要性值进行了比较，随机森林有时被视为一种高度可解释的算法，并证实了结果是一致的。该算法提出了一种压缩神经网络及其参数调整的方法，因为该算法识别了有助于分类或预测结果的路径。", "label": 1, "source": "scigen_human", "idx": 269, "lang": "zh"}
{"text": "我们考虑一个通用的多用户移动云计算（MCC）系统，其中每个移动用户都有多个独立的任务。这些移动用户共享计算和通信资源，同时将任务卸载到云。我们研究了通过无线接入点将任务卸载到云的传统MCC，以及具有计算接入点（CAP）的MCC，其中CAP既是网络接入网关，也是移动用户的计算服务提供商。我们的目标是共同优化所有用户的卸载决策以及计算和通信资源的分配，以最大限度地降低所有用户的能源、计算和延迟的总体成本。优化问题被表述为一个非凸二次约束二次规划，它通常是NP难的。对于没有CAP的情况，通过使用可分离半定松弛（SDR），然后恢复二进制卸载决策和通信资源的最优分配，提出了一种有效的近似解决方案MUMTO。为了解决CAP更复杂的问题，我们进一步提出了一种高效的三步算法，称为MUMTO-C，该算法由具有CAP的广义MUMTO SDR、交替优化和顺序调整组成，始终计算局部最优解。对于性能基准测试，我们进一步给出了有和没有CAP的最小系统成本的数值下限。通过与该下限的比较，我们的仿真结果表明，在各种参数设置下，这两种场景的所提出的解决方案都具有几乎最优的性能，因此CAP的有效利用可以带来可观的成本效益。", "label": 1, "source": "scigen_human", "idx": 270, "lang": "zh"}
{"text": "Osborne迭代是一种在线性代数包中广泛使用的平衡n n矩阵的方法，因为平衡保留了特征值并稳定了它们的数字计算。迭代可以在Rn上的任何范数中实现，但通常在L2范数中使用。范数的选择不仅影响期望的平衡条件，而且定义了迭代平衡步骤本身。在本文中，我们重点讨论Osborne在任何Lp范数中的迭代，其中p。我们设计了Osborne在任何Lp范数中的迭代的具体实现，该迭代在O（-2n9k）次迭代中收敛到严格平衡矩阵，其中K大致测量表示输入矩阵项所需的位数。这是第一个证明Osborne的在L2范数（或任何Lp模，p）中的迭代在多项式时间内严格平衡矩阵的结果。这比我们最近的结果（在2017年的SODA中）有了实质性的改进，该结果显示了Lp规范中的弱平衡。此前，Schulman和Sinclair（STOC 2015）在L范数中表现出奥斯本迭代的强大平衡。他们的结果并不意味着其他规范对严格平衡有任何限制。", "label": 1, "source": "scigen_human", "idx": 271, "lang": "zh"}
{"text": "本文提供了一个LaTeX文档的示例，该文档稍微宽松地符合ACM SIG Proceedings的格式指南。这是一种替代风格，可以制作出更紧凑的论文，是为了回应作者对页面预算的担忧而设计的。它补充了文件作者（备用）使用LaTeX 2和BibTeX准备ACM SIG程序集的指南。编写此源文件的目的是在LaTeX 2和BibTeX下进行编译。开发人员试图将每一种可以想象的“铃声和口哨声”包括在内，如副标题、标题脚注、副标题和作者，以及文本和每一个可选组件（如致谢、其他作者、附录），更不用说方程、定理、表和图的例子了。为了充分利用此示例文档，请通过LaTeX和BibTeX运行它，并将此源代码与dvi文件生成的打印输出进行比较。网页上提供了一个经过编译的PDF版本，可以帮助您实现“外观和感觉”。", "label": 1, "source": "scigen_human", "idx": 272, "lang": "zh"}
{"text": "用于多维数据可视化的t-分布式随机邻域嵌入（t-SNE）已被证明是一种流行的方法，在广泛的领域中有着成功的应用。尽管t-SNE预测很有用，但它可能很难解释，甚至具有误导性，这会损害结果的可信度。了解t-SNE本身的细节及其输出中特定模式背后的原因可能是一项艰巨的任务，尤其是对于非降维专家来说。在这项工作中，我们介绍了t-viSNE，这是一种用于视觉探索t-SNE投影的交互式工具，使分析师能够检查其准确性和意义的不同方面，如超参数的影响、距离和邻域保护、特定邻域的密度和成本，以及维度和视觉模式之间的相关性。我们提出了一个连贯、可访问且集成良好的不同视图集合，用于t-SNE投影的可视化。t-viSNE的适用性和可用性是通过真实数据集的假设使用场景来证明的。最后，我们给出了用户研究的结果，其中对该工具的有效性进行了评估。通过揭示运行t-SNE后通常会丢失的信息，我们希望支持分析师使用t-SNE，并使其结果更容易理解。", "label": 1, "source": "scigen_human", "idx": 273, "lang": "zh"}
{"text": "我们探索了一种新的方法来评估生成模型，使用来自人类玩家之间竞争游戏评估的见解。我们通过实验证明，生成器和鉴别器之间的竞争为评估生成模型提供了一种有效的方法。我们介绍了两种总结锦标赛结果的方法：锦标赛获胜率和技能评级。评估在不同的情况下是有用的，包括监测单个模型在训练过程中学习的进展，以及比较两个不同的完全训练模型的能力。我们证明，由一个单一模型组成的锦标赛与过去和未来的模型进行比赛，可以有效地衡量训练进度。包含多个独立模型（使用不同的种子、超参数和架构）的锦标赛提供了不同训练的GANs之间有用的相对比较。基于锦标赛的评分方法在概念上不同于以往许多类别的生成模型评估方法，并且具有互补的优势和劣势。", "label": 1, "source": "scigen_human", "idx": 274, "lang": "zh"}
{"text": "全共形预测系统、分裂共形预测体系和交叉共形预测系的大多数现有实例对预测分布对手头测试对象的适应施加了严格的限制。在本文中，我们开发了完全自适应的分裂共形和交叉共形预测系统。我们的方法包括校准现有的预测系统；输入预测系统不应满足任何有效性性质，而输出预测系统保证在概率上进行校准。有趣的是，该方法也可以在没有保角预测标准IID假设的情况下工作。本文件的版本（第23号工作文件）更新频率最高。", "label": 1, "source": "scigen_human", "idx": 275, "lang": "zh"}
{"text": "我们提出了一种适用于比较使用不同数值离散化的算法的性能分析。通过考虑求解的总时间、相对于误差范数的数值精度和计算速率，可以进行成本效益分析，以确定哪种算法和离散化特别适合应用。这项工作扩展了性能谱模型，用于解释PDE数值模拟中的硬件和算法权衡。作为概念验证，使用流行的有限元软件包来说明泊松方程的这种分析。", "label": 1, "source": "scigen_human", "idx": 276, "lang": "zh"}
{"text": "信息时代（AoI）的概念已经成为网络和控制系统中一个重要的性能指标。以AoI为代表的信息新鲜度自然出现在缓存环境中。我们解决了内容大小不同的分时段系统的缓存更新的最佳调度问题。缓存和内容更新的容量有限。每个内容都与在AoI中单调递减的效用函数相关联。对于这个组合优化问题，我们给出了以下贡献。首先，我们给出了解决问题可处理性边界的理论结果。特别是，通过使用网络流的重新表述，我们证明了边界本质上是由内容是否大小相等决定的。其次，我们推导了该问题的一个整数线性公式，该公式可以在小规模场景下获得最优解。接下来，通过数学公式，我们推导了一种使用重复列生成的可扩展优化算法。此外，该算法还计算了全局最优的界，可用于评估任何调度解决方案的性能。大规模场景的性能评估证明了该算法与贪婪调度相比的优势。最后，我们将我们的工作扩展到循环调度。", "label": 1, "source": "scigen_human", "idx": 277, "lang": "zh"}
{"text": "欧盟的《通用数据保护条例》（GDPR）是最近颁布的最著名的隐私条例。尽管该法规具有法律、政治和技术方面的影响，但为了更好地理解GDPR对需求工程和软件架构的实际影响，人们进行的研究相对较少。本文建立在与芬兰软件行业密切相关的基础理论方法之上，有助于填补以往研究中的这一空白。在软件开发组织的背景下提出并回答了三个问题。首先，本文阐述了许多中小企业在实施解决新监管需求的解决方案时经常面临的九个实际制约因素。其次，本文从GDPR中引出了软件体系结构的九个监管要求。第三，本文给出了一个软件体系结构的实现，该体系结构既符合所引出的需求，又符合所阐述的约束。", "label": 1, "source": "scigen_human", "idx": 278, "lang": "zh"}
{"text": "通过无处不在的设备提供心理健康干预措施已经显示出很大的前景。会话聊天机器人是一个很有前途的预言家，可以提供适当的即时干预。然而，设计情感感知的代理，特别是在这种背景下，却没有得到充分的探索。此外，通过这种试剂自动提供即时mHealth干预措施的可行性尚未得到充分研究。在本文中，我们通过对N39名参与者进行为期两周的人体受试者实验，介绍了EMMA（EMotion Aware mHealth Agent）的设计和评估。EMMA以同理心的方式提供情感上合适的微活动。我们展示了该系统可以扩展到纯粹从智能手机传感器数据检测用户的情绪。我们的结果表明，通过用户的自我情绪报告，我们的个性化机器学习模型被认为是可爱的。最后，我们为mHealth的情感感知机器人的设计提供了一套指南。", "label": 1, "source": "scigen_human", "idx": 279, "lang": "zh"}
{"text": "在这项工作中，我们训练全卷积网络来检测语音中的愤怒。由于训练这些深度架构需要大量数据，而情绪数据集的大小相对较小，因此我们使用迁移学习。然而，与以前使用基于语音或情绪的任务作为源模型的方法不同，我们使用SoundNet，这是一种在大规模视频数据集上进行多模式训练的全卷积神经网络，用于对音频进行分类，并由基于视觉的分类器提供基本事实标签。作为来自SoundNet的迁移学习的结果，我们训练的愤怒检测模型提高了性能，并在各种行为、引发和自然情绪语音数据集上得到了很好的推广。我们还通过在普通话语音情感数据上评估我们的英语训练模型来测试我们的模型的跨语言有效性。此外，我们提出的系统具有适用于实时应用的低延迟，只需要1.2秒的音频即可进行可靠的分类。", "label": 1, "source": "scigen_human", "idx": 280, "lang": "zh"}
{"text": "在连续消除列表（SCL）解码方案下，循环冗余校验（CRC）辅助极性码能够实现比低密度奇偶校验（LDPC）码更好的性能。然而，SCL解码方案具有非常高的空间和时间复杂性。特别是，在现代移动通信标准中采用极化码时，高空间复杂度是一个主要问题。在本文中，我们提出了一种新的降低复杂度的连续取消列表（R-SCL）解码方案，该方案能够有效地降低空间复杂度。仿真结果表明，在（20481024）CRC辅助极码的情况下，R-SCL解码器的空间复杂度降低了25，时间复杂度减少了8，仍然可以实现与SCL解码器几乎相同的性能水平。为了进一步降低复杂度，我们提出了一种用于极化码的多CRC编码方案。仿真结果表明，在（163848192）多CRC辅助极性码的情况下，R-SCL解码器的空间复杂度降低了约85，时间复杂度减少了约20，导致的最差性能损失仅为0.04dB。", "label": 1, "source": "scigen_human", "idx": 281, "lang": "zh"}
{"text": "在工程应用中，访问具有强标记声音事件的大型语料库是昂贵且困难的。许多研究转向解决如何检测具有仅指定类型的弱标签的声音事件的类型和时间戳的问题。这项任务可以被视为一个多实例学习（MIL）问题，其关键是池函数的设计。在本文中，我们提出了一种自适应功率池函数，它可以自动适应各种声源。在两个公共数据集上，所提出的功率池函数在粗粒度和细粒度度量上都优于最先进的线性softmax池。值得注意的是，相对于这两个数据集，它将基于事件的F1分数（评估事件开始和偏移的检测）提高了11.4和10.2。虽然本文专注于声音事件检测应用，但所提出的方法也可以应用于其他领域的MIL任务。", "label": 1, "source": "scigen_human", "idx": 283, "lang": "zh"}
{"text": "基于局部编码图像特征的方法最近在纹理分类任务中变得流行，特别是在由于照明、尺度和视点的变化而存在大的类内变化的情况下。受图像结构分析理论的启发，本文提出了一种高效、简单、稳健的纹理分类描述符——局部喷射模式（Ljp）。在这种方法中，纹理图像的射流空间表示是根据高斯（DtGs）滤波器响应的一组导数计算的，这些导数高达二阶，即所谓的局部射流矢量（Ljv），它们也满足尺度空间特性。Ljp是利用中心像素与其在喷流空间中的局部邻域的关系得到的。最后，通过串联Ljv的所有元素的Ljp的直方图来形成纹理图像的特征向量。高达二阶的所有DtG响应一起保留了固有的局部图像结构，并实现了对比例、旋转和反射的不变性。这使我们能够为纹理分类设计一个具有鉴别力和鲁棒性的框架。在五个标准纹理图像数据库上进行了广泛的实验，使用最近子空间分类器（Nsc），所提出的描述符分别对OutexTC10、OutexTC12、KTH-TIPS、Brodatz和CUReT实现了100、99.92、99.75、99.16和99.65的准确度，这与最先进的方法相比是更好的。", "label": 1, "source": "scigen_human", "idx": 284, "lang": "zh"}
{"text": "使用大的统计样本对核物理出版物作者的增加进行了调查。这是通过核科学参考文献（NSR）和实验核反应（EXFOR）数据库的核数据挖掘实现的。将讨论这项研究的结果并给出结论。", "label": 1, "source": "scigen_human", "idx": 285, "lang": "zh"}
{"text": "在本文中，我们使用博弈论对中毒攻击场景进行建模。证明了在攻防博弈中纯策略纳什均衡的不存在性。然后，我们提出了我们的博弈模型和算法的混合扩展，以近似防守球员的纳什均衡策略。然后，我们在实验中证明了该算法生成的混合防御策略的有效性。", "label": 1, "source": "scigen_human", "idx": 287, "lang": "zh"}
{"text": "我们提出了Waterfilling电路选择方法，我们设计该方法是为了降低成功的端到端流量相关攻击的风险。Waterfilling通过在用户路径的端点上尽可能均匀地平衡Tor网络负载来进行。由于TorPS和Shadow工具，我们模拟了Waterfilling的使用。应用几个安全指标，我们发现，采用Waterfilling大大增加了对手为了成功发起攻击所需控制的节点数量，同时在一定程度上降低了进行攻击所需的最低带宽。此外，我们评估了Shadow中的Waterfilling，并表明它不会对网络性能产生显著影响。此外，Waterfilling降低了攻击者通过侵入顶级带宽Tor中继可能获得的好处，从而限制了此类中继带来的风险。注水不需要对Tor进行任何重大更改，并且可以与当前电路选择算法共存。", "label": 1, "source": "scigen_human", "idx": 288, "lang": "zh"}
{"text": "我们提出了一个相位检索的信息论框架。具体而言，我们考虑从压缩率为R的m个R n无相位测量中恢复未知向量x R n直至总符号因子的问题，并导出R的一般可达性界。令人惊讶的是，事实证明，压缩率的这一界限与Wu和Verdu（2010）获得的几乎无损模拟压缩的界限相同：无相线性测量与具有全相位信息的线性测量“一样好”，因为忽略m测量的符号只会使我们对x的整体符号因子产生歧义。", "label": 1, "source": "scigen_human", "idx": 289, "lang": "zh"}
{"text": "我们研究一组存储单元之间的竞争与合作。随着储能器数量的增加，在竞争中，储能器的利润接近于零。我们提出了两种存储可以实现最大利润的方法。第一种是去中心化的方法，在这种方法中，存储会产生人为成本，这会激励它们像联盟一样行事。不需要在存储器之间交换私人信息来计算人工成本函数。第二种是集中的方法，其中聚合器协调并分配利润与存储，以实现最大利润。我们不假设存储聚合器关系的性质，也不推导长期合作的必要条件。我们使用纳什公理化讨价还价问题来建模和预测聚合器和存储之间的利润分配。", "label": 1, "source": "scigen_human", "idx": 290, "lang": "zh"}
{"text": "当卷积神经网络用于连续更新时间序列的动态评估时，会执行许多冗余卷积运算。我们提出了深度移位的方法，该方法记住先前计算的卷积运算结果，以最大限度地减少计算次数。复杂性的降低至少是一个常数，在最好的情况下是二次的。我们证明，在实际实现中，这种方法确实节省了大量的计算时间，尤其是当网络接收到大量时间帧时。", "label": 1, "source": "scigen_human", "idx": 291, "lang": "zh"}
{"text": "根据Boutillier、Darwishe和Pearl等人的观点，反复修正的原则可以用改变对条件句的信念来表征。对于迭代收缩，类似的公式是未知的。这尤其是因为对于迭代信念变化，通过Levi和Harper恒等式的修正和收缩之间的联系并不简单，因此，表征结果不容易在迭代修正和收缩间转移。在这篇文章中，我们根据条件信念的变化，发展了迭代收缩的公理化。我们证明了新的公设集在语义上符合一类算子，如Konieczny和Pino-Perez给出的迭代收缩算子。", "label": 1, "source": "scigen_human", "idx": 292, "lang": "zh"}
{"text": "在传播信道条件方面准确和鲁棒的室内定位系统今天仍然是一个技术挑战。特别是，对于基于无线电信号的距离测量的系统，非视距（NLOS）情况可能会导致较大的位置误差。在本文中，我们通过在具有代表性的室内环境中进行测量来解决这些问题。结果表明，除非使用非常大的信号带宽，否则使用高复杂度和低复杂度测距算法的传统跟踪方案会受到NLOS条件的严重影响。此外，我们还讨论和评估了多路径辅助室内导航和跟踪（MINT）的性能，它可以通过利用多路径传播来克服这些障碍。在宽带宽范围内，与传统方案相比，MINT表现出优越的性能，并且几乎没有由于NLOS条件而导致其鲁棒性下降。", "label": 1, "source": "scigen_human", "idx": 293, "lang": "zh"}
{"text": "脸书等在线社交网络披露了前所未有的个人信息量，放大了社交比较的场合。我们检验了一个假设，即社交网站的使用会增加人们对收入的不满。在解决了内生性问题后，我们的研究结果表明，社交网络用户将自己的成就与其他人的成就进行比较的概率更高。这种影响似乎比看电视更强烈，对年轻人来说尤其强烈，对男性和女性的影响也相似。关键词：社交网络；社交网站；社会比较；对收入的满意度；相对匮乏。JEL分类代码：D03；L31", "label": 1, "source": "scigen_human", "idx": 294, "lang": "zh"}
{"text": "为了帮助研究人员有效地识别环境微生物，本文提出了一种用于环境微生物图像分割的多尺度CNN-CRF（MSCC）框架。该框架分为两部分：第一部分是一种新颖的像素级分割方法，使用新引入的卷积神经网络（CNN），即“mU-Net-B3”，并进行密集条件随机场（CRF）后处理。第二种是基于VGG-16的补丁级分割方法，该方法采用了一种新颖的“缓冲”策略，进一步提高了EM细节的分割质量。在实验中，与现有的420张EM图像的方法相比，所提出的MSCC方法将内存需求从355 MB降低到103 MB，将总体评估指标（Dice、Jaccard、Recall、Accuracy）从85.24、77.42、82.27和96.76分别提高到87.13、79.74、87.12和96.91，并将体积重叠误差从22.58降低到20.26。因此，MSCC方法在EM分割领域显示出巨大的潜力。", "label": 1, "source": "scigen_human", "idx": 295, "lang": "zh"}
{"text": "最近在学习本体（层次结构和部分有序结构）方面的工作利用了学习表示的空间的内在几何结构来进行自动服从复杂结构约束的预测。我们探索了一个这样的模型的两个扩展，即用于层次关系学习的顺序嵌入（，）模型，目的是提高文本数据用于常识知识表示的性能。我们的第一个模型以原始文本的形式联合学习排序关系和非层次知识。我们的第二个扩展利用训练数据的偏序结构来寻找嵌入之间的长距离三元组约束，这些约束在成对训练过程中执行得很差。我们发现，与原始顺序嵌入模型和其他强基线相比，结合自由文本和增强训练约束都有所改进。", "label": 1, "source": "scigen_human", "idx": 296, "lang": "zh"}
{"text": "我们分析了有偏随机梯度方法（SGD）的复杂性，其中单个更新被确定性的，即有偏误差项破坏。我们得到了光滑（非凸）函数的收敛结果，并给出了在Polyak ojasewicz条件下的改进率。我们量化了偏差的大小如何影响可达到的精度和收敛速度。我们的框架涵盖了许多应用程序，在这些应用程序中，要么只提供有偏差的梯度更新，要么出于性能原因首选无偏差梯度更新。例如，在分布式学习领域，有偏见的梯度压缩技术（如top-k压缩）已被提出作为缓解通信瓶颈的工具，而在无导数优化中，只有偏见的梯度估计器才能被查询。我们讨论了几个指导性的例子，这些例子表明了我们的分析的广泛适用性。", "label": 1, "source": "scigen_human", "idx": 297, "lang": "zh"}
{"text": "室内场景中的3D布局恢复问题是十多年来的核心研究课题。然而，仍有几个重大挑战尚未解决。在最相关的方法中，最先进的方法的主要部分对场景进行了隐含或明确的假设，例如箱形或曼哈顿布局。此外，当前的方法计算成本高昂，不适合机器人导航和ARVR等实时应用。在这项工作中，我们介绍了CFL（布局角），这是第一个在360图像上恢复3D布局的端到端模型。我们的实验结果表明，我们的表现优于现有技术，与其他作品相比，我们对场景的假设更少，成本更低。我们还表明，通过使用EquiConvs，我们的模型比传统方法更好地推广到相机位置变化，EquiConvs是一种直接应用于球面投影的卷积，因此对等矩形失真不变。", "label": 1, "source": "scigen_human", "idx": 298, "lang": "zh"}
{"text": "深度神经网络具有强大的表达能力，甚至可以记住带有错误标签的样本。重申DNN中针对标签腐败的稳健性和通用性至关重要。为此，本文研究了0-1损失，它与经验对手（重新加权）风险（，）具有单调关系。虽然0-1损失具有一些鲁棒性，但很难进行优化。为了有效地优化0-1损失，同时保持其鲁棒性，我们提出了一种非常简单有效的损失，即课程损失（CL）。与传统的基于求和的替代损失相比，我们的CL是0-1损失的更严格的上限。此外，CL可以自适应地选择样本进行模型训练。因此，我们的损失可以被视为课程样本选择策略的一个新视角，它架起了课程学习和稳健学习之间的桥梁。在基准数据集上的实验结果验证了所提出的损失的稳健性。", "label": 1, "source": "scigen_human", "idx": 299, "lang": "zh"}
{"text": "场景图像中的文本通常由几个字符组成，并呈现出特征性的序列结构。现有方法通过编码器捕获具有序列到序列模型的结构以具有视觉表示，然后通过解码器将特征转换为标签序列。在本文中，我们通过考虑编码器阶段的长期时间依赖性来研究文本识别框架。我们证明了所提出的具有增加的序列范围的时间卷积编码器提高了文本识别的准确性。我们还研究了卷积块中不同注意力模块对学习准确文本表示的影响。我们在七个数据集上进行了比较，实验证明了我们提出的方法的有效性。", "label": 1, "source": "scigen_human", "idx": 300, "lang": "zh"}
{"text": "学习医学应用中的可解释表示对于将数据驱动模型应用于临床实践至关重要。最近的研究表明，学习解纠缠的特征表示对于更紧凑和可解释的数据表示很重要。在本文中，我们引入了一种新的具有全相关约束的对抗性变分自动编码器，以增强潜在表示的独立性，同时保持重建保真度。我们提出的方法在公开的数据集上得到了验证，表明所学习的解纠缠表示不仅是可解释的，而且优于最先进的方法。我们报告了在解纠缠方面的81.50、在聚类方面的11.60和在具有少量标记数据的监督分类方面的2的相对改进。", "label": 1, "source": "scigen_human", "idx": 301, "lang": "zh"}
{"text": "我们在一个统一的框架中研究了两种类型的预条件和预条件随机梯度下降（SGD）方法。由于第一种预条件与牛顿方法关系密切，我们称之为牛顿型，第二种预条件为Fisher型，因为其预条件与Fisher信息矩阵的逆关系密切。两种预条件都可以从一个框架中导出，并使用自然或相对梯度下降在用户指定的任何矩阵李群上有效地学习。许多现有的预处理器和方法都是牛顿型或费雪型的特例。报告了相对大规模的机器学习问题的实验结果，用于性能研究。", "label": 1, "source": "scigen_human", "idx": 302, "lang": "zh"}
{"text": "双直觉稳定时态逻辑（BIST逻辑）是具有Kripke语义的时态逻辑，其中框架中的世界配备有预序以及相对于该预序“稳定”的可访问关系。BIST逻辑是逻辑BiSKt的扩展，它出现在超图的语义上下文中，因为预序的特殊情况可以表示超图的关联结构。本文首次给出了BISKt的Hilbert式公理化，并证明了BISK的强完备性。我们进一步证明了一类BIST逻辑的强完备性，该逻辑是用一定形式的公式推广BiSKt得到的，并且证明了有限模型性质和可判定性是成立的。", "label": 1, "source": "scigen_human", "idx": 303, "lang": "zh"}
{"text": "在实时战略（RTS）游戏中，管理人工智能复杂性的一种常用技术是使用动作和或状态抽象。高层抽象通常可以带来良好的战略决策，但战术决策质量可能会因细节丢失而受到影响。一种竞争性的方法是对搜索空间进行采样，这通常会导致在简单场景中具有良好的战术性能，但高层规划较差。我们建议使用深度卷积神经网络（CNN）在有限的一组抽象动作选择中进行选择，并利用剩余的计算时间进行博弈树搜索，以改进低级别策略。CNN是通过对Puppet Search（一种使用动作抽象的战略搜索算法）标记的游戏状态进行监督学习来训练的。然后，网络被用来选择一个脚本——一个抽象动作——为所有单元生成低级动作。随后，游戏树搜索算法仅考虑靠近对手单位的单位，使用有限的游戏状态视图来改进单位子集的战术行动。RTS游戏中的实验表明，组合算法的胜率高于其两个独立组件和其他最先进的RTS代理中的任何一个。据我们所知，这是卷积网络首次成功应用于在标准游戏地图上玩完整的RTS游戏，因为之前的工作主要关注子问题，如战斗或非常小的地图。", "label": 1, "source": "scigen_human", "idx": 304, "lang": "zh"}
{"text": "语篇连贯性是人工生成语篇和自动生成语篇都需要衡量的一个重要属性；但明确定义的量化指标仍然难以捉摸。在本文中，我们通过分析输入段落潜在的主题结构，提出了一种在实值量表上对其主题连贯性进行评分的指标。我们首先提取出一段文字的句子所涉及的所有可能的主题。然后通过计算来衡量这段文字的连贯性：（a）主题相对于该段的不确定性程度，以及（b）这些主题之间的相关性。我们的模块化框架的所有组件都只依赖于未标记的数据和WordNet，因此使其完全无监督，这是任何度量的通用使用的一个重要特征。实验在两个数据集上进行——一个是用于论文评分的公开数据集（代表人类话语），另一个是通过混合涵盖不同主题的多个段落的内容构建的合成数据集。我们的评估表明，两个数据集的一致性得分与基本事实呈正相关。通过对合成数据进行人工评估，进一步验证了我们的连贯性得分，显示出79.3的显著一致性。", "label": 1, "source": "scigen_human", "idx": 305, "lang": "zh"}
{"text": "虽然预训练和微调，例如BERT（，）、GPT-2（，），在语言理解和生成任务中取得了巨大成功，但预训练的模型在内存成本和推理速度方面通常太大，无法在线部署，这阻碍了它们的实际在线使用。在本文中，我们提出了LightPAFF，这是一种轻量级的预训练和微调框架，它利用两阶段的知识提取，在预训练和调优阶段将知识从大教师模型转移到轻量级学生模型。通过这种方式，轻量级模型可以实现与大型教师模型类似的精度，但参数要少得多，因此在线推理速度更快。LightPAFF可以支持不同的预训练方法（如BERT、GPT-2和MASS），并可应用于许多下游任务。在三个语言理解任务、三个语言建模任务和三个序列到序列生成任务上的实验表明，在实现与大型BERT、GPU-2和MASS模型相似的精度的同时，LightPAFF将模型大小减少了近5倍，并将在线推理速度提高了5倍到7倍。", "label": 1, "source": "scigen_human", "idx": 306, "lang": "zh"}
{"text": "在本文中，我们在卷积神经网络上运行了两种解释方法，即LIME和Grad-CAM，该网络经过训练，可以用图像中可见的乐高积木标记图像。我们根据两个标准来评估它们，即网络核心性能的提高和它们能够为系统用户产生的信任。我们发现，总的来说，Grad CAM在这项特定任务上似乎优于LIME：它从核心性能的角度产生了更详细的见解，当涉及到他们对模型的信任时，被要求在两者之间做出选择的受访者中有80人选择了Grad CAM。然而，我们也认为，将这两种方法结合起来更有用，因为它们产生的见解是互补的。", "label": 1, "source": "scigen_human", "idx": 307, "lang": "zh"}
{"text": "深度学习网络（DNN）的最新突破性进展使其对嵌入式系统具有吸引力。然而，DNN在资源有限的嵌入式设备上进行推理可能需要很长时间。由于隐私问题、高延迟或缺乏连接，将计算卸载到云中通常是不可行的。因此，迫切需要找到一种在设备上本地有效执行DNN模型的方法。本文提出了一种自适应方案，通过考虑所需的精度和推理时间来确定给定输入使用哪个DNN模型。我们的方法使用机器学习来开发预测模型，以快速选择预训练的DNN用于给定的输入和优化约束。我们通过首先离线训练预测模型，然后使用学习的模型选择DNN模型用于新的、看不见的输入来实现这一点。我们将我们的方法应用于图像分类任务，并使用ImageNet ILSVRC 2012验证数据集在Jetson TX2嵌入式深度学习平台上对其进行评估。我们考虑了一系列有影响力的DNN模型。实验结果表明，与最有能力的单个DNN模型相比，我们的方法在推理精度上提高了7.52，推理时间减少了1.8倍。", "label": 1, "source": "scigen_human", "idx": 308, "lang": "zh"}
{"text": "在直接视线被遮挡的情况下，例如当视线在拐角处被遮挡时，视觉对象识别在广泛的应用中具有实际意义。在相干照明的情况下，从漫射壁散射的光形成包含隐藏对象信息的散斑图案。利用这些散斑图案可以实现非视线（NLOS）识别。我们介绍了一种基于深度神经网络斑点模式识别的新方法，该方法比其他NLOS识别方法更简单、更鲁棒。通过仿真和实验验证了该方法的可行性和性能。", "label": 1, "source": "scigen_human", "idx": 309, "lang": "zh"}
{"text": "具有足够记忆随机噪声能力的过参数化深度神经网络（DNN）可以在正常数据集上获得优异的泛化性能，挑战了经典学习理论中的偏差-方差权衡。最近的研究表明，DNN首先学习简单的模式，然后记忆噪音；其他一些工作表明，DNN在训练过程中具有从低频到高频学习目标函数的频谱偏差。这些表明了DNN的泛化、记忆和频谱偏差之间的一些联系：输入空间中的低频分量表示可以泛化的模式，而高频分量表示需要记忆的噪声。然而，我们发现这不是真的：在深度双下降的实验设置下，DNN的高频分量在第二次下降时开始减少，而带有随机标签的例子仍在记忆中。此外，我们发现DNN的频谱可以用于监测测试行为，例如，它可以指示测试误差的第二次下降何时开始，即使频谱仅从训练集计算。", "label": 1, "source": "scigen_human", "idx": 310, "lang": "zh"}
{"text": "本文研究了一种用于设计具有通用调光支持的二进制调制可见光通信（VLC）收发器的深度学习（DL）框架。光学二进制信号的调光控制归结为组合码本设计，使得二进制码字的平均汉明权重与任意调光目标匹配。采用无监督DL技术来获得神经网络，以取代从光传输信号中恢复消息的编码器-解码器对。在这样的任务中，开发了一种新的随机二值化方法来从连续值神经网络输出生成二进制码字集。为了普遍支持任意调光目标，基于DL的VLC收发器使用多个调光约束进行训练，这是一种约束训练优化，用现有的DL方法处理非常具有挑战性。我们开发了一种新的训练算法，通过优化的对偶公式来解决调光约束。基于所开发的算法，可以通过端到端的训练过程来优化所得到的VLC收发器。数值结果验证了所提出的码本在各种VLC设置下都优于理论上最好的恒重码本。", "label": 1, "source": "scigen_human", "idx": 311, "lang": "zh"}
{"text": "我们考虑从多频率无相位远场数据确定声源的反源问题。通过在反向源模型中补充一些参考点源，我们开发了一种新的策略来恢复远场数据的相位信息。这种参考源技术产生了一个易于实现的相位检索公式。从数学上讲，相位反演方法的稳定性是严格合理的。然后，我们采用傅立叶方法来处理具有恢复相位信息的多频反源问题。最后，给出了一些二维和三维数值结果，证明了该方法的可行性和有效性。", "label": 1, "source": "scigen_human", "idx": 312, "lang": "zh"}
{"text": "电影中的视觉和音频信息可以唤起观众的各种情绪。为了更好地理解观众的影响，我们提出了MediaEval 2018电影情感影响任务的方法，以连续预测电影中的预期效价和唤醒。这项任务使用LIRIS-ACCEDE数据集，使研究人员能够比较预测电影对观众影响的不同方法。我们的方法利用了使用预先训练的神经网络计算的基于图像、音频和人脸的特征。这些特征是随着时间的推移而计算的，并使用基于门控递归单元（GRU）的网络进行建模，然后使用混合专家模型来计算多类预测。对于最终结果，我们使用巴特沃斯滤波器对这些预测进行了平滑处理。我们的方法使我们能够在MediaEval 2018任务的三个评估指标中获得最佳性能。", "label": 1, "source": "scigen_human", "idx": 313, "lang": "zh"}
{"text": "我们考虑一个分布式学习问题，其中计算是在由一个主节点和多个工作节点组成的系统上进行的。在这样的系统中，被称为掉队者的慢速运行机器的存在将导致性能的显著下降。最近，Tandon等人建立了一个用于减少分布式学习中掉队者的编码理论框架，称为梯度编码（GC）。大多数关于GC的研究都是为了在完全假设梯度下降（GD）算法作为学习算法的情况下恢复梯度信息。另一方面，如果使用随机梯度下降（SGD）算法，则不需要完全恢复梯度信息，并且其无偏估计器足以进行学习。在本文中，我们提出了一种使用低密度生成矩阵（LDGM）码的分布式SGD方案。在所提出的系统中，与现有的GC方法相比，完全恢复梯度信息可能需要更长的时间，然而，它使主节点能够以低计算成本获得梯度的高质量无偏估计器，并提高了整体性能。", "label": 1, "source": "scigen_human", "idx": 314, "lang": "zh"}
{"text": "本文主要研究由无人机（UAV）和无人地面飞行器（UGV）组成的系统的控制，它们协同操纵物体。这两个单元受到致动器饱和的影响，并协同将物体移动到以其位置和倾斜度为特征的所需姿势。本文提出了一种控制策略，即地面飞行器负责将物体部署到某个位置，而空中飞行器则调整其倾斜度。地面车辆由饱和比例微分控制律控制。飞行器通过专门为该问题设计的级联控制进行调节，该级联控制能够利用机械互连。通过输入状态稳定性和小增益定理论证了整个系统的稳定性。为了解决约束满足问题，实现了一种非线性参考调速器方案。通过数值模拟验证了该方法的有效性。", "label": 1, "source": "scigen_human", "idx": 315, "lang": "zh"}
{"text": "麦克斯韦的魔鬼，“一个能力如此敏锐的人，他可以跟随每一个分子的进程”，一直是关于其违反热力学第二定律的能力的争论的中心。Landauer的假设，即魔鬼必须抹去它的记忆并产生热力学成本，已经成为对Maxwell困境的标准回应，其对计算热力学的影响延伸到量子和经典计算的许多领域。然而，这仍然是一个假设。争论往往集中在盒子里单个粒子的简单玩具模型上。尽管它们很简单，但这些系统准确表示热力学（特别是满足第二定律）的能力，以及它们是否显示朗道擦除，一直是一个争论不休的问题。最近诺顿-Ladyman的争议就是这样一个例子。本文介绍了一种程序设计语言来描述这些简单的热力学过程，并给出了形式化的操作语义和程序逻辑，作为热力学系统形式化推理的基础。我们将基本的单粒子运算形式化为语言中的语句，然后证明这些基本运算的任何组合都必须满足第二定律。这是通过找到系统的计算不变量来实现的。此外，我们证明了这个不变量需要一个擦除代价才能存在于系统中，等于一点信息的k T ln 2：Landauer擦除成为形式系统的一个定理。因此，Norton Ladyman争议可以以严格的方式解决，此外，我们引入的形式主义为进一步分析Landauer擦除提供了一套推理工具，这些推理工具可证明与热力学第二定律一致。", "label": 1, "source": "scigen_human", "idx": 316, "lang": "zh"}
{"text": "多武装土匪问题主要是在长度T的范围内累积的预期总报酬的度量下进行研究的。在本文中，我们解决了多武装土匪问题中的风险问题，并在均值方差（经济学和数学金融中常用的风险度量）的度量下发展了平行结果。我们证明了就奖励过程的平均方差而言，模型特定的后悔和模型独立的后悔分别是（logT）和（T23）的下界。然后，我们展示了为经典的风险中性MAB开发的UCB政策和DSEE政策的变体实现了这些下限。", "label": 1, "source": "scigen_human", "idx": 317, "lang": "zh"}
{"text": "我们使用范畴理论的语言为混合系统的形式综合开发了一个组成框架。更具体地说，我们为分层、顺序和独立的并行组合提供了相互兼容的工具。在我们的框架中，混合系统的层次结构对应于模板锚对，我们将其建模为细分和嵌入半共轭的跨度。模板锚对的分层组成对应于通过纤维产品的跨度的组成。为了对序列组成进行建模，我们引入了“有向混合系统”，在康利理论意义上，每个系统都从初始子系统流到最终子系统。有向系统的序列组成是通过图嵌入的推出来给出的，重写重叠子系统的连续动力学以优先考虑第二个有向系统。独立平行成分对应于关于半共轭性的分类乘积。为了形式化这三种类型的组合的相容性，我们构造了一个混合系统的垂直笛卡尔双范畴，其中垂直态射是半共轭的，水平态射是有向混合系统。", "label": 1, "source": "scigen_human", "idx": 318, "lang": "zh"}
{"text": "我们考虑一个具有无限多个臂的随机土匪问题。在这种情况下，学习者没有机会尝试所有的武器，甚至一次，并且必须将其有限数量的样本仅用于特定数量的武器。之前针对该设置的所有算法都是为了最大限度地减少学习者的累积遗憾而设计的。在本文中，我们提出了一种旨在最小化简单遗憾的算法。正如在无限多武装匪徒的累积后悔设置中一样，简单后悔率将取决于表征近似最优武器分布的参数。我们证明，根据，我们的算法在乘法常数或log（n）因子下都是极小极大最优的。我们还提供了几个重要情况的扩展：当未知时，在接近最优臂具有小方差的自然环境中，以及在未知时间范围的情况下。", "label": 1, "source": "scigen_human", "idx": 319, "lang": "zh"}
{"text": "最大平衡子图问题（MBSP）是找到一个有符号图的子图的问题，该子图是平衡的，并使其顶点集的基数最大。我们对问题的精确解感兴趣：提出了一种改进的分支和切割算法。对文献中先前讨论的三个应用程序的一组实例以及一组随机实例进行了广泛的计算实验。关键词：平衡有符号图；分支和切割；投资组合分析；网络矩阵；社区结构。", "label": 1, "source": "scigen_human", "idx": 320, "lang": "zh"}
{"text": "根据感觉运动偶然性理论，我们从基本的感觉运动角度研究了空间感知问题。尽管它在我们对世界的感知中无处不在，但空间概念的起源在很大程度上仍然是神秘的。例如，在人工感知的背景下，通常通过让工程师预先定义代理必须面对的问题的空间结构来规避这个问题。我们在这里表明，空间的结构可以由天真的主体以感觉运动规律的形式自主发现，这对应于所谓的可补偿的感觉体验：这些体验可以由主体或其环境产生。通过检测这种可补偿的体验，智能体可以推断其身体在其中运动的外部空间的拓扑和度量结构。我们提出了这些规律性质的理论描述，并说明了在配备有类眼传感器的模拟机械臂上与物体相互作用的方法。最后，我们展示了如何使用这些规律来构建传感器外部空间配置的内部表示。", "label": 1, "source": "scigen_human", "idx": 321, "lang": "zh"}
{"text": "众包人工解决或在线打字攻击是具有破坏性的问题。然而，对这些主题的研究是有限的。在本文中，我们关注的是这种攻击，因为它的设计目的，所有的验证码都可以被简单地破坏。在全面分析了Typer现象和验证码的攻击机制后，我们提出了一种新的验证码设计原则，以区分人（Typer）和人（user）。核心思想是CAPTCHA的质询过程应该包含具有私有属性的唯一信息。我们的想法是基于人类之间的信息不对称。如果没有这些私人信息，打字者即使识别出CAPTCHA中的所有字符，也无法完成攻击。根据web交互和密码处理程序，我们在所提出的原理上形式化、设计和实现了两个例子，一个是基于字符的例子，另一个是数据报的例子。我们要求用户从不在密码序列中的随机字符中选择密码，或者将随机排序的序列按正确顺序排列。为了增加人工容错能力和随机猜测攻击的难度，提出了一种新的模糊匹配生成算法。与其他解决方案不同，我们的方法不需要修改典型web服务的主要身份验证协议、用户界面和体验。几项用户研究的结果表明，我们提出的方法既简单（人类可以在不到20秒内准确解决）又高效（Typer只能部署成功率很低的随机猜测攻击）。", "label": 1, "source": "scigen_human", "idx": 322, "lang": "zh"}
{"text": "我们给出了一般类随机对策的虚拟博弈动力学，并分析了其在零和随机对策中的收敛性。我们的动力学涉及到代理人对对手策略和他们自己的持续收益（Q函数）形成信念，并使用估计的持续收益进行短视的最佳反应。代理人通过观察对手的行为来更新他们在状态下的信念。学习动态的一个关键特性是，Q函数信念的更新发生在比策略信念更新更慢的时间尺度上。我们表明，在基于模型和无模型的情况下（不知道代理支付函数和状态转移概率），对策略的信念收敛到零和随机博弈的平稳混合纳什均衡。", "label": 1, "source": "scigen_human", "idx": 323, "lang": "zh"}
{"text": "（全局）Lipschitz光滑条件对于建立大多数优化方法的收敛理论是至关重要的。不幸的是，大多数机器学习和信号处理问题都不是Lipschitz平滑的。这促使我们将Lipschitz光滑条件的概念推广到任何有限阶多项式目标函数都满足的相对光滑条件。此外，这项工作开发了新的基于Bregman散度的算法，这些算法保证收敛到任何相对光滑问题的二阶平稳点。此外，当我们将Bregman散度专门化为欧氏距离时，所提出的优化方法涵盖了近端交替最小化和近端交替线性化最小化。因此，这项工作不仅发展了非Lipschitz光滑问题的保证优化方法，而且解决了这些交替最小化方法的二阶收敛保证的开放问题。", "label": 1, "source": "scigen_human", "idx": 324, "lang": "zh"}
{"text": "本文讨论了程序提取技术在一类新问题上的应用：通过构造正确的经典可满足性问题的决策过程的合成。为此，我们为DPLL证明系统形式化了一个完备证明，并从中提取了一个SAT求解器。当应用于联合正规形式的命题公式时，该程序产生了一个令人满意的赋值或一个显示其不可满足性的DPLL推导。我们使用非计算量词从提取的程序中删除多余的计算内容，并将其翻译为Haskell以提高性能。我们还证明了分辨率证明系统和DPLL证明系统之间的等价性，其结果的分辨率证明的大小有界。这表明，可以在证明级别上捕获关于提取的程序的定量信息。形式化是在交互式证明助手Minlog中进行的。", "label": 1, "source": "scigen_human", "idx": 325, "lang": "zh"}
{"text": "行人轨迹预测对于理解人类运动行为是有价值的，并且由于来自其他行人的社会影响、场景约束和预测轨迹的多模式可能性，它具有挑战性。大多数现有方法只关注上述三个关键要素中的两个。为了综合考虑所有这些因素，我们提出了一种新的轨迹预测方法，称为S cene G ated S social G graph（SGSG）。在所提出的SGSG中，使用动态图来描述行人之间的社会关系。通过将编码的社会图特征和语义场景特征相结合的场景门控社会图特征，考虑了社会和场景的影响。此外，结合了VAE模块来学习场景门控的社会特征，并对潜在变量进行采样，以生成社会和环境可接受的多个轨迹。我们将我们的SGSG与二十种最先进的行人轨迹预测方法进行了比较，结果表明，所提出的方法在两个广泛使用的轨迹预测基准上取得了优异的性能。", "label": 1, "source": "scigen_human", "idx": 326, "lang": "zh"}
{"text": "在一维空间中考虑了理想二值检测器的目标定位问题。在审查和非审查方案中都研究了这个问题。在截尾设置中，该问题等效于通过已知数据样本来估计均匀分布的中心。根据Lehmann-Sheffe先前的结果，它不允许MVU估计。然而，已经证明，如果检测半径是已知的，并且传感器部署区域非常大，则在对欧几里得运动不变的函数中，截尾和非截尾情况都将具有MVU估计器。此外，还表明，当检测半径未知时，截尾情况下仍然有MVU估计器，而在非截尾情况中，即使在估计量对欧几里得运动不变的假设下，MVU估计量也不存在。", "label": 1, "source": "scigen_human", "idx": 327, "lang": "zh"}
{"text": "我们研究了使用由i.i.d.标准高斯项组成的感测向量从一组秩为一的测量值估计低秩正半定（PSD）矩阵的问题，这些感测向量可能被任意异常值破坏。这个问题源于相位检索、协方差绘制、量子空间层析成像和功率谱估计等应用。我们首先提出了一种凸优化算法，该算法寻求具有观测残差的最小1-范数的PSD矩阵。我们的算法的优点是它没有参数，因此无需调整参数，并允许轻松实现。我们确定，只要测量的数量足够大，即使一小部分测量被任意大小的异常值破坏，也可以以高概率准确地恢复低秩PSD矩阵。此外，对于有界噪声，恢复也是稳定的。利用PSD矩阵秩上界的附加信息，我们提出了另一种基于次梯度下降的非凸算法，该算法在计算效率和精度方面表现出优异的经验性能。", "label": 1, "source": "scigen_human", "idx": 328, "lang": "zh"}
{"text": "我们开发了一种无透镜压缩成像架构，该架构由孔径组件和单个传感器组成，不使用任何透镜。提出了一种任意时间算法来从压缩测量中重建图像；该算法产生一系列单调收敛到真实信号的解（因此，任何时候）。该算法是基于局部重叠补丁的稀疏性（在变换域中）开发的，并获得了最先进的结果。对真实数据的实验表明，通过测量大约10个（图像像素的）压缩测量，获得了令人鼓舞的结果。将所提出的算法的重建结果与JPEG压缩（基于文件大小）进行比较，并且重建的图像质量接近JPEG压缩，特别是在高压缩率下。", "label": 1, "source": "scigen_human", "idx": 329, "lang": "zh"}
{"text": "本文使用凸优化中的标准方法，重新表述和简化了LTI系统的鲁棒稳定性和性能的核心工具。特别地，鲁棒性分析可以直接公式化为原始凸（半定规划或SDP）优化问题，使用其闭包是半定锥的Gramian集。这允许直接包括各种约束，如结构不确定性，以及直接从原始变量构建的最坏情况扰动和扰动。众所周知的结果，如KYP引理和各种规模的小增益测试，也可以直接通过标准SDP对偶获得。对于熟悉健壮性和SDP的读者来说，即使只是回顾一下，该框架也应该显得显而易见。但这也是其吸引力的一部分，应该会加强教育学，我们希望建议进行新的研究。有一个关键引理证明了gramman的闭包，这也是显而易见的，但我们目前的证明似乎不必要地繁琐，本文的最终目的是寻求鲁棒控制和凸优化专家的帮助，寻找更简单的替代方案。", "label": 1, "source": "scigen_human", "idx": 330, "lang": "zh"}
{"text": "19世纪50年代首次研究了图中的哈密顿循环。从那时起，大量的研究致力于识别允许哈密顿循环的图类，以及相关问题。相应的决策问题，即询问给定的图是否是哈密尔顿的（即允许哈密尔顿循环），是卡普著名的NP完全问题之一。它在平面三次图上保持NP完全。在本文中，我们研究了远离哈密尔顿的有界度图，其中n个顶点上的图G远离哈密尔顿，如果修改n个边的常分数是使G成为哈密尔顿的必要条件。我们展示了局部哈密尔顿的有界度图的类，即由小顶点集的邻域诱导的每个子图都出现在一些哈密尔顿图中，但它们远不是哈密尔顿的。然后，我们使用这些类来获得属性测试中的下界。我们证明了在有界度图模型中，在单边错误概率和查询复杂度为o（n）的情况下，哈密尔顿性是不可检验的。这与已知的事实形成了对比，即在平面（或较小的自由）图类上，在具有双侧误差的有界度图模型中，哈密尔顿性可以用恒定的查询复杂度来测试。我们的证明是一个复杂的构造，它展示了如何将d-正则图转化为一个远离哈密尔顿的图，并且我们使用d-正则展开图来保持局部哈密尔顿性。", "label": 1, "source": "scigen_human", "idx": 331, "lang": "zh"}
{"text": "我们解释了线性逻辑或交互几何的可实现性模型和隐式计算复杂性领域的最新发展如何导致隐式计算复杂度的新方法。这种基于语义的方法应该统一应用于各种计算范式，并能够使用新的数学方法和工具来解决计算复杂性问题。本文通过拟发展的可实现性理论，提供了这种复杂性的背景、动机和视角，并用最近的结果加以说明。", "label": 1, "source": "scigen_human", "idx": 333, "lang": "zh"}
{"text": "许多肺部疾病，如特发性肺纤维化（IPF），表现为气道扩张。准确测量扩张可以评估疾病的进展。不幸的是，图像噪声和气道分叉的组合导致横截面积轮廓的高度可变性，使得识别受影响区域变得非常困难。在这里，我们介绍了一种噪声鲁棒的方法，用于在给定在不同时间点获取的同一气道的两个轮廓的情况下自动检测进行性气道扩张的位置。我们提出了一个剖面之间突然相对变化的概率模型，并通过可逆跳马尔可夫链蒙特卡罗采样进行推理。我们在两个数据集上证明了所提出的方法的有效性；（i） 具有模拟扩张的健康气道的图像；（ii）每隔1年采集一对受IPF影响的气道的真实图像。我们的模型能够在模拟数据上以2.5mm的精度检测气道扩张的起始位置。在IPF数据集上的实验显示出与放射科医生的合理一致性。我们可以计算气道容积的相对变化，这可能有助于量化IPF疾病的进展。", "label": 1, "source": "scigen_human", "idx": 334, "lang": "zh"}
{"text": "JavaScript的日益普及导致了各种各样的JavaScript框架，旨在帮助开发人员解决编程任务。然而，JavaScript框架的数量已经迅速增加到数千个版本。从业者很难确定最适合他们需求的框架，并开发适合这些需求的新框架。此外，对于是什么驱使开发人员做出选择，还缺乏相关知识。本文探讨了导致JavaScript框架选择的因素和参与者。我们对半结构化访谈进行了定性解释研究。我们采访了18位关于JavaScript框架选择的决策者，直到达到理论饱和。通过对面试回复进行编码，我们提供了一个理想的JavaScript框架采用因素模型。这些因素被分为通过技术接受和使用统一理论得出的类别。这些因素包括性能预期（性能、规模）、努力预期（自动化、可学习性、复杂性、可理解性）、社会影响（竞争对手分析、学院建议、社区规模、社区响应性）、便利条件（适用性、更新、模块化、隔离性、可扩展性）和价格价值。由客户、开发人员、团队和团队负责人这四个参与者的组合来进行选择。我们的模型有助于形成与软件工程师采用技术相关的知识体系。作为一个实际意义，我们的模型对决策者评估JavaScript框架以及开发人员生成所需框架都很有用。", "label": 1, "source": "scigen_human", "idx": 335, "lang": "zh"}
{"text": "我们研究了在大规模多语言语料库（多语言BERT）上训练的现成的深度双向句子表示是否能够开发无监督的通用依赖解析器。这种方法只利用多种语言的单语语料库，不需要任何翻译数据，适用于低资源语言。在我们的实验中，我们在使用单个系统的情况下，在共享任务的六种真正低资源的语言中，都优于最佳的CoNLL 2018语言特定系统。然而，我们还发现，（i）当改变训练语言时，解析精度仍然存在巨大差异，以及（ii）在某些目标语言中，零样本传递在所有测试条件下都失败，这引发了对整个方法“普遍性”的担忧。", "label": 1, "source": "scigen_human", "idx": 336, "lang": "zh"}
{"text": "一个良好的状态时间量化符号抽象已经输入量化控制系统将满足三个条件：接近性，健全性和完整性。不稳定系统的符号抽象的现有方法局限于满足接近性和稳健性，而不是完整性。系统的不稳定性是为有界和量化输入不稳定系统构建完全状态时间量化符号模型的障碍，即使使用监督反馈也是如此。因此，本文通过引入“修剪输入近似双模拟”这一经典概念，提出了一种符号模型完备性的参数化方法。完整性的大小由一个称为输入轨迹集“修剪”的参数指定。随后，我们讨论了构造状态-时间量化符号模型的过程，该模型除了是健全的并且相对于时间量化模型是接近的之外，还接近于完全的。", "label": 1, "source": "scigen_human", "idx": 337, "lang": "zh"}
{"text": "在高性能有限元分析的背景下，随着模拟规模的增加，通过重新网格划分和重新启动分析迭代修改计算域的成本变得难以承受。在本文中，我们展示了一种新的交互式模拟管道，该管道针对高性能流体动力学模拟，其中计算域在原位修改，即在模拟进行时。该管道被设计为模块化的，因此它可以与任何现有的有限元模拟框架对接。采用服务器-客户端架构来管理高性能计算资源上存在的模拟网格数据，同时在单独的工作站上进行用户规定的几何修改。我们使用现有的现场可视化技术来快速通知用户模拟进展，从而实现计算指导。通过在客户端应用程序上以简化的方式表达模拟域，该管道在服务器上管理高度精细的有限元模拟域，同时在客户端应用程序上保持良好的性能。", "label": 1, "source": "scigen_human", "idx": 338, "lang": "zh"}
{"text": "我们研究了一类参数由马尔可夫链在逆时间驱动的系统。给出了二阶矩矩阵的递推性质、均方稳定性的谱半径检验和最优控制公式。我们的结果决定了这个问题：是否可以通过简单地添加马尔可夫跳跃线性系统的跳跃变量来扩展线性系统（其矩阵在对偶问题中被转置）的滤波和控制之间的经典对偶。如果跳跃过程及时逆转，答案是肯定的。", "label": 1, "source": "scigen_human", "idx": 339, "lang": "zh"}
{"text": "中国书法是一种独特的艺术形式，具有很高的艺术价值，但难以掌握。在本文中，我们将书法书写问题公式化为轨迹优化问题，并提出了一种改进的虚拟画笔模型来模拟真实的书写过程。我们的方法受到伪谱最优控制的启发，因为我们将每个行程的致动器轨迹参数化为切比雪夫多项式。所提出的动态虚拟画笔模型在制定待优化的目标函数方面起着关键作用。我们的方法在绘制美观的字符方面表现出了出色的性能，并且比以前的工作效率高得多，为实现实时闭环控制开辟了可能性。", "label": 1, "source": "scigen_human", "idx": 340, "lang": "zh"}
{"text": "在人机交互（HOI）识别中，传统方法将人体视为一个整体，并对整个身体区域给予统一的关注。他们忽略了这样一个事实，即正常情况下，人类通过使用身体的某些部位与物体进行互动。在本文中，我们认为在HOI识别中，不同的身体部位应该受到不同的关注，并且应该进一步考虑不同身体部位之间的相关性。这是因为我们的身体部位总是协同工作。我们提出了一种新的成对身体部位注意力模型，该模型可以学习关注关键部位，以及它们在HOI识别中的相关性。该模型引入了一种新的基于注意力的特征选择方法和一种可以捕捉身体部位之间成对相关性的特征表示方案。在HICO数据集上，我们提出的方法在HOI识别方面比最先进的结果实现了10个相对改进（36.1 mAP 39.9 mAP）。我们将公开我们的模型和源代码。", "label": 1, "source": "scigen_human", "idx": 341, "lang": "zh"}
{"text": "在其早期实现中，背景建模是一个用固定相机为视频背景建立模型，并识别不符合该模型的像素的过程。背景模型没有很好地描述的像素被假设为移动物体。如今，许多系统维护前景和背景的模型，并且这些模型竞相解释视频中的像素。如果前景模型更好地解释了像素，则将其视为前景。否则，他们将被视为背景。在本文中，我们认为这种进化的逻辑终点是简单地使用贝叶斯规则对像素进行分类。特别地，在每个像素处具有背景可能性、前景可能性和先验是至关重要的。然后，贝叶斯规则的一个简单应用给出了标签上的后验概率。剩下的唯一问题是组件模型的质量：背景可能性、前景可能性和先验。我们描述了一个可能性模型，该模型不仅使用给定像素位置的过去观测值，还包括该位置周围空间邻域的观测值。这使我们能够对相邻像素之间的影响进行建模，并且是对不允许这种影响的早期逐像素模型的改进。尽管在精神上与联合域范围模型相似，但我们表明，我们的模型克服了该模型中的某些不足。我们使用空间相关的先验作为背景和前景。前一帧的背景和前景标签在进行空间平滑以考虑对象的移动后，用于构建当前帧的先验。这些组件本身并不是背景建模中的新颖方面。正如我们将要展示的，许多现有的系统以不同的方式解释这些方面。我们认为，如本文所建议的那样，将这些组件分离会产生一个非常简单有效的模型。我们的直观描述还将模型组件与分类或推理步骤隔离开来。对每个模型组件的改进可以在不改变推理或其他组件的情况下进行。因此，可以有效地对各种组件进行建模，并且更容易理解它们对整个系统的影响。", "label": 1, "source": "scigen_human", "idx": 342, "lang": "zh"}
{"text": "基于逆变器的资源渗透率的增加除了传统的线性下垂控制器之外，还为我们在电力系统的频率调节方面提供了更大的灵活性。由于快速的电力电子接口，基于逆变器的资源可以用于实现复杂的控制功能，并且与线性控制器相比，有可能在性能上提供大的增益。强化学习已经成为通过将这些非线性控制器参数化为神经网络来寻找这些非线性控制器的流行方法。基于学习的方法的关键挑战是难以在学习的控制器上实施稳定性约束。此外，由于电力系统的时间耦合动力学，优化控制器是不重要的。在本文中，我们建议明确地设计基于神经网络的控制器的结构，使其保证所有拓扑和参数的系统稳定性。这是通过使用李雅普诺夫函数来指导它们的结构来实现的。使用基于递归神经网络的强化学习结构来有效地训练控制器的权重。所得到的控制器仅使用局部信息，并且优于线性下垂以及纯粹通过使用强化学习学习的策略。", "label": 1, "source": "scigen_human", "idx": 344, "lang": "zh"}
{"text": "知识提炼旨在通过从更大的模型中转移知识来获得一个小而有效的深层模型。以前的方法试图通过简单的“logit监督”教师和学生之间的信息传递来实现这一目标，这种信息传递在某种程度上可以分解为归一化logits和l2范数的传递。我们认为logits的规范实际上是干扰，它损害了传递过程的效率。为了解决这个问题，我们提出了球面知识蒸馏（SKD）。具体来说，我们将教师和学生的logits投影到一个单位球体中，然后我们可以在该球体上有效地进行知识提炼。我们通过理论分析和消融研究来验证我们的论点。大量的实验已经证明了我们的方法相对于SOTA的优越性和可扩展性。", "label": 1, "source": "scigen_human", "idx": 345, "lang": "zh"}
{"text": "针对局部不连续Galerkin（LDG）方法离散的高阶精确Stokes问题，提出了一种快速的多重网格求解器。多重网格算法由一个简单的V循环组成，使用逐元素块高斯-塞德尔平滑器。该方法的有效性取决于LDG压力惩罚稳定参数——只要参数选择得当，数值实验表明：（i）对于稳态Stokes问题，多重网格求解器的收敛速度可以与泊松问题的经典几何多重网格方法相匹配；（ii）对于非定常Stokes问题，随着有效雷诺数的增加，收敛速度进一步加快。广泛的二维和三维测试问题证明了求解器的性能以及高阶精度，包括具有周期性、狄利克雷和应力边界条件的情况；包含几个数量级的密度和粘度不连续性的可变粘度和多相嵌入界面问题；以及使用半非结构化网格的具有弯曲几何形状的测试用例。", "label": 1, "source": "scigen_human", "idx": 346, "lang": "zh"}
{"text": "设计复杂神经网络架构的能力，使其能够通过随机梯度下降进行有效训练，这是深度学习领域取得许多成就的关键。然而，开发这样的体系结构仍然是一个充满挑战和资源密集型的过程，充满了试错迭代。总而言之，网络拓扑结构与其数据建模能力之间的关系仍知之甚少。我们建议用笛卡尔遗传规划（dCGPANN）的可微变体对神经网络进行编码，并提出一种用于架构设计的模因算法：具有梯度下降的局部搜索学习网络参数，而进化算子作用于dCGPANN基因，从而使网络架构朝着更快的学习方向发展。通过研究这种学习方案的特定实例，我们能够通过学习如何重新布线和修剪链接、调整激活函数以及为所选回归任务引入跳过连接来改进起始前馈拓扑。在相同的时间量下，演进的网络架构需要更少的网络参数空间和覆盖范围，平均误差显著更低。", "label": 1, "source": "scigen_human", "idx": 347, "lang": "zh"}
{"text": "众所周知，由神经网络方法（如word2vec（W2V））生成的单词嵌入表现出看似线性的行为，例如，“女人对女王，就像男人对国王”的类比嵌入近似描述了平行四边形。这个特性特别有趣，因为嵌入并没有经过训练来实现它。已经提出了几种解释，但每种解释都引入了在实践中不成立的假设。我们推导了一个基于概率的转述定义，我们将其重新解释为单词转换，即“w x就是w y”的数学描述。从这些概念中，我们证明了W2V型嵌入之间存在线性关系，这是类比现象的基础，识别了显式误差项。", "label": 1, "source": "scigen_human", "idx": 348, "lang": "zh"}
{"text": "机器对问题的理解与底层处理算法的计算能力背景下的衔接识别密切相关。本文提出了一个数学模型来捕捉和区分问题表达中的潜在结构。我们提出了一种目标驱动的方法来表示这种潜在的结构，并表明当没有互补目标的例子时，这种方法是有益的。我们证明了潜在结构可以表示为一个最大化与潜在目标相关的成本函数的系统。此外，我们证明了优化公式可以近似于构建表示为训练的神经自动编码器的模式记忆。使用许多问题集群进行的实验评估显示，在这些问题集群中，每个问题都与一个目标有关，识别准确率为80，误报率可忽略不计。然后，我们将相同的记忆扩展到相关任务，其中的目标是基于潜在的表达迭代地精化问题数据集。我们还展示了一种称为K-指纹的细化方案，该方案在不同的问题集群中实现了近100个识别，误报率可以忽略不计。", "label": 1, "source": "scigen_human", "idx": 349, "lang": "zh"}
{"text": "我们提出了异步通知的多智能体认知逻辑，其中真实的通知是公开发送的，但由代理单独接收，并按照发送的顺序进行。除了认知模态之外，逻辑还包含发布公告和接收公告的动态模态。代理人所相信的是她最初的不确定性和她收到的公告的函数。信念不必是真实的，因为已经发布的公告可能还没有收到。只要公告在发送时是真实的，就可以排除某些消息序列，就像分布式计算中不一致的剪切一样。我们为这个emph{异步宣告逻辑}（AA）提供了一个完整的公理化。它是一个约简系统，也证明了AA中的任何公式都等价于没有动态模态的公式，就像公告逻辑一样。模型检查的复杂性在PSPACE中。一个详细的例子模拟了AA中分布式计算中的消息交换过程，结束了我们的研究。", "label": 1, "source": "scigen_human", "idx": 350, "lang": "zh"}
{"text": "5G及其他无线连接的容量和覆盖要求将与前代网络有很大不同。为了满足这些要求，预计英国的部署成本将在300亿至500亿之间，而移动网络运营商（MNO）目前的年度资本支出（CapEX）为25亿。这一前景产生了巨大影响，并成为5G物理基础设施建设的主要延迟因素之一，而5G发展的其他领域正在以其速度发展。由于物理网络基础设施和频谱的昂贵和复杂性质，第二层运营商，即众所周知的移动虚拟网络运营商（MVNO），完全依赖于MNO。在本文中，进行了广泛的研究，以探索降低5G部署成本和开发商业模式的可能性。这项研究表明，使用现有的公共基础设施（如路灯、电线杆等）有很大的潜力，有助于将预期成本降低约40至60。本文还回顾了英国通信管理局最近以名义成本发布5G兼容无线电频谱基于位置的许可证的举措。我们的研究表明，基础设施和频谱的简化将鼓励特定场景蜂窝网络的指数级增长，并可能破坏电信业务利益相关者的当前商业模式，特别是MNO和TowerCos。这些特定场景的网络预计是：a）专用网络，b）社区网络，以及c）微型运营商。此外，由于5G实现密集设备连接的可行性，传统和非传统数据可用性的分辨率将显著提高。这将鼓励将广泛的数据收集作为一种商业机会，并在中小企业和大型社交网络中发挥作用。因此，新的基础设施和频谱利益相关者的崛起是意料之中的事。这将推动5G数据交换生态系统的发展，在该生态系统中，数据交易被视为高价值的商业商品。这些数据的隐私和安全，以及相关收入模式和所有权的定义，都是具有挑战性的领域，这些领域尚未完全出现和成熟。在这个方向上，本文建议开发一个统一的数据中心，该中心具有分层的结构化隐私和安全，以及区块链和加密的基于链下的所有权跟踪。同时，提出了一种面向数据经济的业务模型。研究发现，随着数据和数据交易的潜在商品化，以及低成本的物理基础设施和频谱，5G网络将对电信业务生态系统造成重大破坏。", "label": 1, "source": "scigen_human", "idx": 351, "lang": "zh"}
{"text": "在过去的几年里，商业开放世界游戏中非玩家角色的高级人工智能质量一直在提高。然而，由于游戏行业的特定限制，这一增长一直很慢，而且是由更大的预算而不是采用新的复杂人工智能技术推动的。当代大多数人工智能仍然以硬编码脚本的形式表达。脚本代码库的复杂性和可管理性是进一步改进人工智能的关键限制因素之一。在本文中，我们将讨论这个问题。我们提出了行为对象——一种开发大型OWG NPC行为的通用方法。行为对象受到面向对象编程的启发，并扩展了智能对象的概念。我们的方法促进将多个相关行为的数据和代码封装在一个地方，隐藏内部细节并将智能嵌入环境中。行为对象是五种不同技术的自然抽象，我们已经在即将推出的AAA OWG中实现了这些技术来管理人工智能的复杂性。我们在行为树的上下文中报告了实现的细节以及在开发过程中吸取的经验教训。我们的工作应该为学术界和行业的人工智能建筑设计师提供灵感。", "label": 1, "source": "scigen_human", "idx": 352, "lang": "zh"}
{"text": "这项研究以德语等低资源语言为例，将神经命名实体识别的性能提高了11分，从而优于现有基线，并在每个开源数据集上建立了新的最先进技术。我们不是设计更深入、更广泛的混合神经架构，而是收集所有可用资源，在将原始数据暴露于任何训练过程之前，进行详细的优化和语法相关的形态学处理，包括引理化和词性标记。我们在a）单一、b）联合和c）优化训练的三重单语实验设置中测试了我们的方法，并阐明了下游任务对用于计算单词嵌入的语料库大小的依赖性。", "label": 1, "source": "scigen_human", "idx": 353, "lang": "zh"}
{"text": "在教育和医疗保健等批量强化学习的应用中，有必要从观察数据中对顺序决策策略进行政策外评估。然而，在这种情况下，观察到的行动往往与未观察到的变量的转变相混淆，导致无法准确评估新政策，即无法识别。我们开发了一种稳健的方法，根据灵敏度模型，在给定来自另一个具有未观察到的混杂的政策的数据的无限时域问题中，估计给定政策的（不可识别的）值的尖锐边界。我们将该问题精确地表述为计算与数据和灵敏度模型一致的所有稳态占有率集合的支持函数。我们展示了如何使用一个新的部分识别的估计方程来表达这个集合，并在我们收集更多混杂数据时证明收敛到锐界。我们证明了集的隶属度可以通过求解线性规划来检查，而支持函数是由一个困难的非凸优化问题给出的。我们利用有限状态空间情况的解析解来开发基于非凸投影梯度下降的近似。我们实证地证明了结果的边界。", "label": 1, "source": "scigen_human", "idx": 354, "lang": "zh"}
{"text": "当必须在给定环境中部署大量对象（例如，机器人、传感器等）时，通常需要规划对象从其初始位置到具有某些全局特性的最终配置的协调运动。在这种情况下，最小化行驶距离的某些函数，从而最小化能量消耗的问题至关重要。在本文中，我们研究了当对象必须在图上移动时出现的几个运动规划问题，以达到某些网络应用程序感兴趣的目标。除其他目标外，这些目标包括广播消息和形成连接或无干扰的网络。我们研究这些问题的目的是尽量减少一些自然测量，如平均总行驶距离、最大行驶距离或需要移动的物体数量。在这方面，我们提供了几个近似和不近似的结果，其中大多数是紧的。", "label": 1, "source": "scigen_human", "idx": 355, "lang": "zh"}
{"text": "递归神经网络（RNN）是流行的动力学模型，用于处理序列数据的机器学习，也用于神经科学，以了解真实神经元网络的突发动力学特性。大多数理解RNN性质的理论工作都集中在具有加性相互作用的模型上，其中单元的输入是网络中剩余单元输出的加权和。然而，有充分证据表明神经元可以具有门控（即乘法）相互作用。这种门控交互对网络的集体动力学有显著影响。此外，机器学习中性能最好的RNN也有门控交互。因此，门控交互以有利于信息处理和学习任务的方式显著地塑造了动态。在这项工作中，我们在正则神经网络模型中发展了门控的动态平均场理论（DMFT），以理解门控产生的定性不同的动力学状态。我们的门控RNN在一定限度内简化为经典的加性RNN，并与机器学习中流行的门控模型密切相关。我们使用非厄米随机矩阵理论（RMT）的技术来分析表征瞬时雅可比的频谱，并展示门控如何产生慢模和边缘稳定性。因此，门控是实现涉及线吸引子动力学的计算的潜在机制。利用其李雅普诺夫谱研究了门控网络的长时间行为，并使用DMFT对最大李雅普ov指数进行了分析预测。提供了使用RMT的最大李雅普诺夫指数的另一种推导方法，该方法表明了李雅普ov指数与动力学弛豫时间之间的密切联系。我们还表明，门控产生了一种新的、不连续的混沌过渡，其中临界点的扩散与混沌动力学的出现解耦，这与Wainrib Touboul对加性RNN的结果相反；详细描述了这种混沌状态的性质。使用DMFT和RMT，我们生成门控RNN的相图，识别参数空间中的临界表面和边缘稳定性区域。最后，我们通过将控制理论中的伴随灵敏度框架结合到Martin Siggia-Rose场论中，来解决训练RNN中出现的梯度，以开发梯度的DMFT。这里发展的理论揭示了门控交互产生的丰富的动力学行为，并对基于梯度信号的建筑选择和学习动力学具有启示。", "label": 1, "source": "scigen_human", "idx": 356, "lang": "zh"}
{"text": "深度域自适应的目标是使在一个域中训练的深度网成为可能，其中在另一个域几乎没有或根本没有注释的训练数据。当前的大多数方法都专注于学习特征表示，这些特征表示对从一个域到另一个域时发生的变化是不变的，这意味着在两个域中使用相同的网络参数。虽然最近的一些算法通过调整网络参数来明确地对变化进行建模，但它们要么严重限制了可能的域变化，要么显著增加了模型参数的数量。相比之下，我们引入了一种包括辅助残差网络的网络架构，我们对其进行训练，以预测域中的参数，而与另一个域中的那些参数相比，几乎没有注释数据。这种架构使我们能够灵活地保留域之间存在的相似性，并在必要时对差异进行建模。我们证明，我们的方法在没有过度复杂的情况下，比最先进的方法产生了更高的准确性。", "label": 1, "source": "scigen_human", "idx": 357, "lang": "zh"}
{"text": "我们设计了一个基于Mask区域的卷积神经网络（Mask R-CNN）框架，用于自动检测并从图像中分别提取蚊子的胸部、翅膀、腹部和腿部的解剖成分。我们的训练数据集由1500张被困在佛罗里达州的九种蚊子的智能手机图像组成。在所提出的技术中，第一步是检测蚊子图像中的解剖成分。然后，我们对提取的解剖成分进行定位和分类，同时在神经网络架构中添加一个分支来分割仅包含解剖成分的像素。评价结果良好。为了评估通用性，我们在大黄蜂图像上测试了仅使用蚊子图像训练的架构。我们再次揭示了有利的结果，特别是在提取翅膀方面。本文中的技术在公共卫生、分类学和公民科学工作中有实际应用。", "label": 1, "source": "scigen_human", "idx": 358, "lang": "zh"}
{"text": "包括人的概念、属性和关系的人本体在数据保护、去识别、商业智能和欺诈预防的知识图谱群体中有许多应用。虽然人工神经网络在实体识别、实体分类和关系提取方面取得了进步，但创建本体在很大程度上仍然是一个手动过程，因为它需要概念之间的一组固定的语义关系。在这项工作中，我们提出了一个使用神经模型进行实体分类和关系提取的系统，用于从非结构化数据中自动填充个人本体图。我们为这些任务引入了一个新的数据集，并讨论了我们的结果。", "label": 1, "source": "scigen_human", "idx": 359, "lang": "zh"}
{"text": "安全研究人员表示，当前访问控制实现背后的核心概念早于互联网。做出这些断言是为了指出这一领域存在根本性的差距，人们应该考虑从头开始重新审视这些概念。内部威胁是针对组织的日益增长的威胁媒介，与访问控制的失败有关。对维基解密等相关数据泄露事件的深入分析，进一步激发了制定新颖有效对策的开箱即用思维。从访问控制矩阵派生出的访问控制模型包括三组实体，主体、对象和操作。通常，对象被视为文件，操作被视为读取、写入和执行。这意味着在授予对数据的访问权限时采用“开放芝麻”方法，即一旦授予访问权限，就不会限制命令的执行。受功能加密的启发，我们建议以更精细的粒度应用访问授权，但我们假设对访问控制进行基本转换，而不是特别的或计算困难的加密方法。从抽象的角度来看，我们建议将访问权限存储为三维张量，我们称之为访问控制张量（ACT）。在基于功能的访问控制（FBAC）中，应用程序不提供盲折叠执行权限，只能调用已授权用于数据段的命令。换言之，一个人可能被授权在一个对象上使用某个命令，而被禁止在另一对象上使用完全相同的命令。显然，这种行为不能使用经典的访问控制矩阵进行建模。本文介绍了FBAC的理论基础及其政策、执行和实现（PEI）要求。还包括对部署FBAC的优势、它将如何导致开发新一代应用程序以及与现有模型和系统的兼容性的关键分析。最后，给出了FBAC的概念验证实现。", "label": 1, "source": "scigen_human", "idx": 360, "lang": "zh"}
{"text": "集成学习是一种将多种算法相结合的机器学习范式，在各种任务中表现出了良好的性能。目前的工作重点是无监督的集合分类。术语无监督是指不知道每个分类器所训练的基本事实标签的集合组合器。虽然大多数先前关于无监督集合分类的工作都是为独立和同分布（i.i.d.）数据设计的，但本工作介绍了一种无监督方案，用于在存在数据依赖性的情况下从分类器集合中学习。考虑了两种类型的数据依赖关系：顺序数据和依赖关系由图捕获的网络数据。针对上述情况开发了矩匹配和期望最大化算法，并在合成和真实数据集上评估了它们的性能。", "label": 1, "source": "scigen_human", "idx": 361, "lang": "zh"}
{"text": "Marcello在1997年正式证明了化学动力学可以制造一台通用计算机，即它们可以复制任何数字电路。最近，Soloveichik等人证明了化学动力学可以进行快速可靠的图灵通用计算。为了模拟化学反应的行为，Sean等人开发了一个名为CAIN的软件，该软件以XML格式表示化学反应。在这项工作中，我们尝试创建一个trans编译器，它可以以类似python的代码作为输入，并提供CAIN支持的化学反应文件作为输出。这可以与从高级编程语言生成汇编代码进行比较。此外，Soloveichik等人还展示了DNA作为实现CRN的通用引物，Andrews Phillips等人开发了用于模拟所有可能的DSD反应的可视化DSD编程语言。Manish Gupta团队开发的CRN2DSD软件已经可以将CAIN文件转换为Microsoft的Visual DSD代码，即组装级到机器级。因此，我们试图将高级代码转换为汇编代码，这使我们离实现化学编译器的梦想又近了一步。", "label": 1, "source": "scigen_human", "idx": 362, "lang": "zh"}
{"text": "比较文本挖掘从类型分析和政治偏见检测延伸到文化和地理差异的揭示，再到专利和科学论文中的现有技术搜索。这些应用程序使用跨集合主题建模来探索、聚类和比较大型文档集，如数字图书馆。然而，由于领域特定的词汇，对来自不同集合的文档进行主题建模是具有挑战性的。我们提出了一个结合自动领域术语提取和短语分割的跨集合主题模型。该模型基于信息熵区分集合专有词和集合无关词，揭示了多个文本集合的共性和差异。我们根据专利、科学论文、报纸文章、论坛帖子和维基百科文章来评估我们的模型。与最先进的跨集合主题建模相比，我们的模型实现了高达13个主题连贯性、低达4个困惑度和高达31个文档分类精度。更重要的是，我们的方法是第一个确保一般和特定单词分布不连续的主题模型，从而产生清晰的主题表示。", "label": 1, "source": "scigen_human", "idx": 363, "lang": "zh"}
{"text": "跨项目缺陷预测（CPDP）在估计最有可能出现缺陷的软件组件方面发挥着重要作用，尤其是对于新项目或非活动项目。据我们所知，很少有先前的研究就如何从大量公共软件存储库中选择合适的高质量训练数据提供明确的指导。在本文中，我们提出了一种用于实际CPDP的训练数据简化方法，该方法考虑了数据集的多个粒度级别和过滤策略。此外，我们还提供了从缺陷倾向率角度选择合适过滤器的定量证据。基于对10个开源项目的34个版本的实证研究，我们使用不同粒度简化的训练数据和两个流行的滤波器，精心比较了用五个著名分类器构建的不同缺陷预测因子的预测性能。结果表明，当使用带有适当滤波器的多粒度简化方法时，基于朴素贝叶斯的预测模型可以获得相当好的性能，并且优于基准方法。", "label": 1, "source": "scigen_human", "idx": 364, "lang": "zh"}
{"text": "我们提出了一种堆叠多个长短期记忆（LSTM）层来建模句子的方法。与仅将隐藏状态作为输入提供给下一层的传统堆叠LSTM相比，所建议的架构接受前一层的隐藏状态和存储单元状态，并使用LSTM的软门控机制融合来自左下上下文的信息。因此，该体系结构不仅在水平递归中而且在垂直连接中调制要传递的信息量，从垂直连接中从较低层提取的有用特征被有效地传递到上层。我们将这种架构称为Cell-aware Stacked LSTM（CAS-LSTM），并从实验中表明，在自然语言推理、转述检测、情感分类和机器翻译的基准数据集上，我们的模型比标准LSTM带来了显著的性能提升。我们还进行了广泛的定性分析，以了解建议方法的内部行为。", "label": 1, "source": "scigen_human", "idx": 365, "lang": "zh"}
{"text": "在本文中，我们研究了一个大型智能表面增强（LIS增强）系统，其中部署了LIS来帮助安全传输。我们的设计旨在最大限度地提高不同信道模型中可实现的保密率，即合法信道和窃听信道的Rician衰落和（或）独立且同分布的高斯衰落。此外，我们还考虑了一种人工噪声辅助传输结构，以进一步提高系统性能。解决上述问题的困难是期望保密率表达式的结构和非凸相移约束。为了便于设计，我们提出了两个框架，即基于样本平均近似（SAA）的算法和混合随机投影梯度收敛策略（混合SPG-CP）算法，来计算保密率表达式中的期望项。同时，针对相移约束的非凸性，采用了优化极小化方法。此外，我们还充分利用期望项对两种特殊情况进行了分析。仿真结果表明，所提出的算法有效地优化了所考虑设置的保密通信速率，与没有LIS的传统架构相比，LIS增强系统大大提高了保密性能。", "label": 1, "source": "scigen_human", "idx": 366, "lang": "zh"}
{"text": "即使是最复杂的分类器也无法区分视觉上相似的物体，如伪造的真实钞票和健康的植物。我们建议使用多路照明来扩展可以成功分类的对象的范围。我们构建了一个紧凑的RGB-IR光台，在光源位置和颜色的不同组合下对样本进行成像。然后，我们开发了一种方法来选择照明模式并使用生成的图像训练分类器。我们使用光台对训练样本进行建模和综合重新照明，并提出了一种贪婪模式选择方案，利用这种能力在模拟中进行训练。然后，我们应用训练后的模式对新对象进行快速分类。我们在视觉上相似的人工和真实水果样本上演示了该方法，与固定光源方法以及更传统的代码选择方案相比，该方法有了显著的改进。这项工作允许对以前无法区分的物体进行快速分类，并有可能应用于伪造检测、农业和制造业的质量控制以及皮肤损伤分类。", "label": 1, "source": "scigen_human", "idx": 367, "lang": "zh"}
{"text": "我们研究了回报冲击对大量近视玩家进化的影响，这些玩家采用了简单的策略修正协议，如“模仿成功”。在无噪声的情况下，这一过程由标准（确定性）复制器动力学控制；然而，在存在噪声的情况下，诱导的随机动力学不同于以前版本的随机复制器动力学（如，的集合冲击模型）。在这种情况下，我们证明了严格均衡总是随机渐近稳定的，与冲击的大小无关；另一方面，在高噪声条件下，非平衡态也可能随机渐近稳定，支配策略可能永远存在（如果噪声较低，它们就会灭绝）。如果玩家不那么短视，并根据他们的累积收益修改策略，这种行为就会被消除。在这种情况下，我们得到了一个二阶随机动力系统，其吸引态与博弈的严格均衡一致，并且无论噪声水平如何，主导策略都会灭绝。", "label": 1, "source": "scigen_human", "idx": 368, "lang": "zh"}
{"text": "本文提出了一个用于民歌主题学习的分布式矢量表示模型。具有负采样的word2vec的跳格版本用于表示高质量嵌入。根据余弦相似性对埃森民歌集的主题进行比较。提出了一种新的基于旋律相似性任务的嵌入质量评估方法，以展示向量空间如何表示复杂的上下文特征，以及如何将其用于民歌变异的研究。", "label": 1, "source": "scigen_human", "idx": 369, "lang": "zh"}
{"text": "当被问及时，大多数人认为，作为行人，他们在做出过街决定时会与驶近车辆的司机进行眼神交流。这项工作提供了证据，证明这种广泛持有的信念是错误的。我们通过表明，在大多数可能发生冲突的情况下，行人早在能够透过挡风玻璃看到司机之前就开始横穿马路。换言之，我们能够绕过行人是否选择与司机进行眼神交流这一非常困难的问题，通过表明无论他们认为自己是否选择，他们都不能。具体而言，我们表明，在具有代表性的照明条件下，超过90的人在15米处无法确定司机的视线，在30米处根本无法看到司机。这意味着，例如，在普通城市限速25英里/小时的情况下，超过99名行人在看到司机或司机的视线之前就已经开始过马路了。换言之，从行人的角度来看，在涉及驶近车辆的大多数情况下，行人仅根据车辆的运动学来做出过街决定，而不需要通过明确检测驾驶员的眼睛来确定是否进行了眼神交流。", "label": 1, "source": "scigen_human", "idx": 370, "lang": "zh"}
{"text": "优先级知识库中的不一致性是因为断言（ABoxes）来自具有不同可靠性级别的多个来源。我们介绍了对这个不一致问题的处理，以查询不一致的DL-Lite知识库。在文献中，首先，修复DL-Lite的不一致知识库中所有不一致的断言。然后，对其进行询问。然而，我们的算法在上直接对知识库进行询问，以恢复给定查询的详尽答案列表。第二次，修复此列表的答案。我们文章的新颖之处在于提出了一个递归函数，该函数计算一致性等级，以管理响应集中的不一致性。与现有算法相比，这种策略使我们能够减少执行时间。我们进行的实验研究和对结果的分析表明，我们的算法比其他算法更有效率，因为它给出了最多的答案，同时从执行时间的角度来看保持最佳。最后，如我们的实验研究所示，它们可以有效地处理不一致性。这些事实使得所有的维修都适合DL Lite。", "label": 1, "source": "scigen_human", "idx": 371, "lang": "zh"}
{"text": "我们为持久同源性建立了一个函数管道。该流水线的输入是由任何有限格索引的滤波的单纯复形，输出是定义为某个单调积分函数的莫比乌斯反演的持久图。我们将Landi等人的Reeb图编辑距离适配到我们的每个类别，并证明我们的管道中的两个函子都是1-Lipschitz，使我们的管道稳定。", "label": 1, "source": "scigen_human", "idx": 372, "lang": "zh"}
{"text": "现有的临床决策支持系统（CDSS）在很大程度上依赖于结构化患者数据和电子健康记录（EHR）的可用性来帮助护理人员。然而，就发展中国家的医院而言，结构化的患者数据格式并没有被广泛采用，那里的医疗专业人员仍然依赖非结构化文本形式的临床笔记。医务人员记录的这种非结构化临床记录也可能是丰富的患者特定信息的潜在来源，这些信息可用于构建CDSS，即使对于发展中国家的医院也是如此。如果可以使用这种非结构化的临床文本，将不再需要手动且耗时的EHR生成过程，从而节省大量的工时和成本。在本文中，我们提出了一种通用的ICD9疾病组预测CDSS，该CDSS建立在使用混合词嵌入建模的非结构化医生笔记上。这些单词嵌入用于训练深度神经网络，以有效预测ICD9疾病组。实验评估表明，所提出的方法在AUROC方面比建立在结构化EHR上的最先进的疾病组预测模型好15倍，在AUPRC方面好40倍，从而证明了我们的假设，消除了对结构化患者数据可用性的依赖。", "label": 1, "source": "scigen_human", "idx": 373, "lang": "zh"}
{"text": "我们研究了单位圆盘图上的Steiner树问题。给定一个n顶点单位圆盘图G、t个顶点的子集R V（G）和一个正整数k，目标是确定G中是否存在一个树t，它跨越R的所有顶点，并使用来自V R的最多k个顶点。R的顶点被称为终端，V（G）R的顶点称为Steiner顶点。首先，我们证明了这个问题是NP难的。接下来，我们证明了单位圆盘图上的Steiner树问题可以在nO（tk）时间内求解。我们还证明了用k参数化的单位圆盘图上的Steiner树问题具有一个运行时间为2O（k）nO（1）的FPT算法。事实上，这些算法是为一类更通用的图设计的，称为团网格图。我们提到，算法结果可以用于具有有界纵横比的盘图上的Steiner树。最后，我们证明了k参数化的盘图上的Steiner树是W[1hard。", "label": 1, "source": "scigen_human", "idx": 374, "lang": "zh"}
{"text": "人工尖峰神经网络已经在激活的时间特性提供优势的领域找到了应用，例如时间序列预测和信号处理。为了提高效率，尖峰架构通常在定制设计的神经形态硬件上运行，但是，尽管它们具有吸引人的特性，但这些实现仅限于数字系统。我们描述了一种人工量子尖峰神经元，它依赖于两个易于实现的哈密顿量的动态演化和随后的局部测量。该架构允许利用测量的复杂振幅和反作用来影响输入。这种学习协议的方法在系统的输入和输出都是量子态的情况下是有利的。我们通过贝尔对的分类来证明这一点，贝尔对可以被视为一种认证协议。将引入的基本构建块堆叠到更大的网络中，将尖峰神经网络的时空特征与图中的非局部量子相关性相结合。", "label": 1, "source": "scigen_human", "idx": 375, "lang": "zh"}
{"text": "由于今天的科学模拟产生了大量的数据，允许用户控制信息丢失的有损压缩可以显著减少数据大小和IO负担。然而，对于大规模宇宙学模拟，如HardwareHybrid加速宇宙学代码（HACC），其中存储器开销约束将压缩限制为一次只能压缩一个快照，由于数据的空间相干性相当低和高度不规则性，有损压缩比极为有限。在这项工作中，我们提出了一种模式匹配（相似性搜索）技术来优化SZ有损压缩器对HACC数据集的预测精度和压缩比。我们用不同的配置评估了我们提出的方法，并将其与最先进的有损压缩机进行了比较。实验表明，与SZ相比，我们提出的优化方法可以提高预测精度并减小量化码的压缩大小。我们为未来的有损压缩模式匹配技术研究提供了一些有益的经验。", "label": 1, "source": "scigen_human", "idx": 376, "lang": "zh"}
{"text": "抽象论证最突出的工具之一是邓的框架，简称AF。它伴随着各种语义，包括基础语义、完整语义、首选语义和稳定语义。AF虽然强大，但也有其缺点，这导致了许多富集的发展。其中最普遍的是抽象辩证框架，也称为ADFs。它们利用所谓的接受条件来表示任意关系。这种抽象级别不仅带来了新的挑战，而且需要解决该领域中现有的问题。最具争议的问题之一，不仅在论证中得到承认，涉及支持周期。在本文中，我们介绍了一种新的方法来确保所选参数的非循环性，并在此基础上提出了一系列基于扩展的语义。我们还继续研究允许循环的语义，并填补了以前工作中的空白。此外，我们还提供了Dung设置中已知属性的ADF版本。最后，我们还介绍了已开发的子语义的分类，并将它们与现有的基于标记的方法联系起来。", "label": 1, "source": "scigen_human", "idx": 377, "lang": "zh"}
{"text": "随机梯度哈密顿蒙特卡罗（SGHMC）是随机梯度下降的动量版本，适当注入高斯噪声以找到全局最小值。在本文中，在非凸优化的背景下，给出了SGHMC的非渐近收敛性分析，其中在i.i.d数据集上使用子采样技术进行梯度更新。我们的结果是对的结果的补充和改进。", "label": 1, "source": "scigen_human", "idx": 378, "lang": "zh"}
{"text": "如今，互联网是获取健康信息的主要来源。大规模的假健康新闻在互联网上传播，已经成为对公众健康的严重威胁。在假新闻检测领域已经做了大量的研究和工作，但很少有研究和工作是为了应对健康新闻中的挑战。例如，假健康新闻检测需要开发可解释的软件。为了缓解这些问题，我们构建了一个全面的知识库FakeHealth，其中包括具有丰富功能的新闻内容、具有详细解释的新闻评论、社交活动和用户-用户社交网络。此外，我们还进行了探索性分析，以了解数据集的特征，分析有用的模式，并验证数据集的质量，用于健康假新闻检测。我们还讨论了健康假新闻检测的新的和潜在的未来研究方向。", "label": 1, "source": "scigen_human", "idx": 379, "lang": "zh"}
{"text": "医院获得性感染是指患者在住院期间发生的感染，但在入院时并不存在。它们是世界各地医疗保健中最常见的不良事件之一，导致死亡率和发病率增加，住院时间延长，给医院和患者带来巨大的经济负担。已经制定了预防性准则和条例，但遵守这些准则和条例的情况往往很差，还有很大的改进空间。本文介绍了在欧盟资助下开发的可扩展、可配置的网络物理系统的原型，该系统将有助于预防医院感染和疫情。我们的解决方案将用于监测临床过程的无线传感器网络与围绕工作流引擎构建的可配置监测软件集成为关键组件，可检测与既定卫生实践的偏差，并在发现感染风险时提供实时信息和警报。该平台从硬件和软件两个角度进行了描述，重点介绍了无线网络的元素以及最重要的软件组件。此外，还详细介绍了系统原型中包含的两个不同复杂度的临床工作流程。最终确定的系统预计将有助于创建和自动监测与90多例医院感染相关的临床工作流程。", "label": 1, "source": "scigen_human", "idx": 380, "lang": "zh"}
{"text": "识别成功的不确定性、连接复杂性的不利缩放或对复杂外部输入的依赖性削弱了当前振荡神经网络用于模式识别的有用性，或将技术实现限制在小型网络上。我们提出了一种新的用于模式识别的耦合振荡器网络架构，该架构没有显示出上述缺陷。此外，我们用仿真结果说明了识别过程，并分析了新的动力学：可能的输出模式是系统的孤立吸引子。此外，识别成功的简单标准来自吸引力盆地的下限。", "label": 1, "source": "scigen_human", "idx": 381, "lang": "zh"}
{"text": "建设性反馈是提高批判性思维能力的有效方法。反驳（CA）是一种建设性反馈形式，已被证明对批判性思维技能有用。然而，在构建大规模的CA语料库方面，很少有人做过工作，该语料库可以推动错误微观层面论点（即单个声明和前提对）的CA自动生成研究。在这项工作中，我们将提供建设性的反馈视为一项自然语言处理任务，并创建了Riposte，一个CA语料库，朝着这个目标前进。众包制作，雷波斯特！包含超过18k个CA。我们指导工作人员首先识别常见的谬论类型，并生成识别谬论的CA。我们分析了员工如何创建CA，并基于我们的分析构建了基线模型。", "label": 1, "source": "scigen_human", "idx": 382, "lang": "zh"}
{"text": "本文介绍了microPhantom，一个玩microRTS并参加2020年microRTS AI比赛的机器人。microPhantom是基于我们之前的机器人POAdaptive，该机器人赢得了2018年和2019年microRTS AI比赛的部分可观测轨道。在本文中，我们通过使用基于约束规划和决策理论相结合的方法来解决单位生产问题，来关注不确定性下的决策。我们表明，使用我们的方法来决定训练哪些单元，可以显著提高与部分可观察轨迹中第二好的microRTS机器人的胜率。我们还证明了我们的方法在混沌环境中是有弹性的，只会有很小的效率损失。为了实现可复制性并促进进一步的研究，提供了microPhantom的源代码以及它使用的约束编程工具包。", "label": 1, "source": "scigen_human", "idx": 383, "lang": "zh"}
{"text": "在本文中，我们提出了一种新的方法，称为WaterFowl，用于存储RDF三元组，该方法解决了大数据和语义网环境中的一些关键问题。我们原型的架构主要基于简洁的数据结构的使用，能够以自索引、紧凑的方式表示三元组，而不需要在查询应答时进行解压缩。此外，由于对本体概念和属性进行了优化编码，不需要完全的推理物化或广泛的查询重写算法，因此它适合于有效地支持RDF和RDFS隐含机制。这种方法意味着在数据准备过程的早期对知识库的术语和组成部分进行区分。e在将数据存储在我们的结构中之前对其进行预处理。本文描述了该系统的完整架构，并介绍了对我们的第一个原型进行评估所获得的一些初步结果。", "label": 1, "source": "scigen_human", "idx": 384, "lang": "zh"}
{"text": "在本文中，我们研究了在源人工噪声（SAN）的基础上使用目的地人工噪声（DAN）来增强物理层的保密性，并提出了一种基于中断概率的方法。假设网络中的所有节点（即源、目的地和窃听者）都配备了多个天线。此外，窃听者是被动的，其信道状态和位置在源和目的地都是未知的。在我们提出的方案中，通过优化SAN、DAN和数据信号的功率分配，在窃听器处保证了中断概率的最小值，同时在目的地处确保了一定水平的信噪比。我们的仿真结果表明，与仅采用SAN来实现窃听者相同的中断概率的方法相比，将DAN与SAN一起使用会显著提高功耗。", "label": 1, "source": "scigen_human", "idx": 385, "lang": "zh"}
{"text": "内容交付网络（CDN）见证了视频流（如个人直播或视频点播）的爆发，手机制作或访问的视频内容必须从网络的一个点快速传输到另一个点。每当用户请求边缘服务器无法直接获得的视频时，CDN网络必须1）确定网络中存储内容的最佳位置，2）建立连接，3）尽快交付视频。出于这个原因，现有的CDN正在采用覆盖结构来减少延迟，利用软件定义网络（SDN）范式引入的灵活性。为了保证用户满意的体验质量（QoE），连接必须遵守几个服务质量（QoS）约束。在本文中，我们专注于子问题2），通过提出一种有效计算和维护覆盖网络中路径的方法。我们的方法允许在跳数、抖动、丢包和中继处理能力的约束下，通过找到最小延迟的覆盖路径来加快视频段的传输。所提出的算法提供了接近最优的解决方案，同时大大减少了执行时间。我们在真实CDN中收集的痕迹上表明，我们的解决方案可以最大限度地提高快速视频传输的数量。", "label": 1, "source": "scigen_human", "idx": 386, "lang": "zh"}
{"text": "最近，使用深度卷积神经网络从原始数据中学习分层表示的端到端方法已在图像、文本和语音领域得到成功探索。这种方法也被应用于音乐信号，但尚未得到充分的探索。为此，我们提出了样本级深度卷积神经网络，该网络从典型帧级输入表示之外的非常小的波形颗粒（例如，2或3个样本）中学习表示。我们的实验表明，具有样本级过滤器的深度架构如何提高音乐自动标记的准确性，并且它们提供的结果与Magnatagatune数据集和Million Song数据集以前最先进的性能相当。此外，我们在每一层中可视化在样本级DCNN中学习的滤波器，以识别分层学习的特征，并表明它们对沿层的对数缩放频率敏感，例如在音乐分类系统中广泛使用的mel频谱图。", "label": 1, "source": "scigen_human", "idx": 387, "lang": "zh"}
{"text": "在本文中，我们提出了一个简单的组合算法来求解近似线性时间内的对称对角占优（SDD）线性系统。它很少使用以前看来是这种算法所必需的机器。它不需要递归预处理、谱稀疏化，甚至不需要切比雪夫方法或共轭梯度。在构建了与线性系统相关的图的“漂亮”生成树后，整个算法由简单（非递归）更新规则的重复应用组成，该规则使用轻量级数据结构实现。该算法在数值上是稳定的，并且可以在不增加先前求解器所需的比特精度的情况下实现。因此，在标准单位成本RAM模型下，该算法具有最快的已知运行时间。我们希望该算法的简单性及其分析所产生的见解将在理论和实践中都有用。", "label": 1, "source": "scigen_human", "idx": 388, "lang": "zh"}
{"text": "机器学习研究的最新进展，即深度学习，引入了在几个复杂任务中优于传统算法和人类的方法，从检测图像和语音识别中的对象到玩艰难的战略游戏。然而，当前的机器学习研究方法以及这些算法在现实世界中的应用似乎存在反复出现的HARKing（在结果已知后假设）问题。在这项工作中，我们详细阐述了这种现象的算法、经济和社会原因及其后果。我们列举了当前进行机器学习研究的常见实践（例如，避免报告负面结果）以及所提出的算法和数据集在实际生活中的泛化能力失败的例子。此外，从负责任、公正、道德和隐私意识的算法决策的角度讨论了机器学习研究和开发的潜在未来轨迹。我们想强调的是，通过这次讨论，我们既不声称提供详尽的论证，也不将所提出的问题归咎于任何特定机构或个人。这只是我们这些机器学习领域的内部人士提出的一个讨论，对我们进行反思。", "label": 1, "source": "scigen_human", "idx": 389, "lang": "zh"}
{"text": "为了在欠驱动系统上实现日益动态的行为，本文提出了一种基于优化的方法来求解欠驱动两足机器人的基于全身动力学的控制器。本文的主要重点是开发一种利用基于控制李雅普诺夫函数的二次规划实现控制器的替代方法。该方法利用了文献中成功的基于逆动力学的控制器的许多理想方面，同时还结合了控制李雅普诺夫函数的变体，该变体在跟踪输出的情况下提供了更好的收敛性。该配方的主要优点包括增加成本的更大能力，从而调节机器人的最终行为。此外，模型误差倾向惯性矩阵只使用了一次，呈非倒置形式。结果成功地演示了控制器在仿真中的行走，并实时应用于硬件上进行动态蹲姿。", "label": 1, "source": "scigen_human", "idx": 390, "lang": "zh"}
{"text": "目前计算语义的一种成功方法是将单词表示为机器学习向量空间中的嵌入。我们提出了一种集成方法，将GloVe和word2vec产生的嵌入与来自语义网络ConceptNet和PPDB的结构化知识相结合，将它们的信息合并为具有大型多语言词汇的公共表示。它生成的嵌入在许多单词相似性评估上实现了最先进的性能。它在稀有单词评估中的得分为.596，比之前最著名的系统高出16分。", "label": 1, "source": "scigen_human", "idx": 391, "lang": "zh"}
{"text": "我们介绍了一种方法，通过该方法，学习动作和未来状态之间的联合分布的生成模型可以用于自动推断任何期望的奖励函数的控制方案，该奖励函数可以在不重新训练模型的情况下动态更改。在这种方法中，动作选择的问题被简化为生成模型的潜在空间上的梯度下降问题，模型本身提供了评估结果和寻找梯度的手段，很像深度Q网络（DQN）中的奖励网络如何为动作生成器提供梯度信息。与DQN或Actor-Critic不同，它们是特定奖励的条件模型，使用完全联合分配的生成模型可以动态改变奖励。此外，可以检查生成的期货，以深入了解网络“认为”会发生什么，以及当结果偏离预测时会出现什么问题。", "label": 1, "source": "scigen_human", "idx": 392, "lang": "zh"}
{"text": "疟疾是一种危及生命的疾病，影响着数百万人。基于显微镜的薄血膜评估是（i）确定疟疾种类和（ii）定量高寄生虫感染的标准方法。通过机器学习（ML）实现疟疾显微镜的完全自动化是一项具有挑战性的任务，因为现场准备的载玻片在质量和表现形式上差异很大，人工制品的数量往往远远超过相对罕见的寄生虫。在这项工作中，我们描述了一个完整、完全自动化的薄膜疟疾分析框架，该框架应用ML方法，包括卷积神经网络（CNNs），在现场制备的血液薄膜的大型多样数据集上进行训练。定量和物种鉴定结果接近足够准确，以满足现场制备样品的耐药性监测和临床使用案例的具体需要。我们将我们的方法和性能指标集中在现场用例需求上。我们讨论了ML方法应用于疟疾显微镜的关键问题和重要指标。", "label": 1, "source": "scigen_human", "idx": 393, "lang": "zh"}
{"text": "本文讨论了在计算机代数系统Maple中构造指数积分器的阶条件的有效实现，如指数分裂和Magnus型方法。该实现的核心是在积分器的局部误差的形式化展开中计算单词系数的新算法。简要回顾了潜在的理论背景，包括对局部误差结构的分析。作为一个应用，计算了涉及最小8个指数的所有8阶自伴随无换向器Magnus型积分器的系数。", "label": 1, "source": "scigen_human", "idx": 394, "lang": "zh"}
{"text": "近年来，最大可满足性（MaxSAT）求解器的性能显著提高。在实践中，MaxSAT算法通常针对最通用的MaxSAT公式，而专门解决MaxSAT的特定子类的求解器尚未得到研究。本文表明，广泛的优化和决策问题要么被自然地公式化为Horn公式上的MaxSAT，要么允许使用Horn MaxSAT进行简单编码。此外，本文还展示了如何使用Horn公式的线性时间决策过程来开发Horn-MaxSAT问题的新算法。", "label": 1, "source": "scigen_human", "idx": 395, "lang": "zh"}
{"text": "脊髓损伤经常损害行走能力。动力下肢外骨骼为恢复行走能力提供了一个很有前途的解决方案。然而，它们目前被限制在平坦的地面上。我们假设，顺从的外骨骼膝盖可以减少在不平坦地形上机动所需的努力，并提高步态速度和稳定性。我们描述了一个案例研究，一名运动完全性脊髓损伤患者（AIS a，Th12）在经过广泛训练后，带着动力外骨骼在平坦和不平坦的地面上多次行走。对于每种配置，在三个不同的日子内对顺应性或刚性外骨骼膝关节进行测量。记录身体运动和拐杖-地面相互作用力，以评估步态表现。我们观察到，与刚性配置（平均值：0.083ms和0.100ms）相比，采用顺应性外骨骼膝关节配置（在不平坦地面上平均值：0.116ms，在平坦地面上为0.145ms）的步行速度更高。在柔顺配置中，挤压力脉冲显著减少。最后，当膝关节顺应时，步态更加对称。总之，与刚性关节相比，柔顺的外骨骼膝关节可以帮助更快地在不平坦的地面上移动，使用者的工作量更少。根据我们的研究结果，外骨骼设计师应该考虑在设计中引入顺应性，以提高步态的鲁棒性和性能，并使外骨骼更适合日常生活使用。", "label": 1, "source": "scigen_human", "idx": 397, "lang": "zh"}
{"text": "区块链及其运行的程序，称为智能合约，越来越多地应用于需要信任和强大认证的所有领域。在这项工作中，我们比较了工业应用的公共区块链和许可区块链。我们提出了一个基于以太坊的完整、原创的解决方案来实现去中心化的应用程序。该解决方案的特点是一组使用权威证明共识运行区块链的验证器节点，包括一个Explorer，使用户可以检查区块链状态以及运行在其上的智能合约的源代码。不时地，最后一个挖掘区块的哈希摘要被写入公共区块链，以确保不变性。验证器节点通过赋予用户挖掘的本地以太坊，将发送交易的权利授予用户。总体而言，所提出的方法具有与公共区块链相同的透明度和不变性，没有缺点。", "label": 1, "source": "scigen_human", "idx": 398, "lang": "zh"}
{"text": "本文旨在简单地解释多代理金融市场模拟中常见的一些机制和代理。我们首先讨论了包含外生价格时间序列的必要性，即每种资产的基本价值，以及生成该序列的三种方法。然后，我们说明了一个过程，通过该过程，贝叶斯代理可以接收基本序列的有限观测值，并估计其当前和未来的值。最后，我们提出了两种在文献中广泛研究的代理，零智能代理和启发式信念学习代理，它们实现了不同的订单安排方法。", "label": 1, "source": "scigen_human", "idx": 399, "lang": "zh"}
{"text": "伪谱格式是一类能够高精度求解光滑问题的数值方法，这得益于它们对真解的指数收敛性。当应用于不连续的问题时，如流体冲击和材料界面，由于吉布斯现象，伪谱解失去了极好的收敛性，并在整个计算域中遭受虚假振荡。幸运的是，对于这些问题，存在着理论上的补救措施，这些补救措施已经在实践中成功地测试了定义明确的不连续性。我们专注于这一过程的一部分——检测光谱数据的不连续性。我们表明，现实应用需要处理随时间动态发展的不连续性，这给冲击检测带来了挑战。更准确地说，由于分辨率不足，解决方案中平滑陡峭的梯度会产生虚假振荡，导致过早的冲击识别和信息丢失。我们改进了现有的光谱冲击检测技术，使我们能够自动检测真实的不连续性，并识别需要进行后处理以抑制因分辨率损失而产生的杂散振荡的情况。然后，我们将这些技术应用于求解一维无粘性Burgers方程，证明了我们的方法正确地处理了由波浪破碎引起的真实冲击，并消除了由数值约束引起的振荡。", "label": 1, "source": "scigen_human", "idx": 400, "lang": "zh"}
{"text": "随着技术的进步，无人机在室内环境中的应用正在兴起。无人机在被占用或难以进入的室内环境中带来了更大的空间灵活性，例如制造业的车间、温室、核电站。无人机以时间高效的方式执行任务，减少人工干预，有助于创建自主制造系统。因此，调度器是需要关注的一个重要组件；然而，关于无人机调度的研究报告数量很少。这项工作提出了一种启发式方法（基于最早可用时间算法），该方法将任务分配给无人机，目标是最小化完工时间。此外，还需要对不确定事件做出快速反应，并迅速制定新的高质量可行时间表。因此，将所提出的启发式算法与粒子群优化算法相结合，以快速找到接近最优的调度。将所提出的方法实现到调度器中，并在基于实际飞行演示生成的几个规模的数据集上进行测试。详细讨论了调度器的性能评估，并报告了从选定的一组参数中获得的最佳解。", "label": 1, "source": "scigen_human", "idx": 401, "lang": "zh"}
{"text": "本文提出了一个基于矩阵极分解的复Stiefel流形乘积优化问题的通用算法框架。利用ojasewicz梯度不等式和Morse Bott性质，我们建立了这种通用算法的弱收敛性、全局收敛性和线性收敛速度。该通用算法及其收敛结果应用于同时近似张量对角化和同时近似张量压缩，其中包括特殊情况下的高阶复张量的低秩正交近似、最佳秩1近似和低多线性秩近似。我们还提出了该通用算法的对称变体来求解这类优化模型的对称变体，该模型本质上在单个Stiefel流形上进行优化。我们用相似的方法建立了它的弱收敛性、全局收敛性和线性收敛率。该对称变式及其收敛结果应用于同时近似对称张量对角化，其中包括作为特例的高阶复对称张量的低阶对称正交近似和最佳对称秩-1近似。事实证明，众所周知的算法，如LROAT、S-LROAT，HOPM，S-HOPM，都是该通用算法框架及其对称变体的特例，我们的收敛结果包含了为这些特例设计的文献中的结果。文中的所有算法和收敛结果也适用于实际情况。", "label": 1, "source": "scigen_human", "idx": 402, "lang": "zh"}
{"text": "长期以来，构建具有规划能力的智能体一直是追求人工智能的主要挑战之一。从AlphaGo到Muzero，基于树的规划方法在离散领域取得了巨大成功，如国际象棋和围棋。不幸的是，在机器人控制和倒立摆等动作空间通常是连续的现实应用中，这些基于树的规划技术将举步维艰。为了解决这些局限性，在本文中，我们提出了一种新的基于模型的强化学习框架，称为Critic PI2，它结合了轨迹优化、深度行动者-批评者学习和基于模型的加强学习的优点。我们的方法被评估为倒立摆模型，适用于许多连续控制系统。大量实验表明，Critic PI2在一系列具有挑战性的连续领域达到了新的技术水平。此外，我们还表明，使用评论家进行规划可以显著提高样本效率和实时性能。我们的工作为学习基于模型的规划系统的组成部分以及如何使用它们开辟了一个新的方向。", "label": 1, "source": "scigen_human", "idx": 403, "lang": "zh"}
{"text": "微功率脉冲多普勒雷达边缘传感是一个新兴的监测和监视领域，在智能城市中有着广泛的应用。杂波与多源雷达分类任务的现有解决方案在准确性或效率方面受到限制，在某些情况下，难以在误报和召回源之间进行权衡。我们发现，这个问题可以通过在多个时间尺度上学习分类器来解决。我们提出了一种多尺度级联递归神经网络架构MSC-RNN，该架构由用于低层杂波识别的高效多实例学习（MIL）递归神经网络（RNN）和用于高层源分类的更复杂的RNN分类器组成。通过有条件地在下层的帮助下控制上层RNN的调用，MSC-RNN实现了0.972的总体准确度。与适用于雷达推理的ML模型相比，我们的方法全面提高了准确性和每类召回率。值得注意的是，我们在时域深度特征学习方面优于跨领域手工特征工程，同时也比竞争解决方案高出3倍。", "label": 1, "source": "scigen_human", "idx": 405, "lang": "zh"}
{"text": "最近，当与基于深度神经网络（DNN）和长短期记忆（LSTM）的单耳语音增强算法相结合时，尤其是在低信噪比（SNR）条件下，渐进学习已经显示出其提高语音质量和语音可懂度的能力。然而，由于大量的参数和高计算复杂度，很难在当前资源有限的微控制器中实现，因此，对于实际应用来说，显著减少参数的数量和计算负载是至关重要的。为此，我们提出了一种新的具有因果卷积递归神经网络的渐进学习框架，称为PL-CRNN，它利用卷积神经网络和递归神经网络来大幅减少参数数量，同时提高语音质量和语音可懂度。大量实验验证了所提出的PL-CRNN模型的有效性，并表明它比PL-DNN和PL-LSTM算法产生了一致的更好的性能，而且在客观测量方面，它的结果甚至比CRNN更好。与PL-DNN、PL-LSTM和CRNN相比，所提出的PL-CRNN算法可以将参数数量分别减少到93、97和92。", "label": 1, "source": "scigen_human", "idx": 406, "lang": "zh"}
{"text": "著名的Watts-Strogatz（WS）小世界网络模型在完全随机化的极限下没有接近Erdos-Renyi（ER）随机图模型，这可能导致混淆并使某些分析复杂化。在本文中，我们讨论了宋和王首次提出的一种简单的替代方案，即在节点对之间绘制边，而不是重新布线，并具有基于距离的连接概率。我们证明了该模型更易于分析，在完全随机极限下接近真实的ER随机图模型，并使用随机行走时间可观察的例子证明了WS模型和替代模型可能产生不同的定量结果。针对备选模型，提出了一种有效的采样算法。给出了关于度分布、度方差、每个节点两颗星的数量、每个节点三角形的数量、聚类系数和随机行走混合时间的分析结果。随后，通过表明随着长程连接概率的增加，聚类系数的下降速度远慢于消息传递时间的上限来说明小世界效应，这将小世界效应从知情搜索推广到随机搜索策略。由于其可用于分析评估，我们建议将此修改后的模型用作研究小世界拓扑结构对动态系统影响的替代参考模型，以及在教授网络科学时引入众多主题的简单模型。", "label": 1, "source": "scigen_human", "idx": 407, "lang": "zh"}
{"text": "蜂窝通信网络受到冗余容量的困扰，这导致网络资本投资的利用率和成本效益低。可以利用冗余容量来提供超弹性和耐延迟的二次流量。在本文中，我们提出了一个分析框架来研究具有频谱聚合的大规模蜂窝网络中弹性二次业务的容量-延迟权衡。我们的框架集成了随机几何和排队理论模型，并对干扰受限条件下的容量延迟性能进行了分析。获得了闭合形式的结果，以将平均延迟和延迟分布表征为每用户吞吐量的函数。研究了频谱聚合、用户和基站（BS）密度、业务会话有效载荷和主要业务动态对容量-延迟折衷关系的影响。推导了基本容量极限，揭示了其缩放行为。我们的分析显示了通过蜂窝网络提供二次通信服务的可行性，并强调了一些关键的设计问题。", "label": 1, "source": "scigen_human", "idx": 408, "lang": "zh"}
{"text": "尽管有几个（公认的）标准，但信息技术或系统工程体系结构中通常使用的核心概念缺乏逻辑、代数和其他数学分支中所遇到的精确基础。在这篇文章中，我们以数学严谨的方式定义了“架构”一词的语法方面。我们通过以下方式来激励我们的特定选择：（i）如何在我们的形式化中适当地识别或推导出由各种标准定义的架构的普遍理解和预期属性，（ii）我们的概念如何与现实生活（业务）架构完全兼容，以及（iii）我们的定义如何补充该领域最近的基础工作。在同态概念的基础上，我们进一步发展了一个严格的建筑相似性概念，允许将建筑类视为一个类别，即Arch。我们通过推导某些类型建筑的分类定理来证明我们的概念对理论的适用性。", "label": 1, "source": "scigen_human", "idx": 409, "lang": "zh"}
{"text": "人类操作员的自然阻抗，或力和运动之间的动态关系，可以决定外骨骼的稳定性，外骨骼使用相互作用扭矩反馈来增强人类力量。虽然人体阻抗通常被建模为线性系统，但我们在涉及10名受试者的单关节外骨骼试验台上进行的实验显示了非线性行为的证据：人体动态刚度的低频渐近阶段不同于预期的零，并且随着刚度和惯性的变化，阻尼比出乎意料地一致。为了解释这些观察结果，本文考虑了一种新的人体关节动力学频域模型，该模型具有复值刚度，包括实刚度项和滞回阻尼项。使用统计F检验，我们表明滞回阻尼项不仅显著，而且比线性阻尼项更显著。进一步的分析揭示了滞后阻尼和刚度实部之间的线性趋势，这使我们能够将复杂的刚度模型简化为一个单参数系统。然后，我们介绍并演示了一种可定制的分数阶控制器，该控制器利用这种滞后阻尼行为来提高强度放大带宽，同时保持稳定性，并探索了一种调谐方法，以确保这种稳定性对每个个体的肌肉共收缩都是鲁棒的。", "label": 1, "source": "scigen_human", "idx": 410, "lang": "zh"}
{"text": "卷积网络是一流的计算机视觉应用程序的中心，用于各种各样的事业。自2014年以来，大量的工作开始制作更好的卷积架构，在不同的基准中产生了大量的添加。尽管扩展的模型大小和计算成本通常意味着大多数项目的质量会迅速提高，但架构现在需要一些额外的信息来提高性能。我们展示了经验证据，表明通过将基于内容的图像相似性和深度学习模型相结合，我们可以提供可用于实现聚类学习的信息流。我们展示了子数据集集群的并行训练不仅降低了计算成本，而且将基准精度提高了5-11%。", "label": 1, "source": "scigen_human", "idx": 411, "lang": "zh"}
{"text": "REST服务的使用已经成为调用第三方提供的代码的一种流行方式，尤其是在web应用程序中。如今，web应用程序的程序员可以选择TypeScript而不是JavaScript，以受益于静态类型检查，该检查允许验证对本地函数或库提供的函数的调用。然而，对REST服务的调用中的错误只能在运行时发现。在本文中，我们提出了SafeRESTScript（简称SRS），它将静态分析的支持扩展到对REST服务的调用，能够静态地发现常见错误，如REST调用中的数据丢失或无效，以及此类调用的结果被滥用。SafeRESTScript的语法与JavaScript相似，并配备了（i）丰富的类型集合（包括对象、数组和细化类型）和（ii）原语，以原生地支持REST调用，这些调用根据相应API的规范进行静态验证。规范是用HeadREST编写的，这种语言也具有精化类型，并支持以Hoare三元组的风格描述RESTAPI的语义方面。我们介绍了基于通用验证工具（Boogie）的SafeRESTScript及其验证系统。还讨论了以Eclipse插件的形式提供的SafeRESTScript及其验证器的原型实现的评估。", "label": 1, "source": "scigen_human", "idx": 412, "lang": "zh"}
{"text": "我们描述了一个证明程序性质的系统。这种方法的关键特征是一种自动合成程序中包含的循环的归纳不变量的方法。该方法是通用的，即它适用于大量的编程语言和应用领域；和惰性，因为它只生成允许导出所需属性的不变量。它依赖于一个名为GPiD的现有系统来进行溯因推理模理论，并依赖于程序验证Why3平台。实验证明了我们的方法具有实际意义。", "label": 1, "source": "scigen_human", "idx": 413, "lang": "zh"}
{"text": "在约束满足问题（CSP）领域，承诺CSP是一个令人兴奋的新研究方向。在promise CSP中，每个约束有两种形式：“严格”和“弱”，在相关的决策问题中，必须区分能够满足所有严格约束和不能满足所有弱约束。promise CSP最常被引用的例子是近似图着色问题，该问题最近取得了令人兴奋的进展，得益于基于“多态性”的promise CSP的系统代数方法，该方法将每个约束的严格形式的元组映射到相应的弱形式的元组。在这项工作中，我们提出了一个简单的算法，该算法在多项式时间内解决了所有允许无限多个对称多态性的promise CS的决策问题，即坐标是排列不变的。这概括了前两位作者以前的工作。我们还将该算法扩展到更一般的一类块对称多态性。作为推论，该单一算法同时求解所有多项式时间可处理的布尔CSP。这些结果为Schaefer的经典二分法定理提供了一个新的视角，并进一步阐明了多态性的对称性是如何实现算法的。最后，我们证明了块对称多态性对于该算法的工作不仅是充分的，而且是必要的，从而建立了它的精确能力。", "label": 1, "source": "scigen_human", "idx": 414, "lang": "zh"}
{"text": "由于移动性或散射环境的差异，无线网络中的各个链路可能会经历不相等的衰落相干时间，在这种实际情况下，通信的基本限制大多是未知的。本文研究了广播和多址信道，其中多个接收机经历不相等的衰落块长度，并且信道状态信息（CSI）在发射机处不可用，或者在任何接收机处都是免费的。换言之，在接收机处获取CSI的成本在自由度中被充分考虑。在广播信道中，采用乘积叠加的方法来寻找可实现的自由度。我们从具有整数比率的不相等相干区间开始。只要相干时间至少是发射和接收天线数量的两倍，这些自由度就在四种情况下满足上限：当发射机的天线比接收机少时，当所有接收机的天线数量相同时，当一个接收机的相干时间比所有其他接收机短得多时，或者当所有接收机具有相同的块衰落间隔时。在相同相干时间下广播的自由度区域以前也是未知的，本文的结果解决了这一问题。相干时间的差异导致不同于由诸如空间复用或多用户分集之类的其他技术产生的增益；这类增益表示为相干分集。内边界进一步扩展到多个接收器经历任意比率或对准的衰落块长度的情况。此外，在具有不相等相干时间的多址信道中，获得了自由度的可实现边界和外部边界。", "label": 1, "source": "scigen_human", "idx": 415, "lang": "zh"}
{"text": "在机器学习和计算机视觉中，时空域的学习仍然是一个极具挑战性的问题。当前用于理解时空视觉数据的计算模型在很大程度上植根于经典的基于单图像的范式。目前还不清楚如何将空间和时间中的信息整合到一个单一的通用模型中。我们提出了一种在空间和时间上递归的神经图模型，适用于捕捉不断变化的世界场景中不同实体和对象的局部外观和复杂的高级交互。我们的图中的节点和边具有用于处理信息的专用神经网络。节点对从空间和时间中的局部部分提取的特征以及以前的内存状态进行操作。边缘处理不同位置和空间尺度的连接节点之间或过去和现在时间之间的消息。消息是迭代传递的，以便在全球范围内传输信息并建立远程交互。我们的模型是通用的，可以学习识别各种高水平的时空概念，并应用于不同的学习任务。我们通过广泛的实验和消融研究证明，我们的模型在识别视频中的复杂活动方面优于强大的基线和顶级出版的方法。此外，我们在具有挑战性的SomethingSomething人机交互数据集上获得了最先进的性能。", "label": 1, "source": "scigen_human", "idx": 416, "lang": "zh"}
{"text": "本文研究分布式扩展对象跟踪问题，旨在通过节点网络协同估计对象的状态和扩展。在传统的跟踪应用中，由于传感器分辨率有限，大多数方法都将对象视为测量的点源。最近，一些研究考虑了空间结构化的扩展对象，即一个对象占用多个分辨率的单元。在此设置中，每个对象在每个时间步长生成多个测量值。本文针对传感器网络中的扩展目标跟踪问题，提出了一个贝叶斯模型。在该模型中，目标扩展由对称正定随机矩阵表示，并假设测量噪声存在但未知。利用该贝叶斯模型，我们首先提出了一种新的基于变分贝叶斯方法的扩展目标跟踪集中式算法。然后，我们将其扩展到基于交替方向乘法器（ADMM）技术的分布式场景中。所提出的算法可以同时估计扩展对象状态（运动状态和扩展）和测量噪声协方差。对扩展目标跟踪和群目标跟踪进行了仿真，验证了模型和算法的有效性。", "label": 1, "source": "scigen_human", "idx": 417, "lang": "zh"}
{"text": "图嵌入在复杂网络中的链路预测中越来越受欢迎，并取得了优异的性能。然而，在代表大多数真实网络的稀疏网络中所做的工作有限。在本文中，我们提出了一个模型，稀疏结构网络嵌入（SSNE），以获得稀疏网络中链路预测的节点表示。SSNE首先将邻接矩阵转换为归一化H阶邻接矩阵之和（SNHAM），然后通过神经网络模型将SNHAM矩阵映射为用于节点表示的d维特征矩阵。证明了映射运算是奇异值分解的等价变换。最后，基于该特征矩阵计算链路预测的节点相似性。通过在合成和真实稀疏网络上的大量测试实验，我们表明，与结构相似性指数、矩阵优化和其他图嵌入模型相比，该方法具有更好的链路预测性能。", "label": 1, "source": "scigen_human", "idx": 418, "lang": "zh"}
{"text": "具有不同模态的设备的校准是机器人视觉中的一个关键问题。常规空间对象（如平面）经常用于此任务。本文讨论了相机图像中椭圆的自动检测，以及与检测到的二维椭圆相对应的球体的三维位置估计。我们提出了两种新方法来（i）检测相机图像中的椭圆，以及（ii）如果已知相应球体的大小，则估计其空间位置。对算法进行了定量和定性测试。它们用于校准配备数字相机、深度传感器和激光雷达设备的自动驾驶汽车的传感器系统。", "label": 1, "source": "scigen_human", "idx": 419, "lang": "zh"}
{"text": "我们为异构无线网络中的数据卸载问题提供了一个通用框架，其中蜂窝用户的一些需求由互补网络提供服务。互补网络是与蜂窝网络共享相同资源的小蜂窝网络，或者是使用正交资源的WiFi网络。对于在蜂窝网络中服务的给定需求，由于彼此看到的干扰的相互耦合，每个小区的负载或资源使用水平以非线性方式取决于其他小区的负载。通过负载耦合，我们优化了在蜂窝网络或互补网络中提供服务的需求，从而使效用函数最大化。我们考虑了三个具有代表性的效用函数，它们在不同程度上平衡了为用户服务的收入与用户公平性。我们建立了优化问题具有可行解和凸性的条件，因此可以进行数值计算。最后，我们提出了一种具有理论合理性的策略，根据实际实施的需要，将负载限制在某个最大值。对欠载和过载网络进行了数值研究。", "label": 1, "source": "scigen_human", "idx": 420, "lang": "zh"}
{"text": "ohyphens随着cl-cps的日益复杂，用户和其他利益相关者越来越难以理解和理解它们的行为和决策。我们的愿景是构建可自我解释的系统，这些系统可以在运行时回答有关系统过去、当前和未来行为的问题。由于到目前为止还不存在用于构建此类系统的设计方法或参考框架，我们提出了c MAB-EX框架，用于构建在运行时利用需求和可解释性模型的自解释系统。c MAB-EX的基本思想是首先监测和分析系统的特定行为，然后从解释模型中构建解释，并以适当的方式将这种解释传达给利益相关者。我们还考虑到，如果系统检测到新的但无法解释的行为，则可以通过更新解释模型来学习新的解释。", "label": 1, "source": "scigen_human", "idx": 421, "lang": "zh"}
{"text": "虽然早期的AutoML框架专注于优化传统的ML管道及其超参数，但AutoML最近的趋势是专注于神经架构搜索。在本文中，我们介绍了Auto PyTorch，它通过联合稳健地优化网络架构和训练超参数，实现全自动深度学习（AutoDL），将这两个世界中最好的结合在一起。Auto PyTorch通过将高保真度优化与深度神经网络（DNN）的热启动和集成以及表格数据的公共基线的组合构建相结合，在几个表格基准上实现了最先进的性能。为了彻底研究我们对如何设计这样一个AutoDL系统的假设，我们还引入了一个新的DNN学习曲线基准，称为LCBench，并在典型的AutoML基准上对全自动PyTorch进行了广泛的消融研究，最终表明自动PyTorc的平均性能优于几个最先进的竞争对手。", "label": 1, "source": "scigen_human", "idx": 422, "lang": "zh"}
{"text": "背景先前最先进的药物名称识别（DNR）和临床概念提取（CCE）系统专注于文本“特征工程”和传统机器学习算法（如条件随机场和支持向量机）的组合。然而，开发好的特性本身就非常耗时。相反，更现代的机器学习方法，如递归神经网络（RNN），已被证明能够从随机分配或自动单词“嵌入”中自动学习有效特征。（i） 创建一个高度准确的DNR和CCE系统，避免传统的、耗时的特征工程。（ii）通过使用MIMIC-III等健康领域数据集创建更丰富、更专业的单词嵌入。（III）在三个当代数据集上评估我们的系统。方法。评估了两种深度学习方法，即双向LSTM和双向LSTM-CRF。CRF模型被设置为基线，以将深度学习系统与传统的机器学习方法进行比较。所有型号都使用相同的功能。后果我们使用双向LSTM-CRF模型获得了最好的结果，该模型的性能优于之前提出的所有系统。专门的嵌入有助于覆盖DrugBank和MedLine中的不寻常单词，但在i2b2VA数据集中没有。结论我们为DNR和CCE提供了最先进的系统。自动单词嵌入使我们能够避免昂贵的特征工程并实现更高的准确性。然而，嵌入需要在适合该领域的数据集上进行再训练，以充分覆盖特定领域的词汇。", "label": 1, "source": "scigen_human", "idx": 423, "lang": "zh"}
{"text": "在工作量和任务各不相同的组织中评估员工绩效是一项挑战。具体而言，重要的是要了解员工成就的定量衡量与主管期望之间的关系，良好绩效的主要驱动因素是什么，以及如何将这些复杂而灵活的绩效评估指标结合起来，准确描述组织绩效，以找出不足并提高整体生产力。为了促进这一过程，我们将常见的组织绩效分析总结为四个视觉探索任务类别。此外，我们还开发了MetricsVis，这是一个由多个协调视图组成的可视化分析系统，用于支持公共安全组织中个人、团队和组织绩效的动态评估和比较。MetricsVis提供了四个主要的视觉组件来加快绩效评估：（1）优先级调整视图，以支持对评估指标的直接操作；（2） 可重新排序的绩效矩阵，用于展示员工个人的详细信息；（3） 团队绩效视图，突出显示每个团队的总体绩效和个人贡献；以及（4）示出具有相似专业的员工的投影视图，以便于轮班分配和培训。我们通过来自中型执法机构的两个案例研究证明了我们的框架的可用性，并强调了其对其他领域的更广泛适用性。", "label": 1, "source": "scigen_human", "idx": 424, "lang": "zh"}
{"text": "最近，协作机器人已经开始训练人类完成复杂的任务，它们之间的相互信息交换可以导致成功的机器人-人类协作。在本文中，我们展示了一种称为相互强化学习（MRL）的新方法的应用和有效性，其中，在持续沟通和反馈的技能转移场景中，人类和自主主体都充当强化学习者。自主代理最初充当讲师，可以使用MRL策略教授新手人类参与者复杂的技能。当在物理（积木式建筑）（n 34）或模拟（俄罗斯方块）环境（n 31）中教授技能时，专家试图确定每个人喜欢的适当奖励渠道，并使用探索利用策略进行相应调整。这些奖励通道偏好可以识别人类参与者的重要行为，因为他们以后在类似的情况下很可能会做出相同的行为。通过这种方式，在专家系统和新手操作员之间进行技能转移。我们将受试者分为三组，观察技能转移现象，并用Simpson的心理测量模型进行分析。我们还使用5点Likert量表来识别人类参与者的认知模型。我们获得了一个共享的认知模型，该模型不仅提高了人类的认知，而且增强了机器人理解人类伙伴心理模型的认知策略，同时构建了一个成功的机器人-人类协作框架。", "label": 1, "source": "scigen_human", "idx": 425, "lang": "zh"}
{"text": "我们提出了一种计算有效的Schwarz方法来求解具有粗糙介质的椭圆方程。使用随机采样策略来寻找离线阶段中所有局部解映射的低秩近似；这些映射用于在在线阶段执行快速Schwarz迭代。数值例子证明了我们方法的准确性和稳健性。", "label": 1, "source": "scigen_human", "idx": 426, "lang": "zh"}
{"text": "尽管领域描述方法最近取得了进展，但文献计量学家仍然不知道他们的主题检测算法在多大程度上重建了“基本真理”，即科学文献中的主题结构。在本文中，我们展示了一种新的主题结构描绘方法，该方法试图将算法与理论推导和经验观察到的所有主题结构的共同特性相匹配。我们对引用链接而不是发表节点进行聚类，主要使用本地信息，并从种子子图开始搜索链接社区，以允许主题的普遍重叠。我们用一个新的成本函数评估链接集，并假设成本景观中的局部极小值对应于链接社区。由于这种成本景观具有许多局部极小值，我们将有效社区定义为在一定范围内具有最低极小值的社区。由于大型网络不可能找到所有有效的社区，我们设计了一种模因算法，将概率进化策略与确定性局部搜索相结合。我们将我们的方法应用于2010年发表的约15000篇天文天体物理学论文及其引用来源的网络，以及通过直接引用链接的约100000篇天文天体物理论文（2003-2010年发表）的网络。", "label": 1, "source": "scigen_human", "idx": 427, "lang": "zh"}
{"text": "为了能够同时访问整个最近的过去，将最近的过去的时间信息编码为空间分布的激活是至关重要的。任何依赖过去来预测未来的生物或合成试剂都将被赋予这种空间分布的时间记忆。简单地说，我们预计资源限制将要求存储器系统仅存储用于未来预测的最有用的信息。对于真实世界中表现出无标度时间波动的自然信号，如果过去的信息是标度不变的粗粒度，则编码在存储器中的预测信息是最大的。在这里，我们研究构造一个尺度不变的粗粒度内存系统的一般机制。值得注意的是，通用构造等效于对过去信息的拉普拉斯变换及其近似逆的线性组合进行编码。这揭示了存储器网络的基本构造约束，该存储器网络试图最大化与自然世界相关的预测信息存储。", "label": 1, "source": "scigen_human", "idx": 428, "lang": "zh"}
{"text": "受生成对抗性网络领域对抗性训练有效性的启发，我们提出了一种在人的重新识别中学习特征表示的新方法。我们研究了在重新识别场景中通常发生的不同类型的偏见，即姿势、身体部位和相机视图，并提出了解决这些问题的通用方法。我们引入了一种控制偏见的对抗性策略，称为偏见控制的对抗性框架（BCA），它有两个互补的分支来减少或增强与偏见相关的特征。在不同基准上的结果和与现有技术的比较表明，我们的框架是一种有效的人员重新识别策略。人们对绩效的改善既有全面的看法，也有局部的看法。", "label": 1, "source": "scigen_human", "idx": 429, "lang": "zh"}
{"text": "最近的研究表明，基于突变的故障定位技术是相对准确和实用的。然而，从未对这些方法进行过比较，仅对简单的手工播种故障进行了评估。因此，当涉及到真正的世界故障时，它们的实际实用性是值得怀疑的。为了解决这一限制，我们在一组真实世界的程序和故障上评估并比较了两种主要的基于突变的故障定位方法，即Metalliaxis和MUSE。我们基于三个典型评估指标的结果表明，基于突变的故障定位方法相对准确，并为开发人员提供了相关信息。总体而言，我们的结果表明，Metalliaxis和MUSE需要18和37个程序语句来查找所查找的故障。此外，当开发人员检查10条和25条语句时，这两种方法都能定位50条和80条所研究的错误。", "label": 1, "source": "scigen_human", "idx": 430, "lang": "zh"}
{"text": "测试套件对于软件开发过程中的故障检测至关重要。一阶突变覆盖率是量化测试套件质量的准确指标。然而，它在计算上是昂贵的。因此，这一指标的采用是有限的。在这项研究中，我们通过提出一个现实的模型来解决这个问题，该模型能够仅使用更高阶的突变覆盖率来估计一阶突变覆盖率。我们的研究显示了估计是如何随着突变的顺序而演变的。我们通过基于17个开源项目的实证研究验证了该模型。", "label": 1, "source": "scigen_human", "idx": 431, "lang": "zh"}
{"text": "在本文中，我们考虑了一个马尔可夫决策过程（MDP），其中自我主体有一个名义目标要追求，同时需要隐藏其状态以避免被对手检测到。在公式化问题后，我们首先提出了一种值迭代（VI）方法来解决它。为了克服“维度诅咒”，从而获得对更大规模问题的可扩展性，我们提出了一个后退时域优化（RHO）方法来获得近似解。我们用例子来说明和比较VI和RHO方法，并展示我们的问题公式在实际应用中的潜力。", "label": 1, "source": "scigen_human", "idx": 432, "lang": "zh"}
{"text": "我们介绍了一种通过与世界互动来理清可控和不可控变异因素的方法。解纠缠可以产生良好的表示，并且在需要解释的领域应用深度神经网络（DNN）时很重要。本研究试图改进现有的强化学习（RL）方法，以理清可控和不可控的变异因素，因为该方法缺乏表示不可控障碍的机制。为了解决这个问题，我们同时训练两个DNN：一个表示可控对象，另一个表示不可控障碍。对于稳定训练，我们应用了一种预训练方法，使用一个对不可控障碍具有鲁棒性的模型。仿真实验表明，该模型能够在没有标注数据的情况下，将可控因素和不可控因素分离开来。", "label": 1, "source": "scigen_human", "idx": 433, "lang": "zh"}
{"text": "本文探讨了自组织实体检索中实体嵌入的有效性，将实体的分布式表示引入到实体检索中。知识图包含大量的知识，并通过良好的结构表示来建模实体语义关系。实体嵌入从知识图中学习大量语义信息，并用低维表示表示实体，这为在查询相关实体和候选实体之间建立交互以进行实体检索提供了机会。我们的实验证明了基于实体嵌入的模型的有效性，它比以前最先进的基于学习到排序的实体检索模型实现了5个以上的改进。我们的进一步分析表明，实体语义匹配特征是有效的，尤其是对于需要更多语义理解的场景。", "label": 1, "source": "scigen_human", "idx": 434, "lang": "zh"}
{"text": "我们开发了并行预测熵搜索（PPES），这是一种用于昂贵黑箱目标函数的贝叶斯优化的新算法。在每次迭代中，PPES旨在选择一批点，这些点将最大化关于目标的全局最大化器的信息增益。已知的策略是基于先前的观察结果来建议单个评估点，而已知的选择并行评估的一批点的策略要少得多。已经研究的少数批次选择方案都采用贪婪方法来计算最优批次。据我们所知，PPES是第一个非贪婪的批量贝叶斯优化策略。我们展示了这种方法在合成和现实世界应用中优化性能的好处，包括机器学习、火箭科学和机器人技术中的问题。", "label": 1, "source": "scigen_human", "idx": 435, "lang": "zh"}
{"text": "自然语言处理中的许多模型定义了语言结构上的概率分布。我们认为（1）可以也应该直接评估模型的后验分布的质量，即概率是否对应于经验频率；以及（2）NLP的不确定性不仅可以投影到管道组件，还可以投影到探索性数据分析，告诉用户何时信任和不信任NLP分析。我们提出了一种分析校准的方法，并将其应用于比较几种常用模型的错误校准。我们还提供了一种共指抽样算法，该算法可以为政治事件提取任务创建置信区间。1脚注1 1脚注1这是2015年《EMNLP学报》上发表的一篇论文的扩展版本。本版本包括鸣谢和附录。有关所有材料，请参见：", "label": 1, "source": "scigen_human", "idx": 436, "lang": "zh"}
{"text": "多任务学习和多任务处理这两个术语很容易混淆。多任务学习是指机器学习中的一种范式，在这种范式中，对网络进行各种相关任务的训练，以促进任务的获取。相比之下，多任务处理被用来表明，尤其是在认知科学文献中，同时执行多个任务的能力。虽然多任务学习利用了以共享表示的形式发现任务之间的共同结构，但通过在任务之间分离表示来避免处理干扰，从而促进了多任务处理。在这里，我们建立在之前涉及浅层网络和简单任务设置的工作基础上，这表明多任务学习和多任务处理之间存在权衡，通过使用共享和分离的表示来进行调解。我们证明了在深度网络中也会出现同样的紧张关系，并讨论了一种元学习算法，用于代理在不熟悉的环境中管理这种权衡。我们通过不同的实验表明，作为环境的函数，agent能够成功地优化其训练策略。", "label": 1, "source": "scigen_human", "idx": 437, "lang": "zh"}
{"text": "实体登记系统（ERS）是一个去中心化的实体登记处，可以用来取代网络，在后者不可用时作为发布链接数据的平台。在发展中国家，离线是默认的操作模式，集中的链接数据解决方案无法满足社区的需求。尽管这些功能已基本完成，但该系统尚未准备好进行部署。该项目旨在提供广泛的测试和可扩展性调查，使其为真实场景做好准备。", "label": 1, "source": "scigen_human", "idx": 438, "lang": "zh"}
{"text": "我们研究了具有认知小细胞的双层异构网络（HetNet）中的共存问题。特别地，我们考虑底层HetNet，其中认知小型基站（C-SBS）被允许使用具有接入概率（AP）的宏小区的频带，只要C-SBS在宏用户（MU）处满足预设干扰概率（IP）约束。为了增强C-SBS的AP（或传输机会），我们提出了一种基于学习的C-SBS算法，并利用宏基站（MBS）和MU之间的距离信息。通常，从MBS到特定MU的信号包含MBS到MU的距离信息。数值结果表明，该算法的性能优于现有方法，达到60AP（或传输机会）。", "label": 1, "source": "scigen_human", "idx": 439, "lang": "zh"}
{"text": "我们提供了一个新的自然VNP-中间多项式族的列表，基于在简约约简下完全的基本（组合）NP-完全问题。在有限域上，这些族在VNP中，并且在合理的假设Mod p p p poly下，既不是VNP-hard（即使在预言电路约简下）也不是VP。在此之前，只有割枚举多项式是已知的VNP-中间体，如Burgisser在2000年所示。接下来，我们证明了在有理和实数上，我们的两个中间多项式，基于可满足性和哈密顿循环，不是永久的单调仿射多项式大小的投影。由于Grochow，这增加了最近沿着这条线的结果。最后，我们描述了一个独立于计算模型定义的（有点自然的）多项式，并证明了它在多项式大小投影下是VP-完全的。这补充了Durand等人（2014）最近的一个结果，该结果建立了相关多项式的VP-完备性，但在恒定深度预言电路约简下。这两个多项式都是基于图同态的。一个简单的限制产生了一个类似于VBP的完全族。", "label": 1, "source": "scigen_human", "idx": 440, "lang": "zh"}
{"text": "对分割良好的三维骨架视频中的动作识别进行了深入的研究。然而，由于难以表示3D骨架视频和缺乏训练数据，流式3D骨架视频的动作检测仍然远远落后于其识别对手和基于图像的对象检测。在本文中，我们提出了一种解决此问题的新方法，该方法利用了有效的骨架视频编码和基于深度回归的图像对象检测。我们的框架由两部分组成：基于骨架的视频图像映射，它以时间保持的方式将骨架视频编码为彩色图像，以及基于图像检测的端到端可训练快速骨架动作检测器（skeleton Boxes）。在最新和最大的PKU-MMD基准数据集上的实验结果表明，我们的方法在很大程度上优于最先进的方法。我们相信我们的想法将激励和有益于这一重要领域的未来研究。", "label": 1, "source": "scigen_human", "idx": 441, "lang": "zh"}
{"text": "在本文中，我们介绍了一种新的方法，当给定基础图像时，根据所需位置上的文本属性生成对象图像。在现有的主要关注对象外观的文本到图像生成研究的基础上，该方法旨在生成保留给定背景信息的对象图像，这是该领域的首次尝试。为了解决这个问题，我们提出了一种多条件GAN（MC-GAN），它联合控制对象和背景信息。作为MC-GAN的核心组件，我们提出了一种在训练阶段解开对象和背景信息的合成块。该块使得MC-GAN能够通过使用来自文本属性的前景信息来控制来自给定基础图像的背景信息的量，从而生成具有期望背景的真实对象图像。通过对Caltech-200鸟类和Oxford-102花朵数据集的实验，我们表明我们的模型能够生成分辨率为128 128的照片逼真图像。MC-GAN的源代码发布。1脚注1 1脚注1", "label": 1, "source": "scigen_human", "idx": 442, "lang": "zh"}
{"text": "自动定理证明器的输出通常使用文本格式表示，它们通常太重而无法理解。在模型检查设置中，如果能够观察模型的结构和验证程序，将是有益的。针对这些问题，本文提出了一种三维可视化工具VMDV。通过将VMDV应用于一个证明系统，说明了它的实用性。", "label": 1, "source": "scigen_human", "idx": 443, "lang": "zh"}
{"text": "基于个性化历史的推荐的目标是在给定用户先前购买序列的情况下，自动输出所有项目的分布。在这项工作中，我们提出了一种新的方法，该方法使用递归网络来总结购买历史，使用连续向量来表示可伸缩性的项目，以及一种基于注意力的递归混合密度网络，该网络顺序输出混合中的每个组分，用于建模多模式条件分布。我们在两个公开可用的数据集MovieLens-20M和RecSys15上评估了所提出的方法。实验表明，所提出的方法明确地对预测分布的多模态性质进行了建模，能够在精度、召回率和NCG方面提高各种基线的性能。", "label": 1, "source": "scigen_human", "idx": 444, "lang": "zh"}
{"text": "本文提出了一种新的可微体系结构搜索方法，将其公式化为分布学习问题。我们将连续松弛结构的混合权重视为随机变量，通过狄利克雷分布进行建模。使用最近开发的路径导数，可以使用基于梯度的优化器以端到端的方式轻松地优化狄利克雷参数。该公式提高了泛化能力，并引入了随机性，这自然鼓励了在搜索空间中的探索。此外，为了缓解可微分NAS的大内存消耗，我们提出了一种简单而有效的渐进式学习方案，该方案能够直接在大规模任务上进行搜索，消除搜索和评估阶段之间的差距。大量的实验证明了我们方法的有效性。具体来说，我们在移动设置下获得了CIFAR-10的2.46的测试误差，ImageNet的23.7的测试误差。在NAS-Bench-201上，我们还在所有三个数据集上获得了最先进的结果，并为神经架构搜索算法的有效设计提供了见解。我们的搜索和评估代码可在", "label": 1, "source": "scigen_human", "idx": 445, "lang": "zh"}
{"text": "亚马逊、淘宝和天猫等电子商务平台上的赞助搜索为卖家提供了一种有效的方式，以最相关的目的接触潜在买家。本文研究了阿里巴巴移动电子商务平台上赞助搜索的拍卖机制优化问题。除了创造收入，我们还应该保持一个拥有大量优质用户的高效市场，保证广告商的合理投资回报率，同时为用户提供愉快的购物体验。这些需求本质上构成了一个受约束的优化问题。直接优化拍卖参数会产生一个不连续的非凸问题，该问题否定了有效的解决方案。我们的主要贡献之一是原始问题的实用凸优化公式。我们设计了一种新的拍卖机制的重新参数化方法，该方法具有离散的代表性实例集。为了构建优化问题，我们构建了一个拍卖模拟系统，该系统通过回放真实在线请求中记录的拍卖来估计所选参数的最终商业指标。我们总结了在真实搜索流量上的实验，以分析拍卖模拟保真度的影响、在各种约束目标下的有效性以及正则化的影响。实验结果表明，通过适当的熵正则化，我们能够在将其他业务指标约束在给定范围内的同时实现收入最大化。", "label": 1, "source": "scigen_human", "idx": 446, "lang": "zh"}
{"text": "虽然社交媒体可以很容易地与任何人建立联系并访问任何人的信息，但它们也促进了基本的影响力和解除好友关系机制，这些机制可能会导致被称为“回音室”的分离和两极分化的集群。在这里，我们通过引入一个简单的在线社交网络信息共享模型来研究这种回音室出现的条件，该模型包含影响力和解除朋友关系两个组成部分。用户可以根据他们通过共享所接触到的信息来改变他们的观点和社交关系。模型动态显示，即使影响力和好友关系很小，社交网络也会迅速演变成隔离、同质的社区。这些预测与Twitter的经验数据一致。尽管我们的研究结果表明，考虑到在线社交媒体中的机制，回音室在一定程度上是不可避免的，但它们也为可能的缓解策略提供了见解。", "label": 1, "source": "scigen_human", "idx": 447, "lang": "zh"}
{"text": "我们提出了Accel，这是一种新颖的语义视频分割系统，它通过组合两个网络分支的预测，以低推理成本实现了高精度：（1）一个参考分支，提取参考关键帧上的高细节特征，并使用逐帧光流估计将这些特征向前扭曲；（2）一个更新分支，计算当前帧上质量可调的特征，对每个视频帧执行时间更新。更新分支的模块性，其中可以插入不同层深度的特征子网络（例如，ResNet-18到ResNet-101），使得能够在新的、最先进的精度-吞吐量权衡频谱上进行操作。在这条曲线上，Accel模型比最接近的可比单帧分割网络实现了更高的精度和更快的推理时间。总的来说，Accel在有效的语义视频分割方面显著优于之前的工作，纠正了在具有复杂动力学的数据集上复合的扭曲相关错误。Accel是端到端可训练的，高度模块化：参考网络、光流网络和更新网络可以根据应用要求单独选择，然后联合微调。其结果是为视频提供了一个健壮、通用的快速、高精度语义分割系统。", "label": 1, "source": "scigen_human", "idx": 448, "lang": "zh"}
{"text": "我们介绍对称算术电路，即具有自然对称限制的算术电路。在电路计算变量矩阵上定义的多项式的情况下，如行列式或永久性，限制相当于要求电路的形状在矩阵的行和列排列下是不变的。我们对任何对称电路的大小建立了无条件的、几乎指数的下界，用于计算除2以外的任何特征域上的永久性。相反，我们证明了在特征零的域上有多项式大小的对称电路来计算行列式。", "label": 1, "source": "scigen_human", "idx": 449, "lang": "zh"}
{"text": "当前的细粒度识别方法如下：首先，招募专家对图像数据集进行注释，还可以选择以零件注释和边界框的形式收集更多结构化数据。其次，利用这些数据训练模型。为了解决细粒度识别的目标，我们引入了一种替代方法，利用来自网络的免费、有噪声的数据和简单、通用的识别方法。这种方法在性能和可扩展性方面都有好处。我们在四个细粒度数据集上展示了它的功效，大大超过了现有的技术水平，甚至不需要手动收集单个标签，而且还首次显示了扩展到10000多个细粒度类别的结果。从数量上讲，在不使用注释训练集的情况下，我们在CUB-上获得了92.3的前1准确率，在Birdsnap上获得了85.4的前1准确性，在FGVC飞机上获得了93.4的前2准确率，而在Stanford Dogs上获得了80.8的前1精度。我们将我们的方法与用于扩展细粒度数据集的主动学习方法进行了比较。", "label": 1, "source": "scigen_human", "idx": 450, "lang": "zh"}
{"text": "压缩映射的Banach不动点定理已被广泛用于分析非凸问题中迭代方法的收敛性。然而，一种常见的经验是，迭代映射在其域中的自然度量下不能全局收缩，这使得Banach定理的适用性受到限制。我们探索了当我们将Banach的不动点定理与精心设计的度量配对时，如何普遍地应用它来建立迭代方法的收敛性。我们的第一个结果是Banach定理的强逆，表明它是一个通用的分析工具，用于建立迭代方法到唯一不动点的全局收敛性，并限制它们的收敛速度。换句话说，我们证明，每当迭代映射全局收敛到唯一不动点时，存在一个度量，在该度量下迭代映射收缩，并且可以用于约束迭代次数，直到收敛。我们在广泛使用的幂方法中说明了我们的方法，提供了一种通过收缩自变量来限制其收敛速度的新方法。接下来我们考虑Banach不动点定理的计算复杂性。通过构造逆定理的证明，证明了Banach不动点定理所保证的不动点的计算是CLS完全的。因此，我们为CLS类提供了第一个自然完全问题，该问题在中被定义为捕获问题的复杂性，如P-矩阵LCP、计算KKT点以及在拥塞和网络协调对策中找到混合纳什均衡。", "label": 1, "source": "scigen_human", "idx": 451, "lang": "zh"}
{"text": "由可开发零件制成的形状是工艺美术、刺绣、现代建筑和CAD等艺术的基础，它激发了许多研究。我们观察到，通过现有方法创建的复杂3D形状的组装通常需要首先制造许多小的平面零件，然后仔细按照说明将它们组装在一起。尽管这一过程意义重大，但在讨论中通常会忽略这一容易出错且乏味的过程。我们提出了一种通过单个可展开零件进行形状表示的方法，该零件附着在自身上，不需要装配说明。我们的灵感来自所谓的拉链袋，它由一条长丝带制成，边缘有拉链。为了“组装”袋子，只需拉上缎带的拉链。我们的方法以同样的方式操作，但它可以用于近似任何形状。给定一个3D模型，我们的算法生成单个2D形状的计划，该形状可以从平面织物或纸上激光切割成几个部分。然后，我们可以沿着边界连接拉链，以便快速组装和拆卸，或者应用更传统的方法，如粘合和缝合。我们展示了物理和虚拟结果，这些结果证明了我们的方法的能力以及组装形状的容易性。", "label": 1, "source": "scigen_human", "idx": 452, "lang": "zh"}
{"text": "我们研究了具有不对称信息的战略代理的动态系统中的贝叶斯学习问题。在文献中的一系列开创性论文中，基于对系统状态的私人嘈杂观察和对过去玩家行为的公开观察，在一个简化模型下研究了这个问题，在该模型中，自私的玩家依次出现并在游戏中行动一次。研究表明，存在用户丢弃私人信息并模仿前任行为的信息级联。在本文中，我们提供了一个框架，用于在比上述更通用的环境中研究贝叶斯学习动力学。特别是，我们的模型包含了玩家在整个游戏过程中都是非短视和战略性参与的情况，以及内生过程选择玩家的哪个子集将在每个时间实例中采取行动的情况。所提出的框架依赖于一种序列分解方法，用于寻找一类具有不对称信息的一般动态博弈的结构化完美贝叶斯均衡（PBE），其中用户特定的状态演化为条件独立的马尔可夫过程，用户对其状态进行独立的有噪观测。使用这种方法，我们研究了一个特定的动态学习模型，在该模型中，参与者根据对每个人类型的估计做出公共投资决策。我们为这个问题刻画了一组信息级联，其中整个团队的学习停止了。我们证明，在这种级联中，所有玩家对其他玩家类型的估计都是冻结的，即使每个玩家都渐近地学习自己的真实类型。", "label": 1, "source": "scigen_human", "idx": 453, "lang": "zh"}
{"text": "近年来，大型多语言NLP项目的数量有所增加。然而，即使在这样的项目中，具有特殊处理要求的语言也经常被排除在外。其中一种语言是日语。日语是在没有空格的情况下编写的，标记化是不平凡的，尽管存在高质量的开源标记化器，但它们可能很难使用，而且缺乏英文文档。本文介绍了fugashi，一个用于Python的MeCab包装器，并介绍了日语的标记化。", "label": 1, "source": "scigen_human", "idx": 454, "lang": "zh"}
{"text": "在广义零样本学习（GZSL）环境下，看不见类的分类精度远低于传统的零样本学习（ZSL），这是公认的事实。其中一个原因是实例总是被错误地分类到错误的域。在这里，我们将可见类和不可见类分别称为两个域。我们提出了一种新的方法来区分实例是来自可见类还是不可见类。首先将实例的视觉特征投影到语义空间中。然后，将投影的语义向量与类语义嵌入向量之间的绝对范数差以及投影的语义矢量与所看到的类的语义嵌入向量的最小距离用作判别依据。这种方法被称为SD（语义鉴别器），因为实例的领域判断是在语义空间中进行的。我们的方法可以与任何现有的ZSL方法和全监督分类模型相结合，形成一种新的GZSL方法。此外，我们的方法非常简单，不需要任何固定的参数。大量实验表明，我们的方法的精度比目前的最佳方法高出8.5到21.9。", "label": 1, "source": "scigen_human", "idx": 455, "lang": "zh"}
{"text": "文体变异对于使会话主体产生的话语自然而引人入胜至关重要。在本文中，我们专注于开放领域对话反应生成的序列到序列模型，并提出了一种新的方法来评估这些模型能够在多大程度上生成反映不同个性特征的反应。", "label": 1, "source": "scigen_human", "idx": 457, "lang": "zh"}
